
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{KTH\_With\_BG\_comparision\_combined2\_with\_testCases}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \hypertarget{human-activity-recognition}{%
\section{Human Activity Recognition}\label{human-activity-recognition}}

\hypertarget{obtain-the-input-files}{%
\subsection{Obtain the input files}\label{obtain-the-input-files}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{c+c1}{\PYZsh{} Imports}
        \PY{k+kn}{import} \PY{n+nn}{os}
        \PY{k+kn}{from} \PY{n+nn}{sklearn.datasets} \PY{k+kn}{import} \PY{n}{load\PYZus{}files}
        \PY{k+kn}{from} \PY{n+nn}{sklearn.model\PYZus{}selection} \PY{k+kn}{import} \PY{n}{train\PYZus{}test\PYZus{}split}
        
        \PY{c+c1}{\PYZsh{} Loading the data}
        \PY{n}{raw\PYZus{}data} \PY{o}{=} \PY{n}{load\PYZus{}files}\PY{p}{(}\PY{n}{os}\PY{o}{.}\PY{n}{getcwd}\PY{p}{(}\PY{p}{)} \PY{o}{+} \PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{/Data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{shuffle}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{)}
        \PY{n}{files} \PY{o}{=} \PY{n}{raw\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{filenames}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        \PY{n}{targets} \PY{o}{=} \PY{n}{raw\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        
        \PY{n}{train\PYZus{}files}\PY{p}{,} \PY{n}{test\PYZus{}files}\PY{p}{,} \PY{n}{train\PYZus{}targets}\PY{p}{,} \PY{n}{test\PYZus{}targets} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{files}\PY{p}{,} \PY{n}{targets}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{191}\PY{p}{)}
        
        \PY{n}{valid\PYZus{}files} \PY{o}{=} \PY{n}{train\PYZus{}files}\PY{p}{[}\PY{l+m+mi}{300}\PY{p}{:}\PY{p}{]}
        \PY{n}{valid\PYZus{}targets} \PY{o}{=} \PY{n}{train\PYZus{}targets}\PY{p}{[}\PY{l+m+mi}{300}\PY{p}{:}\PY{p}{]}
        
        \PY{n}{train\PYZus{}files} \PY{o}{=} \PY{n}{train\PYZus{}files}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{300}\PY{p}{]}
        \PY{n}{train\PYZus{}targets} \PY{o}{=} \PY{n}{train\PYZus{}targets}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{300}\PY{p}{]}
        
        \PY{c+c1}{\PYZsh{} Generic details about the data}
        \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Total number of videos:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{files}\PY{p}{)}\PY{p}{)}
        \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{Number of videos in training data:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{train\PYZus{}files}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
        \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Number of videos in validation data:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{valid\PYZus{}files}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
        \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Number of videos in test data:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{test\PYZus{}files}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
('Total number of videos:', 522)
('\textbackslash{}nNumber of videos in training data:', 300)
('Number of videos in validation data:', 122)
('Number of videos in test data:', 100)

    \end{Verbatim}

    \hypertarget{description-of-the-class-labels}{%
\subsubsection{Description of the class
labels}\label{description-of-the-class-labels}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The categorical labels are converted into integers.}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{Following is the mapping \PYZhy{} }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{k}{for} \PY{n}{label} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{,} \PY{n}{raw\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target\PYZus{}names}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{:}
            \PY{k}{print}\PY{p}{(}\PY{n}{label}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
The categorical labels are converted into integers.
Following is the mapping - 

(0, 'boxing')
(1, 'handclapping')
(2, 'handwaving')
(3, 'jogging')
(4, 'running')
(5, 'walking')

    \end{Verbatim}

    \hypertarget{each-video-file-is-associated-with-its-class-label}{%
\subsubsection{Each video file is associated with its class
label}\label{each-video-file-is-associated-with-its-class-label}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{c+c1}{\PYZsh{} Displaying the first 5 videos (paths) in the training data along with their labels}
        \PY{c+c1}{\PYZsh{} (path of video, class label)}
        \PY{k}{for} \PY{n}{pair} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{train\PYZus{}files}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{,} \PY{n}{train\PYZus{}targets}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)}\PY{p}{:}
            \PY{k}{print}\PY{p}{(}\PY{n}{pair}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
('/home/geet/project/implementation/CNN/Data/handwaving/person05\_handwaving\_d4\_uncomp.avi', 2)
('/home/geet/project/implementation/CNN/Data/walking/person13\_walking\_d3\_uncomp.avi', 5)
('/home/geet/project/implementation/CNN/Data/boxing/person09\_boxing\_d4\_uncomp.avi', 0)
('/home/geet/project/implementation/CNN/Data/running/person11\_running\_d4\_uncomp.avi', 4)
('/home/geet/project/implementation/CNN/Data/walking/person14\_walking\_d1\_uncomp.avi', 5)

    \end{Verbatim}

    \hypertarget{test-case-for-the-number-of-classes-and-their-mapping}{%
\subsection{Test Case for the Number of Classes and their
mapping}\label{test-case-for-the-number-of-classes-and-their-mapping}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{n}{no\PYZus{}of\PYZus{}classes} \PY{o}{=}\PY{l+m+mi}{6}\PY{p}{;}
        \PY{n}{test\PYZus{}result}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{;}
        \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{train\PYZus{}targets}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} THe mappings of targets are from 0\PYZhy{}(number of classes \PYZhy{}1)}
            \PY{k}{if} \PY{n}{i}\PY{o}{\PYZgt{}}\PY{n}{no\PYZus{}of\PYZus{}classes}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}
                \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test Failed: Mapping has exceeded the permissible mapping of the target in training Set}\PY{l+s+s2}{\PYZdq{}}
                \PY{n}{test\PYZus{}result}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{;}
        
        \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{test\PYZus{}targets}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} THe mappings of targets are from 0\PYZhy{}(number of classes \PYZhy{}1)}
            \PY{k}{if} \PY{n}{i}\PY{o}{\PYZgt{}}\PY{n}{no\PYZus{}of\PYZus{}classes}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}
                \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test Failed: Mapping has exceeded the permissible mapping of the target in testing Set}\PY{l+s+s2}{\PYZdq{}}
                \PY{n}{test\PYZus{}result}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{;}
                
        \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{valid\PYZus{}targets}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} THe mappings of targets are from 0\PYZhy{}(number of classes \PYZhy{}1)}
            \PY{k}{if} \PY{n}{i}\PY{o}{\PYZgt{}}\PY{n}{no\PYZus{}of\PYZus{}classes}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}
                \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test Failed: Mapping has exceeded the permissible mapping of the target in validation set}\PY{l+s+s2}{\PYZdq{}}
                \PY{n}{test\PYZus{}result}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{;}
        
        \PY{k}{if} \PY{n}{test\PYZus{}result}\PY{o}{==} \PY{n+nb+bp}{True}\PY{p}{:}
            \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test Successful}\PY{l+s+s2}{\PYZdq{}}
        \PY{k}{else}\PY{p}{:}
            \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test Failed}\PY{l+s+s2}{\PYZdq{}}
            
            
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Test Successful

    \end{Verbatim}

    \hypertarget{negative-testcase}{%
\subsection{Negative testcase}\label{negative-testcase}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{k+kn}{import} \PY{n+nn}{copy}
        
        \PY{n}{no\PYZus{}of\PYZus{}classes} \PY{o}{=}\PY{l+m+mi}{6}\PY{p}{;}
        \PY{n}{test\PYZus{}result}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{;}
        
        \PY{c+c1}{\PYZsh{}Manually Passing the irrelaent Category}
        \PY{n}{valid\PYZus{}targets2}\PY{o}{=}\PY{n}{copy}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{n}{valid\PYZus{}targets}\PY{p}{)}\PY{p}{;}
        \PY{n}{train\PYZus{}targets2}\PY{o}{=}\PY{n}{copy}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{n}{train\PYZus{}targets}\PY{p}{)}\PY{p}{;}
        \PY{n}{test\PYZus{}targets2}\PY{o}{=}\PY{n}{copy}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{n}{test\PYZus{}targets}\PY{p}{)}\PY{p}{;}
        
        \PY{n}{valid\PYZus{}targets2}\PY{p}{[}\PY{l+m+mi}{20}\PY{p}{]}\PY{o}{=}\PY{l+m+mi}{8}\PY{p}{;}
        
        
        \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{train\PYZus{}targets}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} THe mappings of targets are from 0\PYZhy{}(number of classes \PYZhy{}1)}
            \PY{k}{if} \PY{n}{i}\PY{o}{\PYZgt{}}\PY{n}{no\PYZus{}of\PYZus{}classes}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}
                \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test Failed: Mapping has exceeded the permissible mapping of the target in training Set}\PY{l+s+s2}{\PYZdq{}}
                \PY{n}{test\PYZus{}result}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{;}
        
        \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{test\PYZus{}targets}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} THe mappings of targets are from 0\PYZhy{}(number of classes \PYZhy{}1)}
            \PY{k}{if} \PY{n}{i}\PY{o}{\PYZgt{}}\PY{n}{no\PYZus{}of\PYZus{}classes}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}
                \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test Failed: Mapping has exceeded the permissible mapping of the target in testing Set}\PY{l+s+s2}{\PYZdq{}}
                \PY{n}{test\PYZus{}result}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{;}
                
        \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{valid\PYZus{}targets2}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} THe mappings of targets are from 0\PYZhy{}(number of classes \PYZhy{}1)}
            \PY{k}{if} \PY{n}{i}\PY{o}{\PYZgt{}}\PY{n}{no\PYZus{}of\PYZus{}classes}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}
                \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test Failed: Mapping has exceeded the permissible mapping of the target in validation set}\PY{l+s+s2}{\PYZdq{}}
                \PY{n}{test\PYZus{}result}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{;}
        
        \PY{k}{if} \PY{n}{test\PYZus{}result}\PY{o}{==} \PY{n+nb+bp}{True}\PY{p}{:}
            \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test Successful}\PY{l+s+s2}{\PYZdq{}}
        \PY{k}{else}\PY{p}{:}
            \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test Failed}\PY{l+s+s2}{\PYZdq{}}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Test Failed: Mapping has exceeded the permissible mapping of the target in validation set
Test Failed

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{c+c1}{\PYZsh{} Imports}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k+kn}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib.pyplot} \PY{k+kn}{as} \PY{n+nn}{plt}
        \PY{k+kn}{import} \PY{n+nn}{skvideo.io}
        \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
        
        
        \PY{c+c1}{\PYZsh{} The path of a sample video in the training data}
        \PY{n}{sample\PYZus{}files} \PY{o}{=} \PY{n}{train\PYZus{}files}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{1}\PY{p}{]}
        
        \PY{c+c1}{\PYZsh{} An object of the class \PYZsq{}Videos\PYZsq{}}
        \PY{n}{sample} \PY{o}{=} \PY{n}{skvideo}\PY{o}{.}\PY{n}{io}\PY{o}{.}\PY{n}{vread}\PY{p}{(}\PY{n}{sample\PYZus{}files}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{;}
        
        \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{Shape of the sample data:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{sample}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Displaying a frame from the sample video}
        \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{sample}\PY{p}{[}\PY{l+m+mi}{300}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
('\textbackslash{}nShape of the sample data:', (432, 120, 160, 3))

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}6}]:} <matplotlib.image.AxesImage at 0x7f269df1d650>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_10_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{n}{sec}\PY{o}{=}\PY{l+m+mi}{7}
        \PY{n}{frames\PYZus{}needed}\PY{o}{=}\PY{l+m+mi}{35}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k+kn}{as} \PY{n+nn}{np}
        \PY{k+kn}{from} \PY{n+nn}{skvideo.io} \PY{k+kn}{import} \PY{n}{FFmpegReader}\PY{p}{,} \PY{n}{ffprobe}
        \PY{k+kn}{from} \PY{n+nn}{skvideo.utils} \PY{k+kn}{import} \PY{n}{rgb2gray}
        \PY{k+kn}{from} \PY{n+nn}{PIL} \PY{k+kn}{import} \PY{n}{Image}
        \PY{k+kn}{from} \PY{n+nn}{keras.preprocessing} \PY{k+kn}{import} \PY{n}{image}
        \PY{k+kn}{from} \PY{n+nn}{tqdm} \PY{k+kn}{import} \PY{n}{tqdm}
        \PY{k+kn}{from} \PY{n+nn}{keras.utils} \PY{k+kn}{import} \PY{n}{to\PYZus{}categorical}
        \PY{k+kn}{import} \PY{n+nn}{cv2}
        \PY{k}{def} \PY{n+nf}{read\PYZus{}video\PYZus{}bgsub}\PY{p}{(}\PY{n}{path}\PY{p}{,}\PY{n}{size}\PY{p}{)}\PY{p}{:}
        
        
                \PY{n}{cap} \PY{o}{=} \PY{n}{FFmpegReader}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{n}{path}\PY{p}{)}
                \PY{n}{list\PYZus{}of\PYZus{}frames} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                \PY{n}{fps} \PY{o}{=} \PY{n+nb}{int}\PY{p}{(}\PY{n}{cap}\PY{o}{.}\PY{n}{inputfps}\PY{p}{)}                  \PY{c+c1}{\PYZsh{} Frame Rate}
                
                
                \PY{n}{frames\PYZus{}from\PYZus{}sec}\PY{o}{=}\PY{n+nb}{int}\PY{p}{(}\PY{n}{frames\PYZus{}needed}\PY{o}{/}\PY{n}{sec}\PY{p}{)}
                
                \PY{n}{frame\PYZus{}alternate}\PY{o}{=}\PY{n+nb}{int}\PY{p}{(}\PY{n}{fps}\PY{o}{/}\PY{n}{frames\PYZus{}from\PYZus{}sec}\PY{p}{)}
                \PY{n}{kernel} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{getStructuringElement}\PY{p}{(}\PY{n}{cv2}\PY{o}{.}\PY{n}{MORPH\PYZus{}ELLIPSE}\PY{p}{,}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}
                \PY{n}{fgbg} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{createBackgroundSubtractorMOG2}\PY{p}{(}\PY{p}{)}
                \PY{n}{count} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{;}
                
                \PY{k}{for} \PY{n}{index}\PY{p}{,} \PY{n}{frame} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{cap}\PY{o}{.}\PY{n}{nextFrame}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                    \PY{n}{temp\PYZus{}image} \PY{o}{=} \PY{n}{image}\PY{o}{.}\PY{n}{array\PYZus{}to\PYZus{}img}\PY{p}{(}\PY{n}{frame}\PY{p}{)}
                    \PY{c+c1}{\PYZsh{}print type(temp\PYZus{}image)}
                    \PY{n}{fgmask} \PY{o}{=} \PY{n}{fgbg}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{n}{frame}\PY{p}{)}
                    \PY{n}{fgmask} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{morphologyEx}\PY{p}{(}\PY{n}{fgmask}\PY{p}{,} \PY{n}{cv2}\PY{o}{.}\PY{n}{MORPH\PYZus{}OPEN}\PY{p}{,} \PY{n}{kernel}\PY{p}{)}
                    
                    \PY{n}{fgmaskimg} \PY{o}{=} \PY{n}{Image}\PY{o}{.}\PY{n}{fromarray}\PY{p}{(}\PY{n}{fgmask}\PY{p}{)}
                    
                    \PY{c+c1}{\PYZsh{}print type(fgmask)}
                    \PY{n}{frame} \PY{o}{=} \PY{n}{image}\PY{o}{.}\PY{n}{img\PYZus{}to\PYZus{}array}\PY{p}{(}
                                \PY{n}{fgmaskimg}\PY{o}{.}\PY{n}{resize}\PY{p}{(}
                                    \PY{p}{(}\PY{n}{size}\PY{p}{,} \PY{n}{size}\PY{p}{)}\PY{p}{,}
                                    \PY{n}{Image}\PY{o}{.}\PY{n}{ANTIALIAS}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{uint8}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                    \PY{k}{if} \PY{n}{index}\PY{o}{\PYZpc{}}\PY{k}{frame\PYZus{}alternate} == 0:
                        \PY{k}{if} \PY{n}{count} \PY{o}{\PYZlt{}} \PY{n}{frames\PYZus{}needed}\PY{p}{:}
                            \PY{n}{list\PYZus{}of\PYZus{}frames}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{frame}\PY{p}{)}\PY{p}{;}
                            \PY{n}{count}\PY{o}{=} \PY{n}{count}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{;}
        
                \PY{n}{temp\PYZus{}video} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{stack}\PY{p}{(}\PY{n}{list\PYZus{}of\PYZus{}frames}\PY{p}{)}
                
                \PY{c+c1}{\PYZsh{}\PYZsh{}Extract Specific Frames}
                
                \PY{n}{total\PYZus{}frames} \PY{o}{=} \PY{n}{temp\PYZus{}video}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
                
                
                
                \PY{n}{cap}\PY{o}{.}\PY{n}{close}\PY{p}{(}\PY{p}{)}
        
                \PY{n}{temp\PYZus{}video} \PY{o}{=} \PY{n}{rgb2gray}\PY{p}{(}\PY{n}{temp\PYZus{}video}\PY{p}{)}
                        
                \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{expand\PYZus{}dims}\PY{p}{(}\PY{n}{temp\PYZus{}video}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Using TensorFlow backend.

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k+kn}{as} \PY{n+nn}{np}
        \PY{k+kn}{from} \PY{n+nn}{skvideo.io} \PY{k+kn}{import} \PY{n}{FFmpegReader}\PY{p}{,} \PY{n}{ffprobe}
        \PY{k+kn}{from} \PY{n+nn}{skvideo.utils} \PY{k+kn}{import} \PY{n}{rgb2gray}
        \PY{k+kn}{from} \PY{n+nn}{PIL} \PY{k+kn}{import} \PY{n}{Image}
        \PY{k+kn}{from} \PY{n+nn}{keras.preprocessing} \PY{k+kn}{import} \PY{n}{image}
        \PY{k+kn}{from} \PY{n+nn}{tqdm} \PY{k+kn}{import} \PY{n}{tqdm}
        \PY{k+kn}{from} \PY{n+nn}{keras.utils} \PY{k+kn}{import} \PY{n}{to\PYZus{}categorical}
        \PY{k}{def} \PY{n+nf}{read\PYZus{}video}\PY{p}{(}\PY{n}{path}\PY{p}{)}\PY{p}{:}
        
        
                \PY{n}{cap} \PY{o}{=} \PY{n}{FFmpegReader}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{n}{path}\PY{p}{)}
                \PY{n}{list\PYZus{}of\PYZus{}frames} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                \PY{n}{fps} \PY{o}{=} \PY{n+nb}{int}\PY{p}{(}\PY{n}{cap}\PY{o}{.}\PY{n}{inputfps}\PY{p}{)}                  \PY{c+c1}{\PYZsh{} Frame Rate}
                
                \PY{n}{frames\PYZus{}from\PYZus{}sec}\PY{o}{=}\PY{n+nb}{int}\PY{p}{(}\PY{n}{frames\PYZus{}needed}\PY{o}{/}\PY{n}{sec}\PY{p}{)}
                
                \PY{n}{frame\PYZus{}alternate}\PY{o}{=}\PY{n+nb}{int}\PY{p}{(}\PY{n}{fps}\PY{o}{/}\PY{n}{frames\PYZus{}from\PYZus{}sec}\PY{p}{)}
                \PY{n}{count} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{;}
                
                \PY{k}{for} \PY{n}{index}\PY{p}{,} \PY{n}{frame} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{cap}\PY{o}{.}\PY{n}{nextFrame}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                    \PY{n}{temp\PYZus{}image} \PY{o}{=} \PY{n}{image}\PY{o}{.}\PY{n}{array\PYZus{}to\PYZus{}img}\PY{p}{(}\PY{n}{frame}\PY{p}{)}
                    \PY{n}{frame} \PY{o}{=} \PY{n}{image}\PY{o}{.}\PY{n}{img\PYZus{}to\PYZus{}array}\PY{p}{(}
                                \PY{n}{temp\PYZus{}image}\PY{o}{.}\PY{n}{resize}\PY{p}{(}
                                    \PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{)}\PY{p}{,}
                                    \PY{n}{Image}\PY{o}{.}\PY{n}{ANTIALIAS}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{uint8}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                    \PY{k}{if} \PY{n}{index}\PY{o}{\PYZpc{}}\PY{k}{frame\PYZus{}alternate} == 0:
                        \PY{k}{if} \PY{n}{count} \PY{o}{\PYZlt{}} \PY{n}{frames\PYZus{}needed}\PY{p}{:}
                            \PY{n}{list\PYZus{}of\PYZus{}frames}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{frame}\PY{p}{)}\PY{p}{;}
                            \PY{n}{count}\PY{o}{=} \PY{n}{count}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{;}
        
                \PY{n}{temp\PYZus{}video} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{stack}\PY{p}{(}\PY{n}{list\PYZus{}of\PYZus{}frames}\PY{p}{)}
                
                \PY{c+c1}{\PYZsh{}\PYZsh{}Extract Specific Frames}
                
                \PY{n}{total\PYZus{}frames} \PY{o}{=} \PY{n}{temp\PYZus{}video}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
                
                
                
                \PY{n}{cap}\PY{o}{.}\PY{n}{close}\PY{p}{(}\PY{p}{)}
        
                \PY{n}{temp\PYZus{}video} \PY{o}{=} \PY{n}{rgb2gray}\PY{p}{(}\PY{n}{temp\PYZus{}video}\PY{p}{)}
                        
                \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{expand\PYZus{}dims}\PY{p}{(}\PY{n}{temp\PYZus{}video}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{k}{def} \PY{n+nf}{read\PYZus{}videos}\PY{p}{(}\PY{n}{paths}\PY{p}{)}\PY{p}{:}
          
                 \PY{n}{list\PYZus{}of\PYZus{}videos} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                 \PY{n}{list\PYZus{}of\PYZus{}videos} \PY{o}{=} \PY{p}{[}
                     \PY{n}{read\PYZus{}video}\PY{p}{(}\PY{n}{path}\PY{p}{)} \PY{k}{for} \PY{n}{path} \PY{o+ow}{in} \PY{n}{tqdm}\PY{p}{(}\PY{n}{paths}\PY{p}{)}
                 \PY{p}{]}
         
                 \PY{c+c1}{\PYZsh{}print(\PYZdq{}shape1:\PYZdq{}, list\PYZus{}of\PYZus{}videos[:].shape)}
                 \PY{n}{videos} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{vstack}\PY{p}{(}\PY{n}{list\PYZus{}of\PYZus{}videos}\PY{p}{)}
         
                 \PY{c+c1}{\PYZsh{}base = self.normalize\PYZus{}pixels[0]}
                 \PY{c+c1}{\PYZsh{}r = self.normalize\PYZus{}pixels[1] \PYZhy{} base}
                 \PY{n}{min\PYZus{}} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{n}{videos}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{keepdims}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}
                 \PY{n}{max\PYZus{}} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{videos}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{keepdims}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}
                 \PY{c+c1}{\PYZsh{}return ((videos.astype(\PYZsq{}float32\PYZsq{}) \PYZhy{} min\PYZus{}) / (max\PYZus{} \PYZhy{} min\PYZus{})) * r + base}
                 \PY{k}{return} \PY{p}{(}\PY{p}{(}\PY{n}{videos}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{float32}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{min\PYZus{}}\PY{p}{)} \PY{o}{/} \PY{p}{(}\PY{n}{max\PYZus{}} \PY{o}{\PYZhy{}} \PY{n}{min\PYZus{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{k}{def} \PY{n+nf}{read\PYZus{}videos\PYZus{}bgsub}\PY{p}{(}\PY{n}{paths}\PY{p}{,} \PY{n}{size}\PY{p}{)}\PY{p}{:}
              
                 \PY{n}{list\PYZus{}of\PYZus{}videos} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                 \PY{n}{list\PYZus{}of\PYZus{}videos} \PY{o}{=} \PY{p}{[}
                     \PY{n}{read\PYZus{}video\PYZus{}bgsub}\PY{p}{(}\PY{n}{path}\PY{p}{,} \PY{n}{size}\PY{p}{)} \PY{k}{for} \PY{n}{path} \PY{o+ow}{in} \PY{n}{tqdm}\PY{p}{(}\PY{n}{paths}\PY{p}{)}
                 \PY{p}{]}
         
                 \PY{c+c1}{\PYZsh{}print(\PYZdq{}shape1:\PYZdq{}, list\PYZus{}of\PYZus{}videos[:].shape)}
                 \PY{n}{videos} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{vstack}\PY{p}{(}\PY{n}{list\PYZus{}of\PYZus{}videos}\PY{p}{)}
         
                 \PY{c+c1}{\PYZsh{}base = self.normalize\PYZus{}pixels[0]}
                 \PY{c+c1}{\PYZsh{}r = self.normalize\PYZus{}pixels[1] \PYZhy{} base}
                 \PY{n}{min\PYZus{}} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{n}{videos}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{keepdims}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}
                 \PY{n}{max\PYZus{}} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{videos}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{keepdims}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}
                 \PY{c+c1}{\PYZsh{}return ((videos.astype(\PYZsq{}float32\PYZsq{}) \PYZhy{} min\PYZus{}) / (max\PYZus{} \PYZhy{} min\PYZus{})) * r + base}
                 \PY{k}{return} \PY{p}{(}\PY{p}{(}\PY{n}{videos}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{float32}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{min\PYZus{}}\PY{p}{)} \PY{o}{/} \PY{p}{(}\PY{n}{max\PYZus{}} \PY{o}{\PYZhy{}} \PY{n}{min\PYZus{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{c+c1}{\PYZsh{} Reading training videos and one\PYZhy{}hot encoding the training labels}
         \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{read\PYZus{}videos}\PY{p}{(}\PY{n}{train\PYZus{}files}\PY{p}{)}
         \PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{to\PYZus{}categorical}\PY{p}{(}\PY{n}{train\PYZus{}targets}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}
         \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape of training data:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
         \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape of training labels:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
100\%|██████████| 300/300 [01:55<00:00,  2.73it/s]
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
('Shape of training data:', (300, 35, 20, 20, 1))
('Shape of training labels:', (300, 6))

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]


    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{c+c1}{\PYZsh{} Reading validation videos and one\PYZhy{}hot encoding the validation labels}
         \PY{n}{X\PYZus{}valid} \PY{o}{=} \PY{n}{read\PYZus{}videos}\PY{p}{(}\PY{n}{valid\PYZus{}files}\PY{p}{)}
         
         \PY{n}{y\PYZus{}valid} \PY{o}{=} \PY{n}{to\PYZus{}categorical}\PY{p}{(}\PY{n}{valid\PYZus{}targets}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}
         \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape of validation data:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{X\PYZus{}valid}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
         \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape of validation labels:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y\PYZus{}valid}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
100\%|██████████| 122/122 [00:45<00:00,  2.70it/s]
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
('Shape of validation data:', (122, 35, 20, 20, 1))
('Shape of validation labels:', (122, 6))

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]


    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{c+c1}{\PYZsh{} Reading testing videos and one\PYZhy{}hot encoding the testing labels}
         \PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{read\PYZus{}videos}\PY{p}{(}\PY{n}{test\PYZus{}files}\PY{p}{)}
         \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{to\PYZus{}categorical}\PY{p}{(}\PY{n}{test\PYZus{}targets}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}
         \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape of testing data:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
         \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape of testing labels:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
100\%|██████████| 100/100 [00:37<00:00,  2.78it/s]
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
('Shape of testing data:', (100, 35, 20, 20, 1))
('Shape of testing labels:', (100, 6))

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]


    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{n}{X\PYZus{}train\PYZus{}bgsub} \PY{o}{=} \PY{n}{read\PYZus{}videos\PYZus{}bgsub}\PY{p}{(}\PY{n}{train\PYZus{}files}\PY{p}{,}\PY{l+m+mi}{20}\PY{p}{)}
         \PY{n}{y\PYZus{}train\PYZus{}bgsub} \PY{o}{=} \PY{n}{to\PYZus{}categorical}\PY{p}{(}\PY{n}{train\PYZus{}targets}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}
         \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape of training data:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{X\PYZus{}train\PYZus{}bgsub}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
         \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape of training labels:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y\PYZus{}train\PYZus{}bgsub}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
100\%|██████████| 300/300 [03:31<00:00,  1.45it/s]
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
('Shape of training data:', (300, 35, 20, 20, 1))
('Shape of training labels:', (300, 6))

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]


    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{n}{X\PYZus{}test\PYZus{}bgsub} \PY{o}{=} \PY{n}{read\PYZus{}videos\PYZus{}bgsub}\PY{p}{(}\PY{n}{test\PYZus{}files}\PY{p}{,}\PY{l+m+mi}{20}\PY{p}{)}
         \PY{n}{y\PYZus{}test\PYZus{}bgsub} \PY{o}{=} \PY{n}{to\PYZus{}categorical}\PY{p}{(}\PY{n}{test\PYZus{}targets}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}
         \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape of testing data:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{X\PYZus{}test\PYZus{}bgsub}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
         \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape of testing labels:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y\PYZus{}test\PYZus{}bgsub}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
100\%|██████████| 100/100 [01:08<00:00,  1.52it/s]
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
('Shape of testing data:', (100, 35, 20, 20, 1))
('Shape of testing labels:', (100, 6))

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]


    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{n}{X\PYZus{}valid\PYZus{}bgsub} \PY{o}{=} \PY{n}{read\PYZus{}videos\PYZus{}bgsub}\PY{p}{(}\PY{n}{valid\PYZus{}files}\PY{p}{,}\PY{l+m+mi}{20}\PY{p}{)}
         \PY{n}{y\PYZus{}valid\PYZus{}bgsub} \PY{o}{=} \PY{n}{to\PYZus{}categorical}\PY{p}{(}\PY{n}{valid\PYZus{}targets}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}
         \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape of validation data:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{X\PYZus{}valid\PYZus{}bgsub}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
         \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape of validation labels:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y\PYZus{}valid\PYZus{}bgsub}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
100\%|██████████| 122/122 [01:22<00:00,  1.51it/s]
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
('Shape of validation data:', (122, 35, 20, 20, 1))
('Shape of validation labels:', (122, 6))

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]


    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{k}{print}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}bgsub}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[[[[0.8410596]
   [0.8410596]
   [0.8410596]
   {\ldots}
   [0.8410596]
   [0.8410596]
   [0.8410596]]

  [[0.8410596]
   [0.8410596]
   [0.8410596]
   {\ldots}
   [0.8410596]
   [0.8410596]
   [0.8410596]]

  [[0.8410596]
   [0.8410596]
   [0.8410596]
   {\ldots}
   [0.8410596]
   [0.8410596]
   [0.8410596]]

  {\ldots}

  [[0.8410596]
   [0.8410596]
   [0.8410596]
   {\ldots}
   [0.8410596]
   [0.8410596]
   [0.8410596]]

  [[0.8410596]
   [0.8410596]
   [0.8410596]
   {\ldots}
   [0.8410596]
   [0.8410596]
   [0.8410596]]

  [[0.8410596]
   [0.8410596]
   [0.8410596]
   {\ldots}
   [0.8410596]
   [0.8410596]
   [0.8410596]]]


 [[[0.       ]
   [0.       ]
   [0.       ]
   {\ldots}
   [0.       ]
   [0.       ]
   [0.       ]]

  [[0.       ]
   [0.       ]
   [0.       ]
   {\ldots}
   [0.       ]
   [0.       ]
   [0.       ]]

  [[0.       ]
   [0.       ]
   [0.       ]
   {\ldots}
   [0.       ]
   [0.       ]
   [0.       ]]

  {\ldots}

  [[0.       ]
   [0.       ]
   [0.       ]
   {\ldots}
   [0.       ]
   [0.       ]
   [0.       ]]

  [[0.       ]
   [0.       ]
   [0.       ]
   {\ldots}
   [0.       ]
   [0.       ]
   [0.       ]]

  [[0.       ]
   [0.       ]
   [0.       ]
   {\ldots}
   [0.       ]
   [0.       ]
   [0.       ]]]


 [[[0.       ]
   [0.       ]
   [0.       ]
   {\ldots}
   [0.       ]
   [0.       ]
   [0.       ]]

  [[0.       ]
   [0.       ]
   [0.       ]
   {\ldots}
   [0.       ]
   [0.       ]
   [0.       ]]

  [[0.       ]
   [0.       ]
   [0.       ]
   {\ldots}
   [0.       ]
   [0.       ]
   [0.       ]]

  {\ldots}

  [[0.       ]
   [0.       ]
   [0.       ]
   {\ldots}
   [0.       ]
   [0.       ]
   [0.       ]]

  [[0.       ]
   [0.       ]
   [0.       ]
   {\ldots}
   [0.       ]
   [0.       ]
   [0.       ]]

  [[0.       ]
   [0.       ]
   [0.       ]
   {\ldots}
   [0.       ]
   [0.       ]
   [0.       ]]]


 {\ldots}


 [[[0.       ]
   [0.       ]
   [0.       ]
   {\ldots}
   [0.       ]
   [0.       ]
   [0.       ]]

  [[0.       ]
   [0.       ]
   [0.       ]
   {\ldots}
   [0.       ]
   [0.       ]
   [0.       ]]

  [[0.       ]
   [0.       ]
   [0.       ]
   {\ldots}
   [0.       ]
   [0.       ]
   [0.       ]]

  {\ldots}

  [[0.       ]
   [0.       ]
   [0.       ]
   {\ldots}
   [0.       ]
   [0.       ]
   [0.       ]]

  [[0.       ]
   [0.       ]
   [0.       ]
   {\ldots}
   [0.       ]
   [0.       ]
   [0.       ]]

  [[0.       ]
   [0.       ]
   [0.       ]
   {\ldots}
   [0.       ]
   [0.       ]
   [0.       ]]]


 [[[0.       ]
   [0.       ]
   [0.       ]
   {\ldots}
   [0.       ]
   [0.       ]
   [0.       ]]

  [[0.       ]
   [0.       ]
   [0.       ]
   {\ldots}
   [0.       ]
   [0.       ]
   [0.       ]]

  [[0.       ]
   [0.       ]
   [0.       ]
   {\ldots}
   [0.       ]
   [0.       ]
   [0.       ]]

  {\ldots}

  [[0.       ]
   [0.       ]
   [0.       ]
   {\ldots}
   [0.       ]
   [0.       ]
   [0.       ]]

  [[0.       ]
   [0.       ]
   [0.       ]
   {\ldots}
   [0.       ]
   [0.       ]
   [0.       ]]

  [[0.       ]
   [0.       ]
   [0.       ]
   {\ldots}
   [0.       ]
   [0.       ]
   [0.       ]]]


 [[[0.       ]
   [0.       ]
   [0.       ]
   {\ldots}
   [0.       ]
   [0.       ]
   [0.       ]]

  [[0.       ]
   [0.       ]
   [0.       ]
   {\ldots}
   [0.       ]
   [0.       ]
   [0.       ]]

  [[0.       ]
   [0.       ]
   [0.       ]
   {\ldots}
   [0.       ]
   [0.       ]
   [0.       ]]

  {\ldots}

  [[0.       ]
   [0.       ]
   [0.       ]
   {\ldots}
   [0.       ]
   [0.       ]
   [0.       ]]

  [[0.       ]
   [0.       ]
   [0.       ]
   {\ldots}
   [0.       ]
   [0.       ]
   [0.       ]]

  [[0.       ]
   [0.       ]
   [0.       ]
   {\ldots}
   [0.       ]
   [0.       ]
   [0.       ]]]]

    \end{Verbatim}

    \hypertarget{testcase-to-ensure-that-the-number-of-data-is-same-after-processing}{%
\subsection{Testcase to ensure that the number of data is same after
processing}\label{testcase-to-ensure-that-the-number-of-data-is-same-after-processing}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{c+c1}{\PYZsh{}print X\PYZus{}valid.shape[0]}
         
         \PY{n}{test\PYZus{}result}\PY{o}{=} \PY{n+nb+bp}{True}\PY{p}{;}
         
         \PY{k}{if} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{!=}\PY{n}{train\PYZus{}files}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{:}
             \PY{n}{test\PYZus{}result}\PY{o}{=} \PY{n+nb+bp}{False}\PY{p}{;}
             \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test failure: training data doesn}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t match with its input size}\PY{l+s+s2}{\PYZdq{}}
             
         \PY{k}{if} \PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{!=}\PY{n}{test\PYZus{}files}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{:}
             \PY{n}{test\PYZus{}result}\PY{o}{=} \PY{n+nb+bp}{False}\PY{p}{;}
             \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test failure: testing data doesn}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t match with its input size}\PY{l+s+s2}{\PYZdq{}}
         
         \PY{k}{if} \PY{n}{X\PYZus{}valid}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{!=}\PY{n}{valid\PYZus{}files}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{:}
             \PY{n}{test\PYZus{}result}\PY{o}{=} \PY{n+nb+bp}{False}\PY{p}{;}
             \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test failure: validation data doesn}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t match with its input size}\PY{l+s+s2}{\PYZdq{}}
             
         \PY{k}{if} \PY{n}{X\PYZus{}train\PYZus{}bgsub}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{!=}\PY{n}{train\PYZus{}files}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{:}
             \PY{n}{test\PYZus{}result}\PY{o}{=} \PY{n+nb+bp}{False}\PY{p}{;}
             \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test failure: training data doesn}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t match with its input size}\PY{l+s+s2}{\PYZdq{}}
             
         \PY{k}{if} \PY{n}{X\PYZus{}test\PYZus{}bgsub}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{!=}\PY{n}{test\PYZus{}files}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{:}
             \PY{n}{test\PYZus{}result}\PY{o}{=} \PY{n+nb+bp}{False}\PY{p}{;}
             \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test failure: testing data doesn}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t match with its input size}\PY{l+s+s2}{\PYZdq{}}
         
         \PY{k}{if} \PY{n}{X\PYZus{}valid\PYZus{}bgsub}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{!=}\PY{n}{valid\PYZus{}files}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{:}
             \PY{n}{test\PYZus{}result}\PY{o}{=} \PY{n+nb+bp}{False}\PY{p}{;}
             \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test failure: validation data doesn}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t match with its input size}\PY{l+s+s2}{\PYZdq{}}
             
         \PY{k}{if} \PY{n}{test\PYZus{}result} \PY{o}{==} \PY{n+nb+bp}{True}\PY{p}{:}
             \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test Passed}\PY{l+s+s2}{\PYZdq{}}
         \PY{k}{else}\PY{p}{:}
             \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test Failed}\PY{l+s+s2}{\PYZdq{}}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Test Passed

    \end{Verbatim}

    \hypertarget{testcase-to-ensure-that-the-number-of-data-is-same-after-processing}{%
\subsection{Testcase to ensure that the number of data is same after
processing}\label{testcase-to-ensure-that-the-number-of-data-is-same-after-processing}}

\hypertarget{negative-testcase-by-insering-a-redundant-video}{%
\subsubsection{Negative testcase by insering a redundant
video}\label{negative-testcase-by-insering-a-redundant-video}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{k+kn}{import} \PY{n+nn}{copy}
         
         \PY{n}{X\PYZus{}train2}\PY{o}{=}\PY{n}{copy}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{p}{;}
         \PY{n}{X\PYZus{}test2}\PY{o}{=}\PY{n}{copy}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{p}{;}
         \PY{n}{X\PYZus{}valid2}\PY{o}{=}\PY{n}{copy}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{n}{X\PYZus{}valid}\PY{p}{)}\PY{p}{;}
         
         \PY{n}{X\PYZus{}train\PYZus{}bgsub2}\PY{o}{=}\PY{n}{copy}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}bgsub}\PY{p}{)}\PY{p}{;}
         \PY{n}{X\PYZus{}test\PYZus{}bgsub2}\PY{o}{=}\PY{n}{copy}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}bgsub}\PY{p}{)}\PY{p}{;}
         \PY{n}{X\PYZus{}valid\PYZus{}bgsub2}\PY{o}{=}\PY{n}{copy}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{n}{X\PYZus{}valid\PYZus{}bgsub}\PY{p}{)}\PY{p}{;}
         
         \PY{c+c1}{\PYZsh{}print X\PYZus{}valid.shape[0]}
         
         \PY{n}{test\PYZus{}result}\PY{o}{=} \PY{n+nb+bp}{True}\PY{p}{;}
         
         \PY{n}{temp}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{expand\PYZus{}dims}\PY{p}{(}\PY{n}{X\PYZus{}train2}\PY{p}{[}\PY{l+m+mi}{20}\PY{p}{]}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}print temp.shape}
         
         
         \PY{n}{X\PYZus{}train2}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{vstack}\PY{p}{(}\PY{p}{[}\PY{n}{X\PYZus{}train2}\PY{p}{,}\PY{n}{temp}\PY{p}{]}\PY{p}{)}
         
         \PY{k}{print} \PY{n}{X\PYZus{}train2}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         
         \PY{k}{if} \PY{n}{X\PYZus{}train2}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{!=}\PY{n}{train\PYZus{}files}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{:}
             \PY{n}{test\PYZus{}result}\PY{o}{=} \PY{n+nb+bp}{False}\PY{p}{;}
             \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test failure: training data doesn}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t match with its input size}\PY{l+s+s2}{\PYZdq{}}
             
         \PY{k}{if} \PY{n}{X\PYZus{}test2}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{!=}\PY{n}{test\PYZus{}files}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{:}
             \PY{n}{test\PYZus{}result}\PY{o}{=} \PY{n+nb+bp}{False}\PY{p}{;}
             \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test failure: testing data doesn}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t match with its input size}\PY{l+s+s2}{\PYZdq{}}
         
         \PY{k}{if} \PY{n}{X\PYZus{}valid2}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{!=}\PY{n}{valid\PYZus{}files}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{:}
             \PY{n}{test\PYZus{}result}\PY{o}{=} \PY{n+nb+bp}{False}\PY{p}{;}
             \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test failure: validation data doesn}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t match with its input size}\PY{l+s+s2}{\PYZdq{}}
             
         \PY{k}{if} \PY{n}{X\PYZus{}train\PYZus{}bgsub2}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{!=}\PY{n}{train\PYZus{}files}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{:}
             \PY{n}{test\PYZus{}result}\PY{o}{=} \PY{n+nb+bp}{False}\PY{p}{;}
             \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test failure: training data doesn}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t match with its input size}\PY{l+s+s2}{\PYZdq{}}
             
         \PY{k}{if} \PY{n}{X\PYZus{}test\PYZus{}bgsub2}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{!=}\PY{n}{test\PYZus{}files}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{:}
             \PY{n}{test\PYZus{}result}\PY{o}{=} \PY{n+nb+bp}{False}\PY{p}{;}
             \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test failure: testing data doesn}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t match with its input size}\PY{l+s+s2}{\PYZdq{}}
         
         \PY{k}{if} \PY{n}{X\PYZus{}valid\PYZus{}bgsub2}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{!=}\PY{n}{valid\PYZus{}files}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{:}
             \PY{n}{test\PYZus{}result}\PY{o}{=} \PY{n+nb+bp}{False}\PY{p}{;}
             \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test failure: validation data doesn}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t match with its input size}\PY{l+s+s2}{\PYZdq{}}
             
         \PY{k}{if} \PY{n}{test\PYZus{}result} \PY{o}{==} \PY{n+nb+bp}{True}\PY{p}{:}
             \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test Passed}\PY{l+s+s2}{\PYZdq{}}
         \PY{k}{else}\PY{p}{:}
             \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test Failed}\PY{l+s+s2}{\PYZdq{}}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
301
Test failure: training data doesn't match with its input size
Test Failed

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{k+kn}{import} \PY{n+nn}{matplotlib.pyplot} \PY{k+kn}{as} \PY{n+nn}{plt}
         \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
         
         \PY{c+c1}{\PYZsh{} After pre\PYZhy{}processing}
         
         \PY{c+c1}{\PYZsh{} Displaying the 10th frame of the first processed video from the training data}
         \PY{k}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Class of 1st video:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{;}
         \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{10}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gray}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}bgsub}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{10}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gray}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
('Class of 1st video:', 2)

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_27_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}21}]:} <matplotlib.image.AxesImage at 0x7f267ca62150>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_27_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{model-1-without-bg-subtraction}{%
\section{Model 1 without BG
Subtraction}\label{model-1-without-bg-subtraction}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{c+c1}{\PYZsh{} Imports}
         \PY{k+kn}{from} \PY{n+nn}{keras.models} \PY{k+kn}{import} \PY{n}{Sequential}
         \PY{k+kn}{from} \PY{n+nn}{keras.layers} \PY{k+kn}{import} \PY{n}{Conv3D}\PY{p}{,} \PY{n}{MaxPooling3D}\PY{p}{,} \PY{n}{GlobalAveragePooling3D}
         \PY{k+kn}{from} \PY{n+nn}{keras.layers.core} \PY{k+kn}{import} \PY{n}{Dense}
         
         \PY{c+c1}{\PYZsh{} Using the Sequential Model}
         \PY{n}{model1} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Adding Alternate convolutional and pooling layers}
         \PY{n}{model1}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv3D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                          \PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{n}{model1}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling3D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{model1}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv3D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{model1}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling3D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         
         \PY{n}{model1}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{GlobalAveragePooling3D}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{model1}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{model1}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{model1}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
conv3d\_1 (Conv3D)            (None, 12, 20, 20, 16)    1456      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling3d\_1 (MaxPooling3 (None, 12, 10, 10, 16)    0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv3d\_2 (Conv3D)            (None, 12, 10, 10, 64)    46144     
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling3d\_2 (MaxPooling3 (None, 12, 5, 5, 64)      0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
global\_average\_pooling3d\_1 ( (None, 64)                0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_1 (Dense)              (None, 32)                2080      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_2 (Dense)              (None, 6)                 198       
=================================================================
Total params: 49,878
Trainable params: 49,878
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \hypertarget{training-model---1}{%
\subsubsection{Training Model - 1}\label{training-model---1}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{c+c1}{\PYZsh{} Imports}
         \PY{k+kn}{from} \PY{n+nn}{keras.callbacks} \PY{k+kn}{import} \PY{n}{ModelCheckpoint}
         
         \PY{c+c1}{\PYZsh{} Compiling the model}
         \PY{n}{model1}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{adam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Saving the model that performed the best on the validation set}
         \PY{n}{checkpoint1} \PY{o}{=} \PY{n}{ModelCheckpoint}\PY{p}{(}\PY{n}{filepath}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model\PYZus{}1.weights.best.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{save\PYZus{}best\PYZus{}only}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
         
         
         \PY{n}{history1} \PY{o}{=} \PY{n}{model1}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{60}\PY{p}{,} \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{X\PYZus{}valid}\PY{p}{,} \PY{n}{y\PYZus{}valid}\PY{p}{)}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{checkpoint1}\PY{p}{]}\PY{p}{)}
         \PY{c+c1}{\PYZsh{}history = model.fit(X\PYZus{}train, y\PYZus{}train, batch\PYZus{}size=16, epochs=40, verbose=2, callbacks=[checkpoint])}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Train on 300 samples, validate on 122 samples
Epoch 1/60
 - 7s - loss: 1.7896 - acc: 0.2133 - val\_loss: 1.7868 - val\_acc: 0.1475

Epoch 00001: val\_loss improved from inf to 1.78682, saving model to Model\_1.weights.best.hdf5
Epoch 2/60
 - 5s - loss: 1.7726 - acc: 0.2133 - val\_loss: 1.7919 - val\_acc: 0.1475

Epoch 00002: val\_loss did not improve from 1.78682
Epoch 3/60
 - 5s - loss: 1.7746 - acc: 0.2133 - val\_loss: 1.7872 - val\_acc: 0.1475

Epoch 00003: val\_loss did not improve from 1.78682
Epoch 4/60
 - 5s - loss: 1.7664 - acc: 0.2133 - val\_loss: 1.7914 - val\_acc: 0.1475

Epoch 00004: val\_loss did not improve from 1.78682
Epoch 5/60
 - 6s - loss: 1.7660 - acc: 0.2133 - val\_loss: 1.7928 - val\_acc: 0.1475

Epoch 00005: val\_loss did not improve from 1.78682
Epoch 6/60
 - 5s - loss: 1.7669 - acc: 0.2133 - val\_loss: 1.7854 - val\_acc: 0.1475

Epoch 00006: val\_loss improved from 1.78682 to 1.78538, saving model to Model\_1.weights.best.hdf5
Epoch 7/60
 - 5s - loss: 1.7595 - acc: 0.2167 - val\_loss: 1.7725 - val\_acc: 0.1475

Epoch 00007: val\_loss improved from 1.78538 to 1.77251, saving model to Model\_1.weights.best.hdf5
Epoch 8/60
 - 5s - loss: 1.7490 - acc: 0.2133 - val\_loss: 1.7542 - val\_acc: 0.1475

Epoch 00008: val\_loss improved from 1.77251 to 1.75424, saving model to Model\_1.weights.best.hdf5
Epoch 9/60
 - 5s - loss: 1.7199 - acc: 0.2333 - val\_loss: 1.7935 - val\_acc: 0.1230

Epoch 00009: val\_loss did not improve from 1.75424
Epoch 10/60
 - 5s - loss: 1.7056 - acc: 0.2300 - val\_loss: 1.6990 - val\_acc: 0.1967

Epoch 00010: val\_loss improved from 1.75424 to 1.69904, saving model to Model\_1.weights.best.hdf5
Epoch 11/60
 - 5s - loss: 1.6129 - acc: 0.2967 - val\_loss: 1.5990 - val\_acc: 0.2623

Epoch 00011: val\_loss improved from 1.69904 to 1.59896, saving model to Model\_1.weights.best.hdf5
Epoch 12/60
 - 5s - loss: 1.5294 - acc: 0.3200 - val\_loss: 1.5163 - val\_acc: 0.2705

Epoch 00012: val\_loss improved from 1.59896 to 1.51626, saving model to Model\_1.weights.best.hdf5
Epoch 13/60
 - 5s - loss: 1.3875 - acc: 0.3433 - val\_loss: 1.3762 - val\_acc: 0.2787

Epoch 00013: val\_loss improved from 1.51626 to 1.37617, saving model to Model\_1.weights.best.hdf5
Epoch 14/60
 - 5s - loss: 1.2963 - acc: 0.3200 - val\_loss: 1.2838 - val\_acc: 0.2951

Epoch 00014: val\_loss improved from 1.37617 to 1.28380, saving model to Model\_1.weights.best.hdf5
Epoch 15/60
 - 5s - loss: 1.2442 - acc: 0.3033 - val\_loss: 1.2362 - val\_acc: 0.3689

Epoch 00015: val\_loss improved from 1.28380 to 1.23618, saving model to Model\_1.weights.best.hdf5
Epoch 16/60
 - 5s - loss: 1.1978 - acc: 0.3500 - val\_loss: 1.2640 - val\_acc: 0.3279

Epoch 00016: val\_loss did not improve from 1.23618
Epoch 17/60
 - 5s - loss: 1.1959 - acc: 0.3567 - val\_loss: 1.2227 - val\_acc: 0.3279

Epoch 00017: val\_loss improved from 1.23618 to 1.22266, saving model to Model\_1.weights.best.hdf5
Epoch 18/60
 - 6s - loss: 1.1715 - acc: 0.3567 - val\_loss: 1.1869 - val\_acc: 0.4180

Epoch 00018: val\_loss improved from 1.22266 to 1.18690, saving model to Model\_1.weights.best.hdf5
Epoch 19/60
 - 5s - loss: 1.1383 - acc: 0.3567 - val\_loss: 1.1971 - val\_acc: 0.3361

Epoch 00019: val\_loss did not improve from 1.18690
Epoch 20/60
 - 5s - loss: 1.1399 - acc: 0.3467 - val\_loss: 1.1710 - val\_acc: 0.3852

Epoch 00020: val\_loss improved from 1.18690 to 1.17104, saving model to Model\_1.weights.best.hdf5
Epoch 21/60
 - 5s - loss: 1.1359 - acc: 0.2933 - val\_loss: 1.1434 - val\_acc: 0.4016

Epoch 00021: val\_loss improved from 1.17104 to 1.14342, saving model to Model\_1.weights.best.hdf5
Epoch 22/60
 - 5s - loss: 1.1296 - acc: 0.3700 - val\_loss: 1.2043 - val\_acc: 0.3361

Epoch 00022: val\_loss did not improve from 1.14342
Epoch 23/60
 - 5s - loss: 1.1258 - acc: 0.3633 - val\_loss: 1.1335 - val\_acc: 0.3770

Epoch 00023: val\_loss improved from 1.14342 to 1.13354, saving model to Model\_1.weights.best.hdf5
Epoch 24/60
 - 5s - loss: 1.1361 - acc: 0.2933 - val\_loss: 1.1234 - val\_acc: 0.3525

Epoch 00024: val\_loss improved from 1.13354 to 1.12340, saving model to Model\_1.weights.best.hdf5
Epoch 25/60
 - 5s - loss: 1.1399 - acc: 0.3200 - val\_loss: 1.1394 - val\_acc: 0.3443

Epoch 00025: val\_loss did not improve from 1.12340
Epoch 26/60
 - 5s - loss: 1.1134 - acc: 0.3333 - val\_loss: 1.1159 - val\_acc: 0.3852

Epoch 00026: val\_loss improved from 1.12340 to 1.11589, saving model to Model\_1.weights.best.hdf5
Epoch 27/60
 - 5s - loss: 1.1198 - acc: 0.3800 - val\_loss: 1.1645 - val\_acc: 0.3443

Epoch 00027: val\_loss did not improve from 1.11589
Epoch 28/60
 - 5s - loss: 1.1704 - acc: 0.3500 - val\_loss: 1.2018 - val\_acc: 0.2541

Epoch 00028: val\_loss did not improve from 1.11589
Epoch 29/60
 - 5s - loss: 1.1222 - acc: 0.4100 - val\_loss: 1.1382 - val\_acc: 0.2787

Epoch 00029: val\_loss did not improve from 1.11589
Epoch 30/60
 - 5s - loss: 1.1028 - acc: 0.3700 - val\_loss: 1.1220 - val\_acc: 0.3770

Epoch 00030: val\_loss did not improve from 1.11589
Epoch 31/60
 - 5s - loss: 1.1220 - acc: 0.3767 - val\_loss: 1.1272 - val\_acc: 0.3525

Epoch 00031: val\_loss did not improve from 1.11589
Epoch 32/60
 - 5s - loss: 1.1083 - acc: 0.3733 - val\_loss: 1.1254 - val\_acc: 0.3033

Epoch 00032: val\_loss did not improve from 1.11589
Epoch 33/60
 - 5s - loss: 1.1132 - acc: 0.3500 - val\_loss: 1.1307 - val\_acc: 0.3770

Epoch 00033: val\_loss did not improve from 1.11589
Epoch 34/60
 - 5s - loss: 1.1177 - acc: 0.3500 - val\_loss: 1.1211 - val\_acc: 0.3934

Epoch 00034: val\_loss did not improve from 1.11589
Epoch 35/60
 - 5s - loss: 1.0971 - acc: 0.3900 - val\_loss: 1.1087 - val\_acc: 0.4180

Epoch 00035: val\_loss improved from 1.11589 to 1.10866, saving model to Model\_1.weights.best.hdf5
Epoch 36/60
 - 5s - loss: 1.1001 - acc: 0.3833 - val\_loss: 1.1140 - val\_acc: 0.3443

Epoch 00036: val\_loss did not improve from 1.10866
Epoch 37/60
 - 5s - loss: 1.0935 - acc: 0.3900 - val\_loss: 1.1175 - val\_acc: 0.3770

Epoch 00037: val\_loss did not improve from 1.10866
Epoch 38/60
 - 5s - loss: 1.1311 - acc: 0.3700 - val\_loss: 1.1446 - val\_acc: 0.3443

Epoch 00038: val\_loss did not improve from 1.10866
Epoch 39/60
 - 5s - loss: 1.1116 - acc: 0.3667 - val\_loss: 1.2094 - val\_acc: 0.2869

Epoch 00039: val\_loss did not improve from 1.10866
Epoch 40/60
 - 5s - loss: 1.1096 - acc: 0.3500 - val\_loss: 1.1151 - val\_acc: 0.3852

Epoch 00040: val\_loss did not improve from 1.10866
Epoch 41/60
 - 5s - loss: 1.0873 - acc: 0.3500 - val\_loss: 1.1414 - val\_acc: 0.3361

Epoch 00041: val\_loss did not improve from 1.10866
Epoch 42/60
 - 5s - loss: 1.0833 - acc: 0.4100 - val\_loss: 1.1318 - val\_acc: 0.3770

Epoch 00042: val\_loss did not improve from 1.10866
Epoch 43/60
 - 5s - loss: 1.0937 - acc: 0.3800 - val\_loss: 1.0951 - val\_acc: 0.3689

Epoch 00043: val\_loss improved from 1.10866 to 1.09508, saving model to Model\_1.weights.best.hdf5
Epoch 44/60
 - 5s - loss: 1.0875 - acc: 0.3900 - val\_loss: 1.0971 - val\_acc: 0.3689

Epoch 00044: val\_loss did not improve from 1.09508
Epoch 45/60
 - 5s - loss: 1.0898 - acc: 0.3733 - val\_loss: 1.1033 - val\_acc: 0.3770

Epoch 00045: val\_loss did not improve from 1.09508
Epoch 46/60
 - 5s - loss: 1.0833 - acc: 0.3633 - val\_loss: 1.0892 - val\_acc: 0.4180

Epoch 00046: val\_loss improved from 1.09508 to 1.08925, saving model to Model\_1.weights.best.hdf5
Epoch 47/60
 - 5s - loss: 1.0704 - acc: 0.3867 - val\_loss: 1.0775 - val\_acc: 0.4344

Epoch 00047: val\_loss improved from 1.08925 to 1.07750, saving model to Model\_1.weights.best.hdf5
Epoch 48/60
 - 5s - loss: 1.0701 - acc: 0.4200 - val\_loss: 1.1788 - val\_acc: 0.3115

Epoch 00048: val\_loss did not improve from 1.07750
Epoch 49/60
 - 5s - loss: 1.1028 - acc: 0.3900 - val\_loss: 1.0923 - val\_acc: 0.3934

Epoch 00049: val\_loss did not improve from 1.07750
Epoch 50/60
 - 5s - loss: 1.0820 - acc: 0.3667 - val\_loss: 1.0882 - val\_acc: 0.4344

Epoch 00050: val\_loss did not improve from 1.07750
Epoch 51/60
 - 5s - loss: 1.0685 - acc: 0.4500 - val\_loss: 1.0757 - val\_acc: 0.4590

Epoch 00051: val\_loss improved from 1.07750 to 1.07568, saving model to Model\_1.weights.best.hdf5
Epoch 52/60
 - 5s - loss: 1.0543 - acc: 0.4200 - val\_loss: 1.0736 - val\_acc: 0.3934

Epoch 00052: val\_loss improved from 1.07568 to 1.07358, saving model to Model\_1.weights.best.hdf5
Epoch 53/60
 - 5s - loss: 1.0665 - acc: 0.4400 - val\_loss: 1.0770 - val\_acc: 0.4262

Epoch 00053: val\_loss did not improve from 1.07358
Epoch 54/60
 - 5s - loss: 1.0616 - acc: 0.4267 - val\_loss: 1.0899 - val\_acc: 0.3607

Epoch 00054: val\_loss did not improve from 1.07358
Epoch 55/60
 - 5s - loss: 1.0525 - acc: 0.4633 - val\_loss: 1.0682 - val\_acc: 0.4590

Epoch 00055: val\_loss improved from 1.07358 to 1.06820, saving model to Model\_1.weights.best.hdf5
Epoch 56/60
 - 5s - loss: 1.0451 - acc: 0.4300 - val\_loss: 1.0795 - val\_acc: 0.4672

Epoch 00056: val\_loss did not improve from 1.06820
Epoch 57/60
 - 5s - loss: 1.0677 - acc: 0.3933 - val\_loss: 1.1060 - val\_acc: 0.4016

Epoch 00057: val\_loss did not improve from 1.06820
Epoch 58/60
 - 5s - loss: 1.0524 - acc: 0.4000 - val\_loss: 1.0683 - val\_acc: 0.4754

Epoch 00058: val\_loss did not improve from 1.06820
Epoch 59/60
 - 5s - loss: 1.0399 - acc: 0.4567 - val\_loss: 1.0780 - val\_acc: 0.3770

Epoch 00059: val\_loss did not improve from 1.06820
Epoch 60/60
 - 5s - loss: 1.0538 - acc: 0.3967 - val\_loss: 1.0728 - val\_acc: 0.4836

Epoch 00060: val\_loss did not improve from 1.06820

    \end{Verbatim}

    \hypertarget{evaluating-model---1}{%
\subsubsection{Evaluating Model - 1}\label{evaluating-model---1}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} \PY{c+c1}{\PYZsh{} Loading the model that performed the best on the validation set}
         \PY{n}{model1}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model\PYZus{}1.weights.best.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Testing the model on the Test data}
         \PY{p}{(}\PY{n}{loss1}\PY{p}{,} \PY{n}{accuracy1}\PY{p}{)} \PY{o}{=} \PY{n}{model1}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
         
         \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy on test data: \PYZob{}:.2f\PYZcb{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracy1} \PY{o}{*} \PY{l+m+mi}{100}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy on test data: 41.00\%

    \end{Verbatim}

    \hypertarget{model---1-performance}{%
\subsection{Model - 1 Performance}\label{model---1-performance}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} \PY{c+c1}{\PYZsh{} Making the plot larger}
         \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{loss1} \PY{o}{=} \PY{n}{history1}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}                          \PY{c+c1}{\PYZsh{} Loss on the training data}
         \PY{n}{val\PYZus{}loss1} \PY{o}{=} \PY{n}{history1}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}                  \PY{c+c1}{\PYZsh{} Loss on the validation data}
         \PY{n}{epochs} \PY{o}{=} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{61}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{loss1}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ro\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{val\PYZus{}loss1}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{go\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}25}]:} <matplotlib.legend.Legend at 0x7f266889c710>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_35_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn.metrics} \PY{k+kn}{import} \PY{n}{confusion\PYZus{}matrix}
         
         \PY{k}{def} \PY{n+nf}{plot\PYZus{}confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{cm}\PY{p}{,} \PY{n}{classes}\PY{p}{,}
                                   \PY{n}{normalize}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{,}
                                   \PY{n}{title}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Confusion matrix}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                                   \PY{n}{cmap}\PY{o}{=}\PY{n}{plt}\PY{o}{.}\PY{n}{cm}\PY{o}{.}\PY{n}{GnBu}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    This function prints and plots the confusion matrix.}
         \PY{l+s+sd}{    Normalization can be applied by setting `normalize=True`.}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{k}{if} \PY{n}{normalize}\PY{p}{:}
                 \PY{n}{cm} \PY{o}{=} \PY{n}{cm}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{float}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{o}{/} \PY{n}{cm}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{newaxis}\PY{p}{]}
                 \PY{k}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Normalized confusion matrix}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             \PY{k}{else}\PY{p}{:}
                 \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Confusion matrix, without normalization}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{}print(cm)}
         
             \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{cm}\PY{p}{,} \PY{n}{interpolation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{nearest}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{n}{cmap}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{n}{title}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{colorbar}\PY{p}{(}\PY{p}{)}
             \PY{n}{tick\PYZus{}marks} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{classes}\PY{p}{)}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{n}{tick\PYZus{}marks}\PY{p}{,} \PY{n}{classes}\PY{p}{,} \PY{n}{rotation}\PY{o}{=}\PY{l+m+mi}{45}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{yticks}\PY{p}{(}\PY{n}{tick\PYZus{}marks}\PY{p}{,} \PY{n}{classes}\PY{p}{)}
         
             \PY{n}{fmt} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.2f}\PY{l+s+s1}{\PYZsq{}} \PY{k}{if} \PY{n}{normalize} \PY{k}{else} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{d}\PY{l+s+s1}{\PYZsq{}}
             \PY{n}{thresh} \PY{o}{=} \PY{n}{cm}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)} \PY{o}{/} \PY{l+m+mf}{2.}
             \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{j} \PY{o+ow}{in} \PY{n}{itertools}\PY{o}{.}\PY{n}{product}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n}{cm}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n+nb}{range}\PY{p}{(}\PY{n}{cm}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                 \PY{n}{plt}\PY{o}{.}\PY{n}{text}\PY{p}{(}\PY{n}{j}\PY{p}{,} \PY{n}{i}\PY{p}{,} \PY{n}{format}\PY{p}{(}\PY{n}{cm}\PY{p}{[}\PY{n}{i}\PY{p}{,} \PY{n}{j}\PY{p}{]}\PY{p}{,} \PY{n}{fmt}\PY{p}{)}\PY{p}{,}
                          \PY{n}{horizontalalignment}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{center}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
                          \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{white}\PY{l+s+s2}{\PYZdq{}} \PY{k}{if} \PY{n}{cm}\PY{p}{[}\PY{n}{i}\PY{p}{,} \PY{n}{j}\PY{p}{]} \PY{o}{\PYZgt{}} \PY{n}{thresh} \PY{k}{else} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{black}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         
             \PY{n}{plt}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{True label}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Predicted label}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Compute confusion matrix}
         \PY{c+c1}{\PYZsh{}cnf\PYZus{}matrix = confusion\PYZus{}matrix(y\PYZus{}test, y\PYZus{}pred)}
         \PY{c+c1}{\PYZsh{}np.set\PYZus{}printoptions(precision=2)}
         
         \PY{c+c1}{\PYZsh{} Plot non\PYZhy{}normalized confusion matrix}
         \PY{c+c1}{\PYZsh{}plt.figure()}
         \PY{c+c1}{\PYZsh{}plot\PYZus{}confusion\PYZus{}matrix(cnf\PYZus{}matrix, classes=class\PYZus{}names,}
         \PY{c+c1}{\PYZsh{}                      title=\PYZsq{}Confusion matrix, without normalization\PYZsq{})}
         
         \PY{c+c1}{\PYZsh{} Plot normalized confusion matrix}
         \PY{c+c1}{\PYZsh{}plt.figure()}
         \PY{c+c1}{\PYZsh{}plot\PYZus{}confusion\PYZus{}matrix(cnf\PYZus{}matrix, classes=class\PYZus{}names, normalize=True,}
         \PY{c+c1}{\PYZsh{}                      title=\PYZsq{}Normalized confusion matrix\PYZsq{})}
         
         \PY{c+c1}{\PYZsh{}plt.show()}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn.metrics} \PY{k+kn}{import} \PY{n}{confusion\PYZus{}matrix}
         \PY{k+kn}{import} \PY{n+nn}{itertools}
         \PY{n}{y\PYZus{}predictions} \PY{o}{=} \PY{n}{model1}\PY{o}{.}\PY{n}{predict\PYZus{}classes}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{p}{;}
         \PY{n}{y\PYZus{}pred}\PY{o}{=}\PY{n}{to\PYZus{}categorical}\PY{p}{(}\PY{n}{y\PYZus{}predictions}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{;}
         \PY{n}{y\PYZus{}test\PYZus{}cm} \PY{o}{=} \PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{;}
         \PY{c+c1}{\PYZsh{}print y\PYZus{}predictions[:10]}
         \PY{c+c1}{\PYZsh{}print y\PYZus{}test[:10]}
         \PY{n}{model\PYZus{}cnf\PYZus{}matrix} \PY{o}{=} \PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{y\PYZus{}test\PYZus{}cm}\PY{p}{,}\PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{;}
         \PY{n}{confusion\PYZus{}matrix\PYZus{}plot} \PY{o}{=} \PY{n}{plot\PYZus{}confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{model\PYZus{}cnf\PYZus{}matrix}\PY{p}{,} 
                                                       \PY{n}{classes}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{boxing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{handclapping}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{handwaving}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{jogging}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{running}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{walking}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} 
                                                       \PY{n}{normalize}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Normalized confusion matrix

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_37_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{test-case-to-ensure-that-predicted-values-are-within-the-available-class}{%
\subsection{Test Case to ensure that predicted values are within the
available
class}\label{test-case-to-ensure-that-predicted-values-are-within-the-available-class}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}28}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn.metrics} \PY{k+kn}{import} \PY{n}{confusion\PYZus{}matrix}
         \PY{k+kn}{import} \PY{n+nn}{itertools}
         \PY{n}{y\PYZus{}predictions} \PY{o}{=} \PY{n}{model1}\PY{o}{.}\PY{n}{predict\PYZus{}classes}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{p}{;}
         \PY{n}{y\PYZus{}pred}\PY{o}{=}\PY{n}{to\PYZus{}categorical}\PY{p}{(}\PY{n}{y\PYZus{}predictions}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{;}
         
         \PY{c+c1}{\PYZsh{}print y\PYZus{}predictions[:10]}
         
         \PY{n}{no\PYZus{}of\PYZus{}classes} \PY{o}{=}\PY{l+m+mi}{6}\PY{p}{;}
         \PY{n}{test\PYZus{}result}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{;}
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{y\PYZus{}predictions}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} THe mappings of targets are from 0\PYZhy{}(number of classes \PYZhy{}1)}
             \PY{k}{if} \PY{n}{i}\PY{o}{\PYZgt{}}\PY{n}{no\PYZus{}of\PYZus{}classes}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}
                 \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test Failed: Mapping has exceeded the permissible mapping of the prediction values}\PY{l+s+s2}{\PYZdq{}}
                 \PY{n}{test\PYZus{}result}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{;}
         
         \PY{k}{if} \PY{n}{test\PYZus{}result}\PY{o}{==} \PY{n+nb+bp}{True}\PY{p}{:}
             \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test Successful}\PY{l+s+s2}{\PYZdq{}}
         \PY{k}{else}\PY{p}{:}
             \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test Failed}\PY{l+s+s2}{\PYZdq{}}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Test Successful

    \end{Verbatim}

    \hypertarget{negative-test}{%
\subsection{Negative Test}\label{negative-test}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}29}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn.metrics} \PY{k+kn}{import} \PY{n}{confusion\PYZus{}matrix}
         \PY{k+kn}{import} \PY{n+nn}{itertools}
         \PY{k+kn}{import} \PY{n+nn}{copy}
         \PY{n}{y\PYZus{}predictions} \PY{o}{=} \PY{n}{model1}\PY{o}{.}\PY{n}{predict\PYZus{}classes}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{p}{;}
         \PY{n}{y\PYZus{}pred}\PY{o}{=}\PY{n}{to\PYZus{}categorical}\PY{p}{(}\PY{n}{y\PYZus{}predictions}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{;}
         
         \PY{c+c1}{\PYZsh{}print y\PYZus{}predictions[:10]}
         
         \PY{n}{y\PYZus{}predictions2}\PY{o}{=}\PY{n}{copy}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{n}{y\PYZus{}predictions}\PY{p}{)}
         \PY{n}{y\PYZus{}predictions2}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{=}\PY{l+m+mi}{8}
         
         \PY{n}{no\PYZus{}of\PYZus{}classes} \PY{o}{=}\PY{l+m+mi}{6}\PY{p}{;}
         \PY{n}{test\PYZus{}result}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{;}
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{y\PYZus{}predictions2}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} THe mappings of targets are from 0\PYZhy{}(number of classes \PYZhy{}1)}
             \PY{k}{if} \PY{n}{i}\PY{o}{\PYZgt{}}\PY{n}{no\PYZus{}of\PYZus{}classes}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}
                 \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test Failed: Mapping has exceeded the permissible mapping of the prediction values}\PY{l+s+s2}{\PYZdq{}}
                 \PY{n}{test\PYZus{}result}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{;}
         
         \PY{k}{if} \PY{n}{test\PYZus{}result}\PY{o}{==} \PY{n+nb+bp}{True}\PY{p}{:}
             \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test Successful}\PY{l+s+s2}{\PYZdq{}}
         \PY{k}{else}\PY{p}{:}
             \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test Failed}\PY{l+s+s2}{\PYZdq{}}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Test Failed: Mapping has exceeded the permissible mapping of the prediction values
Test Failed

    \end{Verbatim}

    \hypertarget{model-1-with-bg-subtraction}{%
\section{Model 1, With BG
Subtraction}\label{model-1-with-bg-subtraction}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}30}]:} \PY{c+c1}{\PYZsh{} Imports}
         \PY{k+kn}{from} \PY{n+nn}{keras.models} \PY{k+kn}{import} \PY{n}{Sequential}
         \PY{k+kn}{from} \PY{n+nn}{keras.layers} \PY{k+kn}{import} \PY{n}{Conv3D}\PY{p}{,} \PY{n}{MaxPooling3D}\PY{p}{,} \PY{n}{GlobalAveragePooling3D}
         \PY{k+kn}{from} \PY{n+nn}{keras.layers.core} \PY{k+kn}{import} \PY{n}{Dense}
         
         \PY{c+c1}{\PYZsh{} Using the Sequential Model}
         \PY{n}{model1\PYZus{}bg} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Adding Alternate convolutional and pooling layers}
         \PY{n}{model1\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv3D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                          \PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{n}{model1\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling3D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{model1\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv3D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{model1\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling3D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         
         \PY{n}{model1\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{GlobalAveragePooling3D}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{model1\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{model1\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{model1\PYZus{}bg}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
conv3d\_3 (Conv3D)            (None, 12, 20, 20, 16)    1456      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling3d\_3 (MaxPooling3 (None, 12, 10, 10, 16)    0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv3d\_4 (Conv3D)            (None, 12, 10, 10, 64)    46144     
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling3d\_4 (MaxPooling3 (None, 12, 5, 5, 64)      0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
global\_average\_pooling3d\_2 ( (None, 64)                0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_3 (Dense)              (None, 32)                2080      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_4 (Dense)              (None, 6)                 198       
=================================================================
Total params: 49,878
Trainable params: 49,878
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}31}]:} \PY{k+kn}{from} \PY{n+nn}{keras.utils} \PY{k+kn}{import} \PY{n}{plot\PYZus{}model}
         \PY{k+kn}{import} \PY{n+nn}{pydot}
         \PY{n}{plot\PYZus{}model}\PY{p}{(}\PY{n}{model1\PYZus{}bg}\PY{p}{,} \PY{n}{to\PYZus{}file}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{model.png}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{show\PYZus{}shapes}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}32}]:} \PY{k+kn}{from} \PY{n+nn}{IPython.display} \PY{k+kn}{import} \PY{n}{SVG}
         \PY{k+kn}{from} \PY{n+nn}{keras.utils.vis\PYZus{}utils} \PY{k+kn}{import} \PY{n}{model\PYZus{}to\PYZus{}dot}
         
         \PY{n}{SVG}\PY{p}{(}\PY{n}{model\PYZus{}to\PYZus{}dot}\PY{p}{(}\PY{n}{model1\PYZus{}bg}\PY{p}{)}\PY{o}{.}\PY{n}{create}\PY{p}{(}\PY{n}{prog}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dot}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{format}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{svg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}32}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_45_0.pdf}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}33}]:} \PY{c+c1}{\PYZsh{} Imports}
         \PY{k+kn}{from} \PY{n+nn}{keras.callbacks} \PY{k+kn}{import} \PY{n}{ModelCheckpoint}
         
         \PY{c+c1}{\PYZsh{} Compiling the model}
         \PY{n}{model1\PYZus{}bg}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{adam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Saving the model that performed the best on the validation set}
         \PY{n}{checkpoint\PYZus{}bg} \PY{o}{=} \PY{n}{ModelCheckpoint}\PY{p}{(}\PY{n}{filepath}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model\PYZus{}1\PYZus{}bg.weights.best.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{save\PYZus{}best\PYZus{}only}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
         
         
         \PY{n}{history1\PYZus{}bg} \PY{o}{=} \PY{n}{model1\PYZus{}bg}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}bgsub}\PY{p}{,} \PY{n}{y\PYZus{}train\PYZus{}bgsub}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{60}\PY{p}{,} \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{X\PYZus{}valid\PYZus{}bgsub}\PY{p}{,} \PY{n}{y\PYZus{}valid\PYZus{}bgsub}\PY{p}{)}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{checkpoint\PYZus{}bg}\PY{p}{]}\PY{p}{)}
         \PY{c+c1}{\PYZsh{}history = model.fit(X\PYZus{}train, y\PYZus{}train, batch\PYZus{}size=16, epochs=40, verbose=2, callbacks=[checkpoint])}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Train on 300 samples, validate on 122 samples
Epoch 1/60
300/300 [==============================] - 6s 20ms/step - loss: 1.7516 - acc: 0.2000 - val\_loss: 1.7093 - val\_acc: 0.1475

Epoch 00001: val\_loss improved from inf to 1.70934, saving model to Model\_1\_bg.weights.best.hdf5
Epoch 2/60
300/300 [==============================] - 6s 19ms/step - loss: 1.6601 - acc: 0.2133 - val\_loss: 1.6429 - val\_acc: 0.1803

Epoch 00002: val\_loss improved from 1.70934 to 1.64287, saving model to Model\_1\_bg.weights.best.hdf5
Epoch 3/60
300/300 [==============================] - 6s 19ms/step - loss: 1.6033 - acc: 0.2600 - val\_loss: 1.5769 - val\_acc: 0.1803

Epoch 00003: val\_loss improved from 1.64287 to 1.57686, saving model to Model\_1\_bg.weights.best.hdf5
Epoch 4/60
300/300 [==============================] - 6s 19ms/step - loss: 1.5485 - acc: 0.2333 - val\_loss: 1.5285 - val\_acc: 0.1967

Epoch 00004: val\_loss improved from 1.57686 to 1.52848, saving model to Model\_1\_bg.weights.best.hdf5
Epoch 5/60
300/300 [==============================] - 6s 18ms/step - loss: 1.4737 - acc: 0.3200 - val\_loss: 1.4298 - val\_acc: 0.4590

Epoch 00005: val\_loss improved from 1.52848 to 1.42984, saving model to Model\_1\_bg.weights.best.hdf5
Epoch 6/60
300/300 [==============================] - 5s 18ms/step - loss: 1.3987 - acc: 0.3733 - val\_loss: 1.3610 - val\_acc: 0.4262

Epoch 00006: val\_loss improved from 1.42984 to 1.36096, saving model to Model\_1\_bg.weights.best.hdf5
Epoch 7/60
300/300 [==============================] - 6s 19ms/step - loss: 1.2650 - acc: 0.4500 - val\_loss: 1.1791 - val\_acc: 0.5082

Epoch 00007: val\_loss improved from 1.36096 to 1.17913, saving model to Model\_1\_bg.weights.best.hdf5
Epoch 8/60
300/300 [==============================] - 6s 19ms/step - loss: 1.1641 - acc: 0.4967 - val\_loss: 1.0939 - val\_acc: 0.5246

Epoch 00008: val\_loss improved from 1.17913 to 1.09394, saving model to Model\_1\_bg.weights.best.hdf5
Epoch 9/60
300/300 [==============================] - 5s 18ms/step - loss: 1.0254 - acc: 0.5733 - val\_loss: 1.1789 - val\_acc: 0.4590

Epoch 00009: val\_loss did not improve from 1.09394
Epoch 10/60
300/300 [==============================] - 6s 19ms/step - loss: 0.9767 - acc: 0.5967 - val\_loss: 1.0932 - val\_acc: 0.4672

Epoch 00010: val\_loss improved from 1.09394 to 1.09322, saving model to Model\_1\_bg.weights.best.hdf5
Epoch 11/60
300/300 [==============================] - 5s 18ms/step - loss: 0.8788 - acc: 0.6133 - val\_loss: 1.0536 - val\_acc: 0.5820

Epoch 00011: val\_loss improved from 1.09322 to 1.05357, saving model to Model\_1\_bg.weights.best.hdf5
Epoch 12/60
300/300 [==============================] - 6s 19ms/step - loss: 0.8315 - acc: 0.6267 - val\_loss: 0.9677 - val\_acc: 0.5902

Epoch 00012: val\_loss improved from 1.05357 to 0.96775, saving model to Model\_1\_bg.weights.best.hdf5
Epoch 13/60
300/300 [==============================] - 6s 19ms/step - loss: 0.7849 - acc: 0.6733 - val\_loss: 0.8744 - val\_acc: 0.6148

Epoch 00013: val\_loss improved from 0.96775 to 0.87437, saving model to Model\_1\_bg.weights.best.hdf5
Epoch 14/60
300/300 [==============================] - 5s 18ms/step - loss: 0.7537 - acc: 0.6900 - val\_loss: 0.8946 - val\_acc: 0.5984

Epoch 00014: val\_loss did not improve from 0.87437
Epoch 15/60
300/300 [==============================] - 6s 19ms/step - loss: 0.7878 - acc: 0.6533 - val\_loss: 0.8426 - val\_acc: 0.6311

Epoch 00015: val\_loss improved from 0.87437 to 0.84256, saving model to Model\_1\_bg.weights.best.hdf5
Epoch 16/60
300/300 [==============================] - 6s 19ms/step - loss: 0.7533 - acc: 0.6867 - val\_loss: 0.7379 - val\_acc: 0.6557

Epoch 00016: val\_loss improved from 0.84256 to 0.73790, saving model to Model\_1\_bg.weights.best.hdf5
Epoch 17/60
300/300 [==============================] - 6s 19ms/step - loss: 0.7087 - acc: 0.6933 - val\_loss: 0.9593 - val\_acc: 0.6148

Epoch 00017: val\_loss did not improve from 0.73790
Epoch 18/60
300/300 [==============================] - 6s 19ms/step - loss: 0.6904 - acc: 0.7233 - val\_loss: 0.7841 - val\_acc: 0.6557

Epoch 00018: val\_loss did not improve from 0.73790
Epoch 19/60
300/300 [==============================] - 6s 18ms/step - loss: 0.6690 - acc: 0.6767 - val\_loss: 0.7818 - val\_acc: 0.6639

Epoch 00019: val\_loss did not improve from 0.73790
Epoch 20/60
300/300 [==============================] - 6s 19ms/step - loss: 0.6061 - acc: 0.7533 - val\_loss: 0.7644 - val\_acc: 0.6639

Epoch 00020: val\_loss did not improve from 0.73790
Epoch 21/60
300/300 [==============================] - 5s 18ms/step - loss: 0.6290 - acc: 0.7300 - val\_loss: 1.0916 - val\_acc: 0.5574

Epoch 00021: val\_loss did not improve from 0.73790
Epoch 22/60
300/300 [==============================] - 6s 19ms/step - loss: 0.6114 - acc: 0.7333 - val\_loss: 0.8199 - val\_acc: 0.6639

Epoch 00022: val\_loss did not improve from 0.73790
Epoch 23/60
300/300 [==============================] - 6s 19ms/step - loss: 0.5954 - acc: 0.7500 - val\_loss: 0.7378 - val\_acc: 0.6967

Epoch 00023: val\_loss improved from 0.73790 to 0.73780, saving model to Model\_1\_bg.weights.best.hdf5
Epoch 24/60
300/300 [==============================] - 6s 19ms/step - loss: 0.5640 - acc: 0.7667 - val\_loss: 0.7242 - val\_acc: 0.6639

Epoch 00024: val\_loss improved from 0.73780 to 0.72424, saving model to Model\_1\_bg.weights.best.hdf5
Epoch 25/60
300/300 [==============================] - 6s 19ms/step - loss: 0.5454 - acc: 0.7600 - val\_loss: 0.7017 - val\_acc: 0.6803

Epoch 00025: val\_loss improved from 0.72424 to 0.70169, saving model to Model\_1\_bg.weights.best.hdf5
Epoch 26/60
300/300 [==============================] - 6s 19ms/step - loss: 0.5243 - acc: 0.7900 - val\_loss: 0.8887 - val\_acc: 0.6557

Epoch 00026: val\_loss did not improve from 0.70169
Epoch 27/60
300/300 [==============================] - 6s 19ms/step - loss: 0.5591 - acc: 0.7667 - val\_loss: 0.7743 - val\_acc: 0.6557

Epoch 00027: val\_loss did not improve from 0.70169
Epoch 28/60
300/300 [==============================] - 6s 19ms/step - loss: 0.5175 - acc: 0.7867 - val\_loss: 0.7623 - val\_acc: 0.6639

Epoch 00028: val\_loss did not improve from 0.70169
Epoch 29/60
300/300 [==============================] - 6s 19ms/step - loss: 0.6116 - acc: 0.7133 - val\_loss: 0.7868 - val\_acc: 0.6967

Epoch 00029: val\_loss did not improve from 0.70169
Epoch 30/60
300/300 [==============================] - 6s 19ms/step - loss: 0.5173 - acc: 0.8233 - val\_loss: 0.7578 - val\_acc: 0.6885

Epoch 00030: val\_loss did not improve from 0.70169
Epoch 31/60
300/300 [==============================] - 5s 18ms/step - loss: 0.4909 - acc: 0.7933 - val\_loss: 0.7854 - val\_acc: 0.6639

Epoch 00031: val\_loss did not improve from 0.70169
Epoch 32/60
300/300 [==============================] - 5s 18ms/step - loss: 0.4958 - acc: 0.8233 - val\_loss: 0.7876 - val\_acc: 0.6721

Epoch 00032: val\_loss did not improve from 0.70169
Epoch 33/60
300/300 [==============================] - 6s 19ms/step - loss: 0.4927 - acc: 0.7967 - val\_loss: 0.7157 - val\_acc: 0.6721

Epoch 00033: val\_loss did not improve from 0.70169
Epoch 34/60
300/300 [==============================] - 6s 18ms/step - loss: 0.4991 - acc: 0.7867 - val\_loss: 0.6958 - val\_acc: 0.7377

Epoch 00034: val\_loss improved from 0.70169 to 0.69584, saving model to Model\_1\_bg.weights.best.hdf5
Epoch 35/60
300/300 [==============================] - 6s 19ms/step - loss: 0.4379 - acc: 0.8233 - val\_loss: 0.7185 - val\_acc: 0.7213

Epoch 00035: val\_loss did not improve from 0.69584
Epoch 36/60
300/300 [==============================] - 6s 19ms/step - loss: 0.4321 - acc: 0.8633 - val\_loss: 0.7432 - val\_acc: 0.6885

Epoch 00036: val\_loss did not improve from 0.69584
Epoch 37/60
300/300 [==============================] - 6s 19ms/step - loss: 0.4387 - acc: 0.8333 - val\_loss: 0.6705 - val\_acc: 0.7377

Epoch 00037: val\_loss improved from 0.69584 to 0.67050, saving model to Model\_1\_bg.weights.best.hdf5
Epoch 38/60
300/300 [==============================] - 6s 19ms/step - loss: 0.4268 - acc: 0.8333 - val\_loss: 0.6674 - val\_acc: 0.7213

Epoch 00038: val\_loss improved from 0.67050 to 0.66744, saving model to Model\_1\_bg.weights.best.hdf5
Epoch 39/60
300/300 [==============================] - 6s 19ms/step - loss: 0.4248 - acc: 0.8233 - val\_loss: 0.6895 - val\_acc: 0.7213

Epoch 00039: val\_loss did not improve from 0.66744
Epoch 40/60
300/300 [==============================] - 6s 19ms/step - loss: 0.3851 - acc: 0.8567 - val\_loss: 0.6464 - val\_acc: 0.7295

Epoch 00040: val\_loss improved from 0.66744 to 0.64636, saving model to Model\_1\_bg.weights.best.hdf5
Epoch 41/60
300/300 [==============================] - 5s 18ms/step - loss: 0.4387 - acc: 0.8200 - val\_loss: 0.6989 - val\_acc: 0.7213

Epoch 00041: val\_loss did not improve from 0.64636
Epoch 42/60
300/300 [==============================] - 5s 18ms/step - loss: 0.4361 - acc: 0.8467 - val\_loss: 0.6178 - val\_acc: 0.7869

Epoch 00042: val\_loss improved from 0.64636 to 0.61782, saving model to Model\_1\_bg.weights.best.hdf5
Epoch 43/60
300/300 [==============================] - 6s 18ms/step - loss: 0.3666 - acc: 0.8833 - val\_loss: 0.7748 - val\_acc: 0.7213

Epoch 00043: val\_loss did not improve from 0.61782
Epoch 44/60
300/300 [==============================] - 6s 19ms/step - loss: 0.3630 - acc: 0.8567 - val\_loss: 0.7448 - val\_acc: 0.7295

Epoch 00044: val\_loss did not improve from 0.61782
Epoch 45/60
300/300 [==============================] - 6s 18ms/step - loss: 0.3830 - acc: 0.8533 - val\_loss: 0.7991 - val\_acc: 0.7213

Epoch 00045: val\_loss did not improve from 0.61782
Epoch 46/60
300/300 [==============================] - 6s 20ms/step - loss: 0.3734 - acc: 0.8667 - val\_loss: 0.7339 - val\_acc: 0.7295

Epoch 00046: val\_loss did not improve from 0.61782
Epoch 47/60
300/300 [==============================] - 5s 18ms/step - loss: 0.3602 - acc: 0.8567 - val\_loss: 0.7766 - val\_acc: 0.7131

Epoch 00047: val\_loss did not improve from 0.61782
Epoch 48/60
300/300 [==============================] - 6s 19ms/step - loss: 0.3711 - acc: 0.8567 - val\_loss: 0.7549 - val\_acc: 0.7049

Epoch 00048: val\_loss did not improve from 0.61782
Epoch 49/60
300/300 [==============================] - 5s 18ms/step - loss: 0.3173 - acc: 0.8933 - val\_loss: 0.7037 - val\_acc: 0.7541

Epoch 00049: val\_loss did not improve from 0.61782
Epoch 50/60
300/300 [==============================] - 5s 18ms/step - loss: 0.3258 - acc: 0.8733 - val\_loss: 0.6561 - val\_acc: 0.7787

Epoch 00050: val\_loss did not improve from 0.61782
Epoch 51/60
300/300 [==============================] - 6s 19ms/step - loss: 0.3178 - acc: 0.9067 - val\_loss: 0.7905 - val\_acc: 0.7131

Epoch 00051: val\_loss did not improve from 0.61782
Epoch 52/60
300/300 [==============================] - 6s 18ms/step - loss: 0.3544 - acc: 0.8833 - val\_loss: 0.8364 - val\_acc: 0.6885

Epoch 00052: val\_loss did not improve from 0.61782
Epoch 53/60
300/300 [==============================] - 5s 18ms/step - loss: 0.3214 - acc: 0.8900 - val\_loss: 0.6101 - val\_acc: 0.8033

Epoch 00053: val\_loss improved from 0.61782 to 0.61015, saving model to Model\_1\_bg.weights.best.hdf5
Epoch 54/60
300/300 [==============================] - 6s 18ms/step - loss: 0.3039 - acc: 0.8967 - val\_loss: 0.7331 - val\_acc: 0.7541

Epoch 00054: val\_loss did not improve from 0.61015
Epoch 55/60
300/300 [==============================] - 6s 18ms/step - loss: 0.2769 - acc: 0.9267 - val\_loss: 0.7134 - val\_acc: 0.7787

Epoch 00055: val\_loss did not improve from 0.61015
Epoch 56/60
300/300 [==============================] - 6s 18ms/step - loss: 0.2891 - acc: 0.9000 - val\_loss: 0.7935 - val\_acc: 0.7377

Epoch 00056: val\_loss did not improve from 0.61015
Epoch 57/60
300/300 [==============================] - 6s 19ms/step - loss: 0.3401 - acc: 0.8733 - val\_loss: 0.6578 - val\_acc: 0.7951

Epoch 00057: val\_loss did not improve from 0.61015
Epoch 58/60
300/300 [==============================] - 6s 19ms/step - loss: 0.2805 - acc: 0.9167 - val\_loss: 0.6170 - val\_acc: 0.7541

Epoch 00058: val\_loss did not improve from 0.61015
Epoch 59/60
300/300 [==============================] - 6s 18ms/step - loss: 0.3094 - acc: 0.8967 - val\_loss: 0.6555 - val\_acc: 0.7623

Epoch 00059: val\_loss did not improve from 0.61015
Epoch 60/60
300/300 [==============================] - 6s 18ms/step - loss: 0.2719 - acc: 0.9167 - val\_loss: 0.6965 - val\_acc: 0.7705

Epoch 00060: val\_loss did not improve from 0.61015

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}34}]:} \PY{n}{model1\PYZus{}bg}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model\PYZus{}1\PYZus{}bg.weights.best.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Testing the model on the Test data}
         \PY{p}{(}\PY{n}{loss1\PYZus{}bg}\PY{p}{,} \PY{n}{accuracy1\PYZus{}bg}\PY{p}{)} \PY{o}{=} \PY{n}{model1\PYZus{}bg}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}bgsub}\PY{p}{,} \PY{n}{y\PYZus{}test\PYZus{}bgsub}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
         
         \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy on test data: \PYZob{}:.2f\PYZcb{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracy1\PYZus{}bg} \PY{o}{*} \PY{l+m+mi}{100}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy on test data: 75.00\%

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}35}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{loss1\PYZus{}bg} \PY{o}{=} \PY{n}{history1\PYZus{}bg}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}                          \PY{c+c1}{\PYZsh{} Loss on the training data}
         \PY{n}{val\PYZus{}loss1\PYZus{}bg} \PY{o}{=} \PY{n}{history1\PYZus{}bg}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}                  \PY{c+c1}{\PYZsh{} Loss on the validation data}
         \PY{n}{epochs} \PY{o}{=} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{61}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{loss1\PYZus{}bg}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ro\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Loss bgsub}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{val\PYZus{}loss1\PYZus{}bg}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{go\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation Loss bgsub}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}35}]:} <matplotlib.legend.Legend at 0x7f2645997dd0>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_48_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}36}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn.metrics} \PY{k+kn}{import} \PY{n}{confusion\PYZus{}matrix}
         \PY{k+kn}{import} \PY{n+nn}{itertools}
         \PY{n}{y\PYZus{}predictions} \PY{o}{=} \PY{n}{model1\PYZus{}bg}\PY{o}{.}\PY{n}{predict\PYZus{}classes}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}bgsub}\PY{p}{)}\PY{p}{;}
         \PY{n}{y\PYZus{}pred}\PY{o}{=}\PY{n}{to\PYZus{}categorical}\PY{p}{(}\PY{n}{y\PYZus{}predictions}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{;}
         \PY{n}{y\PYZus{}test\PYZus{}cm} \PY{o}{=} \PY{n}{y\PYZus{}test\PYZus{}bgsub}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{;}
         \PY{c+c1}{\PYZsh{}print y\PYZus{}predictions[:10]}
         \PY{c+c1}{\PYZsh{}print y\PYZus{}test[:10]}
         \PY{n}{model\PYZus{}cnf\PYZus{}matrix} \PY{o}{=} \PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{y\PYZus{}test\PYZus{}cm}\PY{p}{,}\PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{;}
         \PY{n}{confusion\PYZus{}matrix\PYZus{}plot} \PY{o}{=} \PY{n}{plot\PYZus{}confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{model\PYZus{}cnf\PYZus{}matrix}\PY{p}{,} 
                                                       \PY{n}{classes}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{boxing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{handclapping}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{handwaving}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{jogging}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{running}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{walking}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} 
                                                       \PY{n}{normalize}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Normalized confusion matrix

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_49_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{comparision-of-model1-with-bg-sub-and-without-bg-sub}{%
\subsection{Comparision of Model1 with BG-sub and without
BG-sub}\label{comparision-of-model1-with-bg-sub-and-without-bg-sub}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}37}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{loss1}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ro\PYZhy{}.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{val\PYZus{}loss1}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{go\PYZhy{}.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{loss1\PYZus{}bg}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bo\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Loss \PYZhy{} BG Sub}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{val\PYZus{}loss1\PYZus{}bg}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{yo\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation Loss \PYZhy{} BG Sub}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}37}]:} <matplotlib.legend.Legend at 0x7f2645808e10>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_51_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{model---2}{%
\subsection{Model - 2}\label{model---2}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}38}]:} \PY{c+c1}{\PYZsh{} Imports}
         \PY{k+kn}{from} \PY{n+nn}{keras.models} \PY{k+kn}{import} \PY{n}{Sequential}
         \PY{k+kn}{from} \PY{n+nn}{keras.layers} \PY{k+kn}{import} \PY{n}{Conv3D}\PY{p}{,} \PY{n}{MaxPooling3D}\PY{p}{,} \PY{n}{GlobalAveragePooling3D}
         \PY{k+kn}{from} \PY{n+nn}{keras.layers.core} \PY{k+kn}{import} \PY{n}{Dense}\PY{p}{,} \PY{n}{Dropout}
         
         \PY{c+c1}{\PYZsh{} Using the Sequential Model}
         \PY{n}{model2} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
         
         
         \PY{n}{model2}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv3D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                          \PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{n}{model2}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling3D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{model2}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv3D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{model2}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling3D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{model2}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv3D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{model2}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling3D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         
         \PY{n}{model2}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{GlobalAveragePooling3D}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{model2}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}model2.add(Dropout(0.5))}
         
         \PY{n}{model2}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{model2}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
conv3d\_5 (Conv3D)            (None, 7, 10, 10, 16)     1456      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling3d\_5 (MaxPooling3 (None, 7, 5, 5, 16)       0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv3d\_6 (Conv3D)            (None, 3, 5, 5, 64)       46144     
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling3d\_6 (MaxPooling3 (None, 3, 3, 3, 64)       0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv3d\_7 (Conv3D)            (None, 1, 3, 3, 256)      442624    
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling3d\_7 (MaxPooling3 (None, 1, 2, 2, 256)      0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
global\_average\_pooling3d\_3 ( (None, 256)               0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_5 (Dense)              (None, 32)                8224      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_6 (Dense)              (None, 6)                 198       
=================================================================
Total params: 498,646
Trainable params: 498,646
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \hypertarget{training-model---2}{%
\subsubsection{Training Model - 2}\label{training-model---2}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}39}]:} \PY{c+c1}{\PYZsh{} Imports}
         \PY{k+kn}{from} \PY{n+nn}{keras.callbacks} \PY{k+kn}{import} \PY{n}{ModelCheckpoint}
         \PY{k+kn}{from} \PY{n+nn}{keras} \PY{k+kn}{import} \PY{n}{optimizers}
         
         \PY{c+c1}{\PYZsh{} Compiling the model}
         \PY{c+c1}{\PYZsh{}sgd = optimizers.SGD(lr=0.01, decay=1e\PYZhy{}6, momentum=0.9, nesterov=True)}
         \PY{c+c1}{\PYZsh{}adam=optimizers.Adam(lr=0.01, beta\PYZus{}1=0.9, beta\PYZus{}2=0.999, epsilon=1e\PYZhy{}8)}
         \PY{c+c1}{\PYZsh{}model2.compile(loss=\PYZsq{}mean\PYZus{}squared\PYZus{}error\PYZsq{}, optimizer=sgd, metrics=[\PYZsq{}accuracy\PYZsq{}])}
         
         \PY{c+c1}{\PYZsh{}model2.compile(loss=\PYZsq{}categorical\PYZus{}crossentropy\PYZsq{}, optimizer=\PYZsq{}adam\PYZsq{}, metrics=[\PYZsq{}accuracy\PYZsq{}])}
         \PY{n}{model2}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{adam}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} Saving the model that performed the best on the validation set}
         \PY{n}{checkpoint2} \PY{o}{=} \PY{n}{ModelCheckpoint}\PY{p}{(}\PY{n}{filepath}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model\PYZus{}2.weights.best.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{save\PYZus{}best\PYZus{}only}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
         
         \PY{n}{history2} \PY{o}{=} \PY{n}{model2}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{60}\PY{p}{,} 
                             \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{X\PYZus{}valid}\PY{p}{,} \PY{n}{y\PYZus{}valid}\PY{p}{)}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{checkpoint2}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Train on 300 samples, validate on 122 samples
Epoch 1/60
 - 2s - loss: 1.7894 - acc: 0.2100 - val\_loss: 1.7873 - val\_acc: 0.1475

Epoch 00001: val\_loss improved from inf to 1.78728, saving model to Model\_2.weights.best.hdf5
Epoch 2/60
 - 1s - loss: 1.7758 - acc: 0.1933 - val\_loss: 1.7825 - val\_acc: 0.1475

Epoch 00002: val\_loss improved from 1.78728 to 1.78252, saving model to Model\_2.weights.best.hdf5
Epoch 3/60
 - 1s - loss: 1.7639 - acc: 0.2133 - val\_loss: 1.7839 - val\_acc: 0.1475

Epoch 00003: val\_loss did not improve from 1.78252
Epoch 4/60
 - 1s - loss: 1.7497 - acc: 0.1900 - val\_loss: 1.7736 - val\_acc: 0.1475

Epoch 00004: val\_loss improved from 1.78252 to 1.77357, saving model to Model\_2.weights.best.hdf5
Epoch 5/60
 - 1s - loss: 1.6915 - acc: 0.2133 - val\_loss: 1.6105 - val\_acc: 0.1475

Epoch 00005: val\_loss improved from 1.77357 to 1.61050, saving model to Model\_2.weights.best.hdf5
Epoch 6/60
 - 1s - loss: 1.4201 - acc: 0.3233 - val\_loss: 1.3124 - val\_acc: 0.2623

Epoch 00006: val\_loss improved from 1.61050 to 1.31237, saving model to Model\_2.weights.best.hdf5
Epoch 7/60
 - 1s - loss: 1.3005 - acc: 0.3067 - val\_loss: 1.2416 - val\_acc: 0.3443

Epoch 00007: val\_loss improved from 1.31237 to 1.24161, saving model to Model\_2.weights.best.hdf5
Epoch 8/60
 - 1s - loss: 1.2240 - acc: 0.3133 - val\_loss: 1.1441 - val\_acc: 0.3607

Epoch 00008: val\_loss improved from 1.24161 to 1.14414, saving model to Model\_2.weights.best.hdf5
Epoch 9/60
 - 1s - loss: 1.1623 - acc: 0.3600 - val\_loss: 1.2513 - val\_acc: 0.2951

Epoch 00009: val\_loss did not improve from 1.14414
Epoch 10/60
 - 1s - loss: 1.1610 - acc: 0.2967 - val\_loss: 1.1411 - val\_acc: 0.3279

Epoch 00010: val\_loss improved from 1.14414 to 1.14114, saving model to Model\_2.weights.best.hdf5
Epoch 11/60
 - 1s - loss: 1.1392 - acc: 0.3567 - val\_loss: 1.1156 - val\_acc: 0.3852

Epoch 00011: val\_loss improved from 1.14114 to 1.11561, saving model to Model\_2.weights.best.hdf5
Epoch 12/60
 - 1s - loss: 1.1298 - acc: 0.3500 - val\_loss: 1.1355 - val\_acc: 0.2951

Epoch 00012: val\_loss did not improve from 1.11561
Epoch 13/60
 - 1s - loss: 1.0970 - acc: 0.3767 - val\_loss: 1.1010 - val\_acc: 0.2869

Epoch 00013: val\_loss improved from 1.11561 to 1.10104, saving model to Model\_2.weights.best.hdf5
Epoch 14/60
 - 1s - loss: 1.1109 - acc: 0.3567 - val\_loss: 1.1475 - val\_acc: 0.2787

Epoch 00014: val\_loss did not improve from 1.10104
Epoch 15/60
 - 1s - loss: 1.0924 - acc: 0.3500 - val\_loss: 1.0701 - val\_acc: 0.4344

Epoch 00015: val\_loss improved from 1.10104 to 1.07012, saving model to Model\_2.weights.best.hdf5
Epoch 16/60
 - 1s - loss: 1.0937 - acc: 0.3900 - val\_loss: 1.2623 - val\_acc: 0.3852

Epoch 00016: val\_loss did not improve from 1.07012
Epoch 17/60
 - 1s - loss: 1.0770 - acc: 0.4167 - val\_loss: 1.0206 - val\_acc: 0.4590

Epoch 00017: val\_loss improved from 1.07012 to 1.02062, saving model to Model\_2.weights.best.hdf5
Epoch 18/60
 - 1s - loss: 1.0295 - acc: 0.4700 - val\_loss: 1.0157 - val\_acc: 0.4672

Epoch 00018: val\_loss improved from 1.02062 to 1.01566, saving model to Model\_2.weights.best.hdf5
Epoch 19/60
 - 1s - loss: 1.0036 - acc: 0.4967 - val\_loss: 0.9820 - val\_acc: 0.4426

Epoch 00019: val\_loss improved from 1.01566 to 0.98196, saving model to Model\_2.weights.best.hdf5
Epoch 20/60
 - 1s - loss: 0.9740 - acc: 0.4900 - val\_loss: 0.9239 - val\_acc: 0.5492

Epoch 00020: val\_loss improved from 0.98196 to 0.92387, saving model to Model\_2.weights.best.hdf5
Epoch 21/60
 - 1s - loss: 0.9527 - acc: 0.5233 - val\_loss: 0.9784 - val\_acc: 0.4754

Epoch 00021: val\_loss did not improve from 0.92387
Epoch 22/60
 - 1s - loss: 0.9193 - acc: 0.5233 - val\_loss: 0.8949 - val\_acc: 0.5492

Epoch 00022: val\_loss improved from 0.92387 to 0.89493, saving model to Model\_2.weights.best.hdf5
Epoch 23/60
 - 1s - loss: 0.9435 - acc: 0.5067 - val\_loss: 0.8939 - val\_acc: 0.5574

Epoch 00023: val\_loss improved from 0.89493 to 0.89392, saving model to Model\_2.weights.best.hdf5
Epoch 24/60
 - 1s - loss: 0.8824 - acc: 0.5567 - val\_loss: 0.9544 - val\_acc: 0.4836

Epoch 00024: val\_loss did not improve from 0.89392
Epoch 25/60
 - 1s - loss: 0.9252 - acc: 0.4933 - val\_loss: 0.9346 - val\_acc: 0.5574

Epoch 00025: val\_loss did not improve from 0.89392
Epoch 26/60
 - 1s - loss: 0.8505 - acc: 0.5933 - val\_loss: 0.9277 - val\_acc: 0.4836

Epoch 00026: val\_loss did not improve from 0.89392
Epoch 27/60
 - 1s - loss: 0.8250 - acc: 0.5967 - val\_loss: 0.8915 - val\_acc: 0.5492

Epoch 00027: val\_loss improved from 0.89392 to 0.89150, saving model to Model\_2.weights.best.hdf5
Epoch 28/60
 - 1s - loss: 0.8246 - acc: 0.5833 - val\_loss: 0.7986 - val\_acc: 0.5984

Epoch 00028: val\_loss improved from 0.89150 to 0.79857, saving model to Model\_2.weights.best.hdf5
Epoch 29/60
 - 1s - loss: 0.7845 - acc: 0.5967 - val\_loss: 0.8157 - val\_acc: 0.5820

Epoch 00029: val\_loss did not improve from 0.79857
Epoch 30/60
 - 1s - loss: 0.7644 - acc: 0.6033 - val\_loss: 0.9412 - val\_acc: 0.5328

Epoch 00030: val\_loss did not improve from 0.79857
Epoch 31/60
 - 1s - loss: 0.8230 - acc: 0.6067 - val\_loss: 0.9477 - val\_acc: 0.5164

Epoch 00031: val\_loss did not improve from 0.79857
Epoch 32/60
 - 1s - loss: 0.7565 - acc: 0.6233 - val\_loss: 0.8092 - val\_acc: 0.4918

Epoch 00032: val\_loss did not improve from 0.79857
Epoch 33/60
 - 1s - loss: 0.7647 - acc: 0.6267 - val\_loss: 0.9907 - val\_acc: 0.5328

Epoch 00033: val\_loss did not improve from 0.79857
Epoch 34/60
 - 1s - loss: 0.7824 - acc: 0.6033 - val\_loss: 0.8303 - val\_acc: 0.5574

Epoch 00034: val\_loss did not improve from 0.79857
Epoch 35/60
 - 1s - loss: 0.7266 - acc: 0.6500 - val\_loss: 0.7805 - val\_acc: 0.5574

Epoch 00035: val\_loss improved from 0.79857 to 0.78047, saving model to Model\_2.weights.best.hdf5
Epoch 36/60
 - 1s - loss: 0.7423 - acc: 0.6267 - val\_loss: 0.8042 - val\_acc: 0.5492

Epoch 00036: val\_loss did not improve from 0.78047
Epoch 37/60
 - 1s - loss: 0.7336 - acc: 0.6433 - val\_loss: 0.7437 - val\_acc: 0.6311

Epoch 00037: val\_loss improved from 0.78047 to 0.74368, saving model to Model\_2.weights.best.hdf5
Epoch 38/60
 - 1s - loss: 0.6895 - acc: 0.6200 - val\_loss: 0.7987 - val\_acc: 0.5820

Epoch 00038: val\_loss did not improve from 0.74368
Epoch 39/60
 - 1s - loss: 0.6783 - acc: 0.6800 - val\_loss: 0.7703 - val\_acc: 0.5984

Epoch 00039: val\_loss did not improve from 0.74368
Epoch 40/60
 - 1s - loss: 0.6389 - acc: 0.6767 - val\_loss: 0.8560 - val\_acc: 0.5410

Epoch 00040: val\_loss did not improve from 0.74368
Epoch 41/60
 - 1s - loss: 0.6459 - acc: 0.6467 - val\_loss: 0.7311 - val\_acc: 0.6148

Epoch 00041: val\_loss improved from 0.74368 to 0.73113, saving model to Model\_2.weights.best.hdf5
Epoch 42/60
 - 1s - loss: 0.6239 - acc: 0.7067 - val\_loss: 0.8468 - val\_acc: 0.5410

Epoch 00042: val\_loss did not improve from 0.73113
Epoch 43/60
 - 1s - loss: 0.6209 - acc: 0.7100 - val\_loss: 0.9031 - val\_acc: 0.5738

Epoch 00043: val\_loss did not improve from 0.73113
Epoch 44/60
 - 1s - loss: 0.5775 - acc: 0.7533 - val\_loss: 0.8075 - val\_acc: 0.6066

Epoch 00044: val\_loss did not improve from 0.73113
Epoch 45/60
 - 1s - loss: 0.5725 - acc: 0.7333 - val\_loss: 0.7376 - val\_acc: 0.6148

Epoch 00045: val\_loss did not improve from 0.73113
Epoch 46/60
 - 1s - loss: 0.5737 - acc: 0.7300 - val\_loss: 0.7635 - val\_acc: 0.5984

Epoch 00046: val\_loss did not improve from 0.73113
Epoch 47/60
 - 1s - loss: 0.5247 - acc: 0.7833 - val\_loss: 0.7500 - val\_acc: 0.6148

Epoch 00047: val\_loss did not improve from 0.73113
Epoch 48/60
 - 1s - loss: 0.5967 - acc: 0.7167 - val\_loss: 0.8688 - val\_acc: 0.6230

Epoch 00048: val\_loss did not improve from 0.73113
Epoch 49/60
 - 1s - loss: 0.6076 - acc: 0.7267 - val\_loss: 0.9731 - val\_acc: 0.5574

Epoch 00049: val\_loss did not improve from 0.73113
Epoch 50/60
 - 1s - loss: 0.5711 - acc: 0.7167 - val\_loss: 0.7530 - val\_acc: 0.6311

Epoch 00050: val\_loss did not improve from 0.73113
Epoch 51/60
 - 1s - loss: 0.4904 - acc: 0.7900 - val\_loss: 0.7293 - val\_acc: 0.6066

Epoch 00051: val\_loss improved from 0.73113 to 0.72929, saving model to Model\_2.weights.best.hdf5
Epoch 52/60
 - 1s - loss: 0.4968 - acc: 0.7667 - val\_loss: 0.6653 - val\_acc: 0.6393

Epoch 00052: val\_loss improved from 0.72929 to 0.66526, saving model to Model\_2.weights.best.hdf5
Epoch 53/60
 - 1s - loss: 0.5036 - acc: 0.7533 - val\_loss: 0.7740 - val\_acc: 0.6475

Epoch 00053: val\_loss did not improve from 0.66526
Epoch 54/60
 - 1s - loss: 0.4669 - acc: 0.7833 - val\_loss: 0.8423 - val\_acc: 0.6557

Epoch 00054: val\_loss did not improve from 0.66526
Epoch 55/60
 - 1s - loss: 0.4157 - acc: 0.8167 - val\_loss: 0.7329 - val\_acc: 0.6066

Epoch 00055: val\_loss did not improve from 0.66526
Epoch 56/60
 - 1s - loss: 0.4427 - acc: 0.8033 - val\_loss: 0.6993 - val\_acc: 0.6639

Epoch 00056: val\_loss did not improve from 0.66526
Epoch 57/60
 - 1s - loss: 0.3860 - acc: 0.8467 - val\_loss: 0.6900 - val\_acc: 0.6885

Epoch 00057: val\_loss did not improve from 0.66526
Epoch 58/60
 - 1s - loss: 0.4232 - acc: 0.8000 - val\_loss: 0.6922 - val\_acc: 0.6475

Epoch 00058: val\_loss did not improve from 0.66526
Epoch 59/60
 - 1s - loss: 0.4487 - acc: 0.8100 - val\_loss: 0.8221 - val\_acc: 0.5984

Epoch 00059: val\_loss did not improve from 0.66526
Epoch 60/60
 - 1s - loss: 0.3963 - acc: 0.8400 - val\_loss: 0.7540 - val\_acc: 0.6475

Epoch 00060: val\_loss did not improve from 0.66526

    \end{Verbatim}

    \hypertarget{evaluating-model---2}{%
\subsubsection{Evaluating Model - 2}\label{evaluating-model---2}}

Evaluating \texttt{Model-2} on the test data.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}40}]:} \PY{c+c1}{\PYZsh{} Loading the model that performed the best on the validation set}
         \PY{n}{model2}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model\PYZus{}2.weights.best.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Testing the model on the Test data}
         \PY{p}{(}\PY{n}{loss2}\PY{p}{,} \PY{n}{accuracy2}\PY{p}{)} \PY{o}{=} \PY{n}{model2}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
         
         \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy on test data: \PYZob{}:.2f\PYZcb{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracy2} \PY{o}{*} \PY{l+m+mi}{100}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy on test data: 53.00\%

    \end{Verbatim}

    \hypertarget{model---2-performance}{%
\subsection{Model - 2 Performance}\label{model---2-performance}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}41}]:} \PY{c+c1}{\PYZsh{} Making the plot larger}
         \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{loss2} \PY{o}{=} \PY{n}{history2}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}                          \PY{c+c1}{\PYZsh{} Loss on the training data}
         \PY{n}{val\PYZus{}loss2} \PY{o}{=} \PY{n}{history2}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}                  \PY{c+c1}{\PYZsh{} Loss on the validation data}
         \PY{n}{epochs} \PY{o}{=} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{61}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{loss2}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ro\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{val\PYZus{}loss2}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{go\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}41}]:} <matplotlib.legend.Legend at 0x7f263a90cbd0>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_59_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}42}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn.metrics} \PY{k+kn}{import} \PY{n}{confusion\PYZus{}matrix}
         \PY{k+kn}{import} \PY{n+nn}{itertools}
         \PY{n}{y\PYZus{}predictions2} \PY{o}{=} \PY{n}{model2}\PY{o}{.}\PY{n}{predict\PYZus{}classes}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{p}{;}
         \PY{n}{y\PYZus{}pred2}\PY{o}{=}\PY{n}{to\PYZus{}categorical}\PY{p}{(}\PY{n}{y\PYZus{}predictions2}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{;}
         \PY{n}{y\PYZus{}test\PYZus{}cm} \PY{o}{=} \PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{;}
         \PY{c+c1}{\PYZsh{}print y\PYZus{}predictions[:10]}
         \PY{c+c1}{\PYZsh{}print y\PYZus{}test[:10]}
         \PY{n}{model\PYZus{}cnf\PYZus{}matrix} \PY{o}{=} \PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{y\PYZus{}test\PYZus{}cm}\PY{p}{,}\PY{n}{y\PYZus{}pred2}\PY{p}{)}\PY{p}{;}
         \PY{n}{confusion\PYZus{}matrix\PYZus{}plot} \PY{o}{=} \PY{n}{plot\PYZus{}confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{model\PYZus{}cnf\PYZus{}matrix}\PY{p}{,} 
                                                       \PY{n}{classes}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{boxing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{handclapping}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{handwaving}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{jogging}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{running}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{walking}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} 
                                                       \PY{n}{normalize}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Normalized confusion matrix

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_60_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{model-2---with-bg}{%
\section{Model 2 - With BG}\label{model-2---with-bg}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}43}]:} \PY{c+c1}{\PYZsh{} Imports}
         \PY{k+kn}{from} \PY{n+nn}{keras.models} \PY{k+kn}{import} \PY{n}{Sequential}
         \PY{k+kn}{from} \PY{n+nn}{keras.layers} \PY{k+kn}{import} \PY{n}{Conv3D}\PY{p}{,} \PY{n}{MaxPooling3D}\PY{p}{,} \PY{n}{GlobalAveragePooling3D}
         \PY{k+kn}{from} \PY{n+nn}{keras.layers.core} \PY{k+kn}{import} \PY{n}{Dense}\PY{p}{,} \PY{n}{Dropout}
         
         \PY{c+c1}{\PYZsh{} Using the Sequential Model}
         \PY{n}{model2\PYZus{}bg} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
         
         
         \PY{n}{model2\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv3D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                          \PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{n}{model2\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling3D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{model2\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv3D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{model2\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling3D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{model2\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv3D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{model2\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling3D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         
         \PY{n}{model2\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{GlobalAveragePooling3D}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{model2\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}model2.add(Dropout(0.5))}
         
         \PY{n}{model2\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{model2\PYZus{}bg}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
conv3d\_8 (Conv3D)            (None, 7, 10, 10, 16)     1456      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling3d\_8 (MaxPooling3 (None, 7, 5, 5, 16)       0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv3d\_9 (Conv3D)            (None, 3, 5, 5, 64)       46144     
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling3d\_9 (MaxPooling3 (None, 3, 3, 3, 64)       0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv3d\_10 (Conv3D)           (None, 1, 3, 3, 256)      442624    
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling3d\_10 (MaxPooling (None, 1, 2, 2, 256)      0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
global\_average\_pooling3d\_4 ( (None, 256)               0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_7 (Dense)              (None, 32)                8224      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_8 (Dense)              (None, 6)                 198       
=================================================================
Total params: 498,646
Trainable params: 498,646
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}44}]:} \PY{c+c1}{\PYZsh{} Imports}
         \PY{k+kn}{from} \PY{n+nn}{keras.callbacks} \PY{k+kn}{import} \PY{n}{ModelCheckpoint}
         \PY{k+kn}{from} \PY{n+nn}{keras} \PY{k+kn}{import} \PY{n}{optimizers}
         
         \PY{c+c1}{\PYZsh{} Compiling the model}
         \PY{c+c1}{\PYZsh{}sgd = optimizers.SGD(lr=0.01, decay=1e\PYZhy{}6, momentum=0.9, nesterov=True)}
         \PY{c+c1}{\PYZsh{}adam=optimizers.Adam(lr=0.01, beta\PYZus{}1=0.9, beta\PYZus{}2=0.999, epsilon=1e\PYZhy{}8)}
         \PY{c+c1}{\PYZsh{}model2.compile(loss=\PYZsq{}mean\PYZus{}squared\PYZus{}error\PYZsq{}, optimizer=sgd, metrics=[\PYZsq{}accuracy\PYZsq{}])}
         
         \PY{c+c1}{\PYZsh{}model2.compile(loss=\PYZsq{}categorical\PYZus{}crossentropy\PYZsq{}, optimizer=\PYZsq{}adam\PYZsq{}, metrics=[\PYZsq{}accuracy\PYZsq{}])}
         \PY{n}{model2\PYZus{}bg}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{adam}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} Saving the model that performed the best on the validation set}
         \PY{n}{checkpoint2\PYZus{}bg} \PY{o}{=} \PY{n}{ModelCheckpoint}\PY{p}{(}\PY{n}{filepath}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model\PYZus{}2\PYZus{}bg.weights.best.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{save\PYZus{}best\PYZus{}only}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
         
         
         \PY{n}{history2\PYZus{}bg} \PY{o}{=} \PY{n}{model2\PYZus{}bg}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}bgsub}\PY{p}{,} \PY{n}{y\PYZus{}train\PYZus{}bgsub}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{60}\PY{p}{,} 
                             \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{X\PYZus{}valid\PYZus{}bgsub}\PY{p}{,} \PY{n}{y\PYZus{}valid\PYZus{}bgsub}\PY{p}{)}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{checkpoint2\PYZus{}bg}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Train on 300 samples, validate on 122 samples
Epoch 1/60
 - 2s - loss: 1.6292 - acc: 0.2600 - val\_loss: 1.4395 - val\_acc: 0.4262

Epoch 00001: val\_loss improved from inf to 1.43952, saving model to Model\_2\_bg.weights.best.hdf5
Epoch 2/60
 - 1s - loss: 1.3840 - acc: 0.4400 - val\_loss: 1.1988 - val\_acc: 0.4918

Epoch 00002: val\_loss improved from 1.43952 to 1.19876, saving model to Model\_2\_bg.weights.best.hdf5
Epoch 3/60
 - 1s - loss: 1.1282 - acc: 0.5167 - val\_loss: 0.9623 - val\_acc: 0.5328

Epoch 00003: val\_loss improved from 1.19876 to 0.96230, saving model to Model\_2\_bg.weights.best.hdf5
Epoch 4/60
 - 1s - loss: 0.9450 - acc: 0.5733 - val\_loss: 0.8997 - val\_acc: 0.6885

Epoch 00004: val\_loss improved from 0.96230 to 0.89973, saving model to Model\_2\_bg.weights.best.hdf5
Epoch 5/60
 - 1s - loss: 0.8543 - acc: 0.6600 - val\_loss: 0.8304 - val\_acc: 0.6311

Epoch 00005: val\_loss improved from 0.89973 to 0.83039, saving model to Model\_2\_bg.weights.best.hdf5
Epoch 6/60
 - 1s - loss: 0.6696 - acc: 0.7167 - val\_loss: 0.7333 - val\_acc: 0.6639

Epoch 00006: val\_loss improved from 0.83039 to 0.73326, saving model to Model\_2\_bg.weights.best.hdf5
Epoch 7/60
 - 1s - loss: 0.5207 - acc: 0.8267 - val\_loss: 0.7303 - val\_acc: 0.7213

Epoch 00007: val\_loss improved from 0.73326 to 0.73033, saving model to Model\_2\_bg.weights.best.hdf5
Epoch 8/60
 - 1s - loss: 0.5083 - acc: 0.8100 - val\_loss: 1.4567 - val\_acc: 0.6066

Epoch 00008: val\_loss did not improve from 0.73033
Epoch 9/60
 - 1s - loss: 0.5370 - acc: 0.7933 - val\_loss: 0.8244 - val\_acc: 0.6967

Epoch 00009: val\_loss did not improve from 0.73033
Epoch 10/60
 - 1s - loss: 0.4053 - acc: 0.8167 - val\_loss: 0.9130 - val\_acc: 0.6393

Epoch 00010: val\_loss did not improve from 0.73033
Epoch 11/60
 - 1s - loss: 0.3595 - acc: 0.8633 - val\_loss: 0.7714 - val\_acc: 0.6803

Epoch 00011: val\_loss did not improve from 0.73033
Epoch 12/60
 - 1s - loss: 0.3032 - acc: 0.8933 - val\_loss: 0.7048 - val\_acc: 0.7459

Epoch 00012: val\_loss improved from 0.73033 to 0.70483, saving model to Model\_2\_bg.weights.best.hdf5
Epoch 13/60
 - 1s - loss: 0.2937 - acc: 0.9033 - val\_loss: 0.6839 - val\_acc: 0.7377

Epoch 00013: val\_loss improved from 0.70483 to 0.68393, saving model to Model\_2\_bg.weights.best.hdf5
Epoch 14/60
 - 1s - loss: 0.2125 - acc: 0.9033 - val\_loss: 0.8326 - val\_acc: 0.7213

Epoch 00014: val\_loss did not improve from 0.68393
Epoch 15/60
 - 1s - loss: 0.2001 - acc: 0.9333 - val\_loss: 0.9154 - val\_acc: 0.6885

Epoch 00015: val\_loss did not improve from 0.68393
Epoch 16/60
 - 1s - loss: 0.1371 - acc: 0.9567 - val\_loss: 0.7133 - val\_acc: 0.7787

Epoch 00016: val\_loss did not improve from 0.68393
Epoch 17/60
 - 1s - loss: 0.1116 - acc: 0.9833 - val\_loss: 0.8402 - val\_acc: 0.7213

Epoch 00017: val\_loss did not improve from 0.68393
Epoch 18/60
 - 1s - loss: 0.1006 - acc: 0.9767 - val\_loss: 0.8300 - val\_acc: 0.7787

Epoch 00018: val\_loss did not improve from 0.68393
Epoch 19/60
 - 1s - loss: 0.0712 - acc: 0.9933 - val\_loss: 0.7825 - val\_acc: 0.7459

Epoch 00019: val\_loss did not improve from 0.68393
Epoch 20/60
 - 1s - loss: 0.0570 - acc: 0.9933 - val\_loss: 0.9378 - val\_acc: 0.7295

Epoch 00020: val\_loss did not improve from 0.68393
Epoch 21/60
 - 1s - loss: 0.0416 - acc: 0.9967 - val\_loss: 0.9721 - val\_acc: 0.7705

Epoch 00021: val\_loss did not improve from 0.68393
Epoch 22/60
 - 1s - loss: 0.0348 - acc: 0.9933 - val\_loss: 0.9001 - val\_acc: 0.7705

Epoch 00022: val\_loss did not improve from 0.68393
Epoch 23/60
 - 1s - loss: 0.0243 - acc: 1.0000 - val\_loss: 0.9640 - val\_acc: 0.7869

Epoch 00023: val\_loss did not improve from 0.68393
Epoch 24/60
 - 1s - loss: 0.0182 - acc: 1.0000 - val\_loss: 1.0493 - val\_acc: 0.7459

Epoch 00024: val\_loss did not improve from 0.68393
Epoch 25/60
 - 1s - loss: 0.0167 - acc: 1.0000 - val\_loss: 1.0484 - val\_acc: 0.7869

Epoch 00025: val\_loss did not improve from 0.68393
Epoch 26/60
 - 1s - loss: 0.0119 - acc: 1.0000 - val\_loss: 1.0523 - val\_acc: 0.7705

Epoch 00026: val\_loss did not improve from 0.68393
Epoch 27/60
 - 1s - loss: 0.0100 - acc: 1.0000 - val\_loss: 1.0521 - val\_acc: 0.7869

Epoch 00027: val\_loss did not improve from 0.68393
Epoch 28/60
 - 1s - loss: 0.0092 - acc: 1.0000 - val\_loss: 1.1514 - val\_acc: 0.7541

Epoch 00028: val\_loss did not improve from 0.68393
Epoch 29/60
 - 1s - loss: 0.0090 - acc: 1.0000 - val\_loss: 1.1520 - val\_acc: 0.7623

Epoch 00029: val\_loss did not improve from 0.68393
Epoch 30/60
 - 1s - loss: 0.0081 - acc: 1.0000 - val\_loss: 1.2140 - val\_acc: 0.7623

Epoch 00030: val\_loss did not improve from 0.68393
Epoch 31/60
 - 1s - loss: 0.0064 - acc: 1.0000 - val\_loss: 1.1572 - val\_acc: 0.7787

Epoch 00031: val\_loss did not improve from 0.68393
Epoch 32/60
 - 1s - loss: 0.0058 - acc: 1.0000 - val\_loss: 1.1611 - val\_acc: 0.7869

Epoch 00032: val\_loss did not improve from 0.68393
Epoch 33/60
 - 1s - loss: 0.0043 - acc: 1.0000 - val\_loss: 1.1937 - val\_acc: 0.7623

Epoch 00033: val\_loss did not improve from 0.68393
Epoch 34/60
 - 1s - loss: 0.0039 - acc: 1.0000 - val\_loss: 1.1852 - val\_acc: 0.7869

Epoch 00034: val\_loss did not improve from 0.68393
Epoch 35/60
 - 1s - loss: 0.0030 - acc: 1.0000 - val\_loss: 1.2241 - val\_acc: 0.7705

Epoch 00035: val\_loss did not improve from 0.68393
Epoch 36/60
 - 1s - loss: 0.0031 - acc: 1.0000 - val\_loss: 1.2330 - val\_acc: 0.7787

Epoch 00036: val\_loss did not improve from 0.68393
Epoch 37/60
 - 1s - loss: 0.0026 - acc: 1.0000 - val\_loss: 1.2313 - val\_acc: 0.7705

Epoch 00037: val\_loss did not improve from 0.68393
Epoch 38/60
 - 1s - loss: 0.0024 - acc: 1.0000 - val\_loss: 1.2345 - val\_acc: 0.7705

Epoch 00038: val\_loss did not improve from 0.68393
Epoch 39/60
 - 1s - loss: 0.0021 - acc: 1.0000 - val\_loss: 1.2416 - val\_acc: 0.7787

Epoch 00039: val\_loss did not improve from 0.68393
Epoch 40/60
 - 1s - loss: 0.0021 - acc: 1.0000 - val\_loss: 1.2432 - val\_acc: 0.7869

Epoch 00040: val\_loss did not improve from 0.68393
Epoch 41/60
 - 1s - loss: 0.0018 - acc: 1.0000 - val\_loss: 1.2813 - val\_acc: 0.7869

Epoch 00041: val\_loss did not improve from 0.68393
Epoch 42/60
 - 1s - loss: 0.0017 - acc: 1.0000 - val\_loss: 1.2682 - val\_acc: 0.7787

Epoch 00042: val\_loss did not improve from 0.68393
Epoch 43/60
 - 1s - loss: 0.0016 - acc: 1.0000 - val\_loss: 1.2756 - val\_acc: 0.7787

Epoch 00043: val\_loss did not improve from 0.68393
Epoch 44/60
 - 1s - loss: 0.0014 - acc: 1.0000 - val\_loss: 1.2882 - val\_acc: 0.7869

Epoch 00044: val\_loss did not improve from 0.68393
Epoch 45/60
 - 1s - loss: 0.0014 - acc: 1.0000 - val\_loss: 1.2948 - val\_acc: 0.7705

Epoch 00045: val\_loss did not improve from 0.68393
Epoch 46/60
 - 1s - loss: 0.0013 - acc: 1.0000 - val\_loss: 1.3014 - val\_acc: 0.7787

Epoch 00046: val\_loss did not improve from 0.68393
Epoch 47/60
 - 1s - loss: 0.0012 - acc: 1.0000 - val\_loss: 1.3186 - val\_acc: 0.7787

Epoch 00047: val\_loss did not improve from 0.68393
Epoch 48/60
 - 1s - loss: 0.0012 - acc: 1.0000 - val\_loss: 1.3039 - val\_acc: 0.7869

Epoch 00048: val\_loss did not improve from 0.68393
Epoch 49/60
 - 1s - loss: 0.0011 - acc: 1.0000 - val\_loss: 1.3196 - val\_acc: 0.7869

Epoch 00049: val\_loss did not improve from 0.68393
Epoch 50/60
 - 1s - loss: 0.0010 - acc: 1.0000 - val\_loss: 1.3267 - val\_acc: 0.7869

Epoch 00050: val\_loss did not improve from 0.68393
Epoch 51/60
 - 1s - loss: 9.9675e-04 - acc: 1.0000 - val\_loss: 1.3357 - val\_acc: 0.7869

Epoch 00051: val\_loss did not improve from 0.68393
Epoch 52/60
 - 1s - loss: 9.0084e-04 - acc: 1.0000 - val\_loss: 1.3331 - val\_acc: 0.7869

Epoch 00052: val\_loss did not improve from 0.68393
Epoch 53/60
 - 1s - loss: 8.7175e-04 - acc: 1.0000 - val\_loss: 1.3468 - val\_acc: 0.7869

Epoch 00053: val\_loss did not improve from 0.68393
Epoch 54/60
 - 1s - loss: 8.5232e-04 - acc: 1.0000 - val\_loss: 1.3502 - val\_acc: 0.7787

Epoch 00054: val\_loss did not improve from 0.68393
Epoch 55/60
 - 1s - loss: 7.9014e-04 - acc: 1.0000 - val\_loss: 1.3524 - val\_acc: 0.7869

Epoch 00055: val\_loss did not improve from 0.68393
Epoch 56/60
 - 1s - loss: 7.5347e-04 - acc: 1.0000 - val\_loss: 1.3522 - val\_acc: 0.7869

Epoch 00056: val\_loss did not improve from 0.68393
Epoch 57/60
 - 1s - loss: 7.1122e-04 - acc: 1.0000 - val\_loss: 1.3615 - val\_acc: 0.7869

Epoch 00057: val\_loss did not improve from 0.68393
Epoch 58/60
 - 1s - loss: 6.7484e-04 - acc: 1.0000 - val\_loss: 1.3722 - val\_acc: 0.7869

Epoch 00058: val\_loss did not improve from 0.68393
Epoch 59/60
 - 1s - loss: 6.4352e-04 - acc: 1.0000 - val\_loss: 1.3669 - val\_acc: 0.7869

Epoch 00059: val\_loss did not improve from 0.68393
Epoch 60/60
 - 1s - loss: 6.1018e-04 - acc: 1.0000 - val\_loss: 1.3775 - val\_acc: 0.7869

Epoch 00060: val\_loss did not improve from 0.68393

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}45}]:} \PY{c+c1}{\PYZsh{} Loading the model that performed the best on the validation set}
         \PY{n}{model2\PYZus{}bg}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model\PYZus{}2\PYZus{}bg.weights.best.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Testing the model on the Test data}
         \PY{p}{(}\PY{n}{loss2\PYZus{}bg}\PY{p}{,} \PY{n}{accuracy2\PYZus{}bg}\PY{p}{)} \PY{o}{=} \PY{n}{model2\PYZus{}bg}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}bgsub}\PY{p}{,} \PY{n}{y\PYZus{}test\PYZus{}bgsub}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
         
         \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy on test data: \PYZob{}:.2f\PYZcb{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracy2\PYZus{}bg} \PY{o}{*} \PY{l+m+mi}{100}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy on test data: 72.00\%

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}46}]:} \PY{c+c1}{\PYZsh{} Making the plot larger}
         \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{loss2\PYZus{}bg} \PY{o}{=} \PY{n}{history2\PYZus{}bg}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}                          \PY{c+c1}{\PYZsh{} Loss on the training data}
         \PY{n}{val\PYZus{}loss2\PYZus{}bg} \PY{o}{=} \PY{n}{history2\PYZus{}bg}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}                  \PY{c+c1}{\PYZsh{} Loss on the validation data}
         \PY{n}{epochs} \PY{o}{=} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{61}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{loss2}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ro\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Loss \PYZhy{} BG Sub}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{val\PYZus{}loss2}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{go\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation Loss \PYZhy{} BG Sub}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}46}]:} <matplotlib.legend.Legend at 0x7f2639bb3790>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_65_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}47}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn.metrics} \PY{k+kn}{import} \PY{n}{confusion\PYZus{}matrix}
         \PY{k+kn}{import} \PY{n+nn}{itertools}
         \PY{n}{y\PYZus{}predictions2\PYZus{}bg} \PY{o}{=} \PY{n}{model2\PYZus{}bg}\PY{o}{.}\PY{n}{predict\PYZus{}classes}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}bgsub}\PY{p}{)}\PY{p}{;}
         \PY{n}{y\PYZus{}pred2\PYZus{}bg}\PY{o}{=}\PY{n}{to\PYZus{}categorical}\PY{p}{(}\PY{n}{y\PYZus{}predictions2}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{;}
         \PY{n}{y\PYZus{}test\PYZus{}cm} \PY{o}{=} \PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{;}
         \PY{c+c1}{\PYZsh{}print y\PYZus{}predictions[:10]}
         \PY{c+c1}{\PYZsh{}print y\PYZus{}test[:10]}
         \PY{n}{model\PYZus{}cnf\PYZus{}matrix} \PY{o}{=} \PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{y\PYZus{}test\PYZus{}cm}\PY{p}{,}\PY{n}{y\PYZus{}pred2\PYZus{}bg}\PY{p}{)}\PY{p}{;}
         \PY{n}{confusion\PYZus{}matrix\PYZus{}plot} \PY{o}{=} \PY{n}{plot\PYZus{}confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{model\PYZus{}cnf\PYZus{}matrix}\PY{p}{,} 
                                                       \PY{n}{classes}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{boxing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{handclapping}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{handwaving}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{jogging}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{running}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{walking}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} 
                                                       \PY{n}{normalize}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Normalized confusion matrix

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_66_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}48}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{loss2}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ro\PYZhy{}.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{val\PYZus{}loss2}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{go\PYZhy{}.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{loss2\PYZus{}bg}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bo\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Loss \PYZhy{} BG Sub}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{val\PYZus{}loss2\PYZus{}bg}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{yo\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation Loss \PYZhy{} BG Sub}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}48}]:} <matplotlib.legend.Legend at 0x7f2639740ad0>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_67_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{the-model-that-worked}{%
\section{The model that worked}\label{the-model-that-worked}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}49}]:} \PY{c+c1}{\PYZsh{} from keras.models import Sequential}
         \PY{k+kn}{from} \PY{n+nn}{keras.layers} \PY{k+kn}{import} \PY{n}{Conv3D}\PY{p}{,} \PY{n}{MaxPooling3D}\PY{p}{,} \PY{n}{GlobalAveragePooling3D}\PY{p}{,} \PY{n}{BatchNormalization}
         \PY{k+kn}{from} \PY{n+nn}{keras.layers.core} \PY{k+kn}{import} \PY{n}{Dense}\PY{p}{,} \PY{n}{Dropout}
         
         \PY{c+c1}{\PYZsh{} Using the Sequential Model}
         \PY{n}{model3} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Adding Alternate convolutional and pooling layers}
         \PY{n}{model3}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv3D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                          \PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{n}{model3}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling3D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{model3}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv3D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{model3}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling3D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{model3}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv3D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{model3}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling3D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}model.add(Conv3D(filters=1024, kernel\PYZus{}size=(2, 3, 3), strides=(1, 1, 1), padding=\PYZsq{}same\PYZsq{}, activation=\PYZsq{}relu\PYZsq{}))}
         \PY{c+c1}{\PYZsh{}model.add(MaxPooling3D(pool\PYZus{}size=2, strides=(2, 2, 2), padding=\PYZsq{}same\PYZsq{}))}
         
         \PY{c+c1}{\PYZsh{} A global average pooling layer to get a 1\PYZhy{}d vector}
         \PY{c+c1}{\PYZsh{} The vector will have a depth (same as number of elements in the vector) of 1024}
         \PY{n}{model3}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{GlobalAveragePooling3D}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Hidden layer}
         \PY{n}{model3}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Dropout Layer}
         \PY{n}{model3}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Output layer}
         \PY{n}{model3}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{model3}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
conv3d\_11 (Conv3D)           (None, 35, 20, 20, 16)    736       
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling3d\_11 (MaxPooling (None, 18, 10, 10, 16)    0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv3d\_12 (Conv3D)           (None, 18, 10, 10, 64)    18496     
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling3d\_12 (MaxPooling (None, 9, 5, 5, 64)       0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv3d\_13 (Conv3D)           (None, 9, 5, 5, 256)      295168    
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling3d\_13 (MaxPooling (None, 5, 3, 3, 256)      0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
global\_average\_pooling3d\_5 ( (None, 256)               0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_9 (Dense)              (None, 32)                8224      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_1 (Dropout)          (None, 32)                0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_10 (Dense)             (None, 6)                 198       
=================================================================
Total params: 322,822
Trainable params: 322,822
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}50}]:} \PY{c+c1}{\PYZsh{} Imports}
         \PY{k+kn}{from} \PY{n+nn}{keras.callbacks} \PY{k+kn}{import} \PY{n}{ModelCheckpoint}
         
         \PY{c+c1}{\PYZsh{} Compiling the model}
         \PY{n}{model3}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{nadam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Saving the model that performed the best on the validation set}
         \PY{n}{checkpoint3} \PY{o}{=} \PY{n}{ModelCheckpoint}\PY{p}{(}\PY{n}{filepath}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model\PYZus{}3.weights.best.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{save\PYZus{}best\PYZus{}only}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
         
         
         \PY{n}{history3} \PY{o}{=} \PY{n}{model3}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{60}\PY{p}{,} 
                             \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{X\PYZus{}valid}\PY{p}{,} \PY{n}{y\PYZus{}valid}\PY{p}{)}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{checkpoint3}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Train on 300 samples, validate on 122 samples
Epoch 1/60
 - 10s - loss: 1.7984 - acc: 0.1467 - val\_loss: 1.7780 - val\_acc: 0.2213

Epoch 00001: val\_loss improved from inf to 1.77801, saving model to Model\_3.weights.best.hdf5
Epoch 2/60
 - 9s - loss: 1.7667 - acc: 0.2067 - val\_loss: 1.7126 - val\_acc: 0.2213

Epoch 00002: val\_loss improved from 1.77801 to 1.71256, saving model to Model\_3.weights.best.hdf5
Epoch 3/60
 - 9s - loss: 1.7761 - acc: 0.2433 - val\_loss: 1.8050 - val\_acc: 0.1475

Epoch 00003: val\_loss did not improve from 1.71256
Epoch 4/60
 - 9s - loss: 1.7089 - acc: 0.2400 - val\_loss: 1.5502 - val\_acc: 0.3279

Epoch 00004: val\_loss improved from 1.71256 to 1.55016, saving model to Model\_3.weights.best.hdf5
Epoch 5/60
 - 9s - loss: 1.5364 - acc: 0.3133 - val\_loss: 1.3723 - val\_acc: 0.3934

Epoch 00005: val\_loss improved from 1.55016 to 1.37233, saving model to Model\_3.weights.best.hdf5
Epoch 6/60
 - 9s - loss: 1.4041 - acc: 0.3433 - val\_loss: 1.3018 - val\_acc: 0.3852

Epoch 00006: val\_loss improved from 1.37233 to 1.30180, saving model to Model\_3.weights.best.hdf5
Epoch 7/60
 - 9s - loss: 1.3533 - acc: 0.3867 - val\_loss: 1.2367 - val\_acc: 0.3443

Epoch 00007: val\_loss improved from 1.30180 to 1.23673, saving model to Model\_3.weights.best.hdf5
Epoch 8/60
 - 8s - loss: 1.2730 - acc: 0.3667 - val\_loss: 1.1714 - val\_acc: 0.3443

Epoch 00008: val\_loss improved from 1.23673 to 1.17141, saving model to Model\_3.weights.best.hdf5
Epoch 9/60
 - 9s - loss: 1.2513 - acc: 0.3533 - val\_loss: 1.1669 - val\_acc: 0.4426

Epoch 00009: val\_loss improved from 1.17141 to 1.16691, saving model to Model\_3.weights.best.hdf5
Epoch 10/60
 - 9s - loss: 1.2028 - acc: 0.4033 - val\_loss: 1.1724 - val\_acc: 0.3607

Epoch 00010: val\_loss did not improve from 1.16691
Epoch 11/60
 - 9s - loss: 1.1667 - acc: 0.4333 - val\_loss: 1.0054 - val\_acc: 0.4836

Epoch 00011: val\_loss improved from 1.16691 to 1.00540, saving model to Model\_3.weights.best.hdf5
Epoch 12/60
 - 9s - loss: 1.1908 - acc: 0.4167 - val\_loss: 1.0302 - val\_acc: 0.4508

Epoch 00012: val\_loss did not improve from 1.00540
Epoch 13/60
 - 9s - loss: 1.1504 - acc: 0.4200 - val\_loss: 0.9979 - val\_acc: 0.4426

Epoch 00013: val\_loss improved from 1.00540 to 0.99794, saving model to Model\_3.weights.best.hdf5
Epoch 14/60
 - 9s - loss: 1.0500 - acc: 0.4900 - val\_loss: 0.9093 - val\_acc: 0.6393

Epoch 00014: val\_loss improved from 0.99794 to 0.90932, saving model to Model\_3.weights.best.hdf5
Epoch 15/60
 - 9s - loss: 1.3228 - acc: 0.4567 - val\_loss: 1.2525 - val\_acc: 0.4344

Epoch 00015: val\_loss did not improve from 0.90932
Epoch 16/60
 - 8s - loss: 1.0888 - acc: 0.4867 - val\_loss: 1.0112 - val\_acc: 0.5246

Epoch 00016: val\_loss did not improve from 0.90932
Epoch 17/60
 - 9s - loss: 1.0275 - acc: 0.5067 - val\_loss: 0.8976 - val\_acc: 0.5984

Epoch 00017: val\_loss improved from 0.90932 to 0.89758, saving model to Model\_3.weights.best.hdf5
Epoch 18/60
 - 9s - loss: 1.2113 - acc: 0.4267 - val\_loss: 1.0500 - val\_acc: 0.4262

Epoch 00018: val\_loss did not improve from 0.89758
Epoch 19/60
 - 9s - loss: 1.0577 - acc: 0.4667 - val\_loss: 0.9829 - val\_acc: 0.4262

Epoch 00019: val\_loss did not improve from 0.89758
Epoch 20/60
 - 9s - loss: 1.0095 - acc: 0.4833 - val\_loss: 0.9361 - val\_acc: 0.4918

Epoch 00020: val\_loss did not improve from 0.89758
Epoch 21/60
 - 9s - loss: 0.9986 - acc: 0.4867 - val\_loss: 0.8608 - val\_acc: 0.6066

Epoch 00021: val\_loss improved from 0.89758 to 0.86076, saving model to Model\_3.weights.best.hdf5
Epoch 22/60
 - 9s - loss: 0.9436 - acc: 0.4900 - val\_loss: 0.8621 - val\_acc: 0.6393

Epoch 00022: val\_loss did not improve from 0.86076
Epoch 23/60
 - 8s - loss: 0.9329 - acc: 0.4967 - val\_loss: 0.8621 - val\_acc: 0.5000

Epoch 00023: val\_loss did not improve from 0.86076
Epoch 24/60
 - 9s - loss: 0.8863 - acc: 0.5567 - val\_loss: 0.9299 - val\_acc: 0.4754

Epoch 00024: val\_loss did not improve from 0.86076
Epoch 25/60
 - 9s - loss: 0.9001 - acc: 0.5600 - val\_loss: 3.5008 - val\_acc: 0.3033

Epoch 00025: val\_loss did not improve from 0.86076
Epoch 26/60
 - 9s - loss: 1.5435 - acc: 0.4067 - val\_loss: 1.0307 - val\_acc: 0.5574

Epoch 00026: val\_loss did not improve from 0.86076
Epoch 27/60
 - 9s - loss: 0.9922 - acc: 0.5067 - val\_loss: 0.8912 - val\_acc: 0.5574

Epoch 00027: val\_loss did not improve from 0.86076
Epoch 28/60
 - 9s - loss: 0.8906 - acc: 0.5433 - val\_loss: 0.8522 - val\_acc: 0.5574

Epoch 00028: val\_loss improved from 0.86076 to 0.85218, saving model to Model\_3.weights.best.hdf5
Epoch 29/60
 - 9s - loss: 0.9201 - acc: 0.5400 - val\_loss: 0.8354 - val\_acc: 0.5902

Epoch 00029: val\_loss improved from 0.85218 to 0.83535, saving model to Model\_3.weights.best.hdf5
Epoch 30/60
 - 9s - loss: 0.8721 - acc: 0.5433 - val\_loss: 0.8452 - val\_acc: 0.5984

Epoch 00030: val\_loss did not improve from 0.83535
Epoch 31/60
 - 9s - loss: 0.8183 - acc: 0.5767 - val\_loss: 0.8643 - val\_acc: 0.6393

Epoch 00031: val\_loss did not improve from 0.83535
Epoch 32/60
 - 9s - loss: 0.8646 - acc: 0.5833 - val\_loss: 0.8440 - val\_acc: 0.6721

Epoch 00032: val\_loss did not improve from 0.83535
Epoch 33/60
 - 9s - loss: 0.8255 - acc: 0.5967 - val\_loss: 0.8421 - val\_acc: 0.6639

Epoch 00033: val\_loss did not improve from 0.83535
Epoch 34/60
 - 9s - loss: 0.7764 - acc: 0.5867 - val\_loss: 0.7717 - val\_acc: 0.7377

Epoch 00034: val\_loss improved from 0.83535 to 0.77175, saving model to Model\_3.weights.best.hdf5
Epoch 35/60
 - 9s - loss: 0.8139 - acc: 0.6067 - val\_loss: 0.7399 - val\_acc: 0.6557

Epoch 00035: val\_loss improved from 0.77175 to 0.73994, saving model to Model\_3.weights.best.hdf5
Epoch 36/60
 - 8s - loss: 0.7442 - acc: 0.6500 - val\_loss: 0.8604 - val\_acc: 0.5656

Epoch 00036: val\_loss did not improve from 0.73994
Epoch 37/60
 - 9s - loss: 0.8258 - acc: 0.6300 - val\_loss: 0.7992 - val\_acc: 0.6721

Epoch 00037: val\_loss did not improve from 0.73994
Epoch 38/60
 - 8s - loss: 0.7919 - acc: 0.6133 - val\_loss: 0.8844 - val\_acc: 0.6148

Epoch 00038: val\_loss did not improve from 0.73994
Epoch 39/60
 - 9s - loss: 0.7904 - acc: 0.6467 - val\_loss: 0.8188 - val\_acc: 0.6311

Epoch 00039: val\_loss did not improve from 0.73994
Epoch 40/60
 - 9s - loss: 0.7973 - acc: 0.5933 - val\_loss: 0.7404 - val\_acc: 0.7049

Epoch 00040: val\_loss did not improve from 0.73994
Epoch 41/60
 - 10s - loss: 0.7285 - acc: 0.6667 - val\_loss: 0.8015 - val\_acc: 0.6393

Epoch 00041: val\_loss did not improve from 0.73994
Epoch 42/60
 - 9s - loss: 0.7689 - acc: 0.6367 - val\_loss: 0.7870 - val\_acc: 0.6475

Epoch 00042: val\_loss did not improve from 0.73994
Epoch 43/60
 - 9s - loss: 0.7730 - acc: 0.6300 - val\_loss: 0.7188 - val\_acc: 0.6967

Epoch 00043: val\_loss improved from 0.73994 to 0.71879, saving model to Model\_3.weights.best.hdf5
Epoch 44/60
 - 8s - loss: 0.6841 - acc: 0.6733 - val\_loss: 0.7387 - val\_acc: 0.6967

Epoch 00044: val\_loss did not improve from 0.71879
Epoch 45/60
 - 9s - loss: 0.7738 - acc: 0.6633 - val\_loss: 0.6747 - val\_acc: 0.6885

Epoch 00045: val\_loss improved from 0.71879 to 0.67474, saving model to Model\_3.weights.best.hdf5
Epoch 46/60
 - 9s - loss: 0.7178 - acc: 0.6800 - val\_loss: 0.6970 - val\_acc: 0.7541

Epoch 00046: val\_loss did not improve from 0.67474
Epoch 47/60
 - 9s - loss: 0.6764 - acc: 0.6833 - val\_loss: 0.6645 - val\_acc: 0.6885

Epoch 00047: val\_loss improved from 0.67474 to 0.66448, saving model to Model\_3.weights.best.hdf5
Epoch 48/60
 - 9s - loss: 0.6676 - acc: 0.7033 - val\_loss: 0.6574 - val\_acc: 0.7705

Epoch 00048: val\_loss improved from 0.66448 to 0.65745, saving model to Model\_3.weights.best.hdf5
Epoch 49/60
 - 9s - loss: 0.6844 - acc: 0.6800 - val\_loss: 0.6179 - val\_acc: 0.7295

Epoch 00049: val\_loss improved from 0.65745 to 0.61792, saving model to Model\_3.weights.best.hdf5
Epoch 50/60
 - 9s - loss: 0.6905 - acc: 0.6633 - val\_loss: 0.6683 - val\_acc: 0.7869

Epoch 00050: val\_loss did not improve from 0.61792
Epoch 51/60
 - 8s - loss: 0.6615 - acc: 0.7100 - val\_loss: 0.6557 - val\_acc: 0.7787

Epoch 00051: val\_loss did not improve from 0.61792
Epoch 52/60
 - 9s - loss: 0.6822 - acc: 0.6833 - val\_loss: 0.5945 - val\_acc: 0.7623

Epoch 00052: val\_loss improved from 0.61792 to 0.59446, saving model to Model\_3.weights.best.hdf5
Epoch 53/60
 - 8s - loss: 0.6950 - acc: 0.6667 - val\_loss: 0.6546 - val\_acc: 0.6885

Epoch 00053: val\_loss did not improve from 0.59446
Epoch 54/60
 - 9s - loss: 0.6392 - acc: 0.7167 - val\_loss: 0.5917 - val\_acc: 0.8033

Epoch 00054: val\_loss improved from 0.59446 to 0.59174, saving model to Model\_3.weights.best.hdf5
Epoch 55/60
 - 9s - loss: 0.6660 - acc: 0.6833 - val\_loss: 0.5408 - val\_acc: 0.7869

Epoch 00055: val\_loss improved from 0.59174 to 0.54077, saving model to Model\_3.weights.best.hdf5
Epoch 56/60
 - 9s - loss: 0.6006 - acc: 0.7167 - val\_loss: 0.6268 - val\_acc: 0.7295

Epoch 00056: val\_loss did not improve from 0.54077
Epoch 57/60
 - 9s - loss: 0.6355 - acc: 0.7100 - val\_loss: 0.5816 - val\_acc: 0.7541

Epoch 00057: val\_loss did not improve from 0.54077
Epoch 58/60
 - 9s - loss: 0.6576 - acc: 0.7033 - val\_loss: 0.6113 - val\_acc: 0.7951

Epoch 00058: val\_loss did not improve from 0.54077
Epoch 59/60
 - 9s - loss: 0.7414 - acc: 0.6867 - val\_loss: 0.8004 - val\_acc: 0.7213

Epoch 00059: val\_loss did not improve from 0.54077
Epoch 60/60
 - 9s - loss: 0.7101 - acc: 0.6833 - val\_loss: 0.7074 - val\_acc: 0.6803

Epoch 00060: val\_loss did not improve from 0.54077

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}51}]:} \PY{c+c1}{\PYZsh{} Loading the model that performed the best on the validation set}
         \PY{n}{model3}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model\PYZus{}3.weights.best.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Testing the model on the Test data}
         \PY{p}{(}\PY{n}{loss3}\PY{p}{,} \PY{n}{accuracy3}\PY{p}{)} \PY{o}{=} \PY{n}{model3}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
         
         \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy on test data: \PYZob{}:.2f\PYZcb{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracy3} \PY{o}{*} \PY{l+m+mi}{100}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy on test data: 71.00\%

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}52}]:} \PY{c+c1}{\PYZsh{} Making the plot larger}
         \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{14}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{loss3} \PY{o}{=} \PY{n}{history3}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}                          \PY{c+c1}{\PYZsh{} Loss on the training data}
         \PY{n}{val\PYZus{}loss3} \PY{o}{=} \PY{n}{history3}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}                  \PY{c+c1}{\PYZsh{} Loss on the validation data}
         \PY{n}{epochs} \PY{o}{=} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{61}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{loss3}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ro\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{val\PYZus{}loss3}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{go\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}52}]:} <matplotlib.legend.Legend at 0x7f262f93fb90>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_72_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}53}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn.metrics} \PY{k+kn}{import} \PY{n}{confusion\PYZus{}matrix}
         \PY{k+kn}{import} \PY{n+nn}{itertools}
         \PY{n}{y\PYZus{}predictions3} \PY{o}{=} \PY{n}{model3}\PY{o}{.}\PY{n}{predict\PYZus{}classes}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{p}{;}
         \PY{n}{y\PYZus{}pred3}\PY{o}{=}\PY{n}{to\PYZus{}categorical}\PY{p}{(}\PY{n}{y\PYZus{}predictions3}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{;}
         \PY{n}{y\PYZus{}test\PYZus{}cm} \PY{o}{=} \PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{;}
         
         \PY{c+c1}{\PYZsh{}print y\PYZus{}predictions3[:50]}
         \PY{c+c1}{\PYZsh{}print y\PYZus{}test[:50]}
         \PY{n}{model\PYZus{}cnf\PYZus{}matrix3} \PY{o}{=} \PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{y\PYZus{}test\PYZus{}cm}\PY{p}{,}\PY{n}{y\PYZus{}pred3}\PY{p}{)}\PY{p}{;}
         \PY{c+c1}{\PYZsh{}print(model\PYZus{}cnf\PYZus{}matrix3)}
         \PY{n}{confusion\PYZus{}matrix\PYZus{}plot} \PY{o}{=} \PY{n}{plot\PYZus{}confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{model\PYZus{}cnf\PYZus{}matrix3}\PY{p}{,} 
                                                       \PY{n}{classes}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{boxing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{handclapping}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{handwaving}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{jogging}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{running}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{walking}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} 
                                                       \PY{n}{normalize}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Normalized confusion matrix

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_73_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{model3-bg-subtraction}{%
\subsection{Model3 BG Subtraction}\label{model3-bg-subtraction}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}73}]:} \PY{k+kn}{from} \PY{n+nn}{keras.models} \PY{k+kn}{import} \PY{n}{Sequential}
         \PY{k+kn}{from} \PY{n+nn}{keras.layers} \PY{k+kn}{import} \PY{n}{Conv3D}\PY{p}{,} \PY{n}{MaxPooling3D}\PY{p}{,} \PY{n}{GlobalAveragePooling3D}\PY{p}{,} \PY{n}{BatchNormalization}
         \PY{k+kn}{from} \PY{n+nn}{keras.layers.core} \PY{k+kn}{import} \PY{n}{Dense}\PY{p}{,} \PY{n}{Dropout}
         
         \PY{c+c1}{\PYZsh{} Using the Sequential Model}
         \PY{n}{model3\PYZus{}bg} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Adding Alternate convolutional and pooling layers}
         \PY{n}{model3\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv3D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                          \PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{n}{model3\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling3D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{model3\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv3D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{model3\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling3D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{model3\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv3D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{model3\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling3D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}model.add(Conv3D(filters=1024, kernel\PYZus{}size=(2, 3, 3), strides=(1, 1, 1), padding=\PYZsq{}same\PYZsq{}, activation=\PYZsq{}relu\PYZsq{}))}
         \PY{c+c1}{\PYZsh{}model.add(MaxPooling3D(pool\PYZus{}size=2, strides=(2, 2, 2), padding=\PYZsq{}same\PYZsq{}))}
         
         \PY{c+c1}{\PYZsh{} A global average pooling layer to get a 1\PYZhy{}d vector}
         \PY{c+c1}{\PYZsh{} The vector will have a depth (same as number of elements in the vector) of 1024}
         \PY{n}{model3\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{GlobalAveragePooling3D}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         \PY{n}{model3\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} Hidden layer}
         \PY{n}{model3\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{model3\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{)}
         \PY{n}{model3\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Dropout Layer}
         \PY{n}{model3\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Output layer}
         \PY{n}{model3\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{model3\PYZus{}bg}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
conv3d\_26 (Conv3D)           (None, 35, 20, 20, 16)    2016      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling3d\_26 (MaxPooling (None, 18, 10, 10, 16)    0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv3d\_27 (Conv3D)           (None, 18, 10, 10, 64)    27712     
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling3d\_27 (MaxPooling (None, 9, 5, 5, 64)       0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv3d\_28 (Conv3D)           (None, 9, 5, 5, 256)      442624    
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling3d\_28 (MaxPooling (None, 5, 3, 3, 256)      0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
global\_average\_pooling3d\_10  (None, 256)               0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_15 (Dropout)         (None, 256)               0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_20 (Dense)             (None, 64)                16448     
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_16 (Dropout)         (None, 64)                0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_21 (Dense)             (None, 32)                2080      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_17 (Dropout)         (None, 32)                0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_22 (Dense)             (None, 6)                 198       
=================================================================
Total params: 491,078
Trainable params: 491,078
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}74}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{} Imports}
         \PY{k+kn}{from} \PY{n+nn}{keras.callbacks} \PY{k+kn}{import} \PY{n}{ModelCheckpoint}
         
         \PY{c+c1}{\PYZsh{} Compiling the model}
         \PY{n}{model3\PYZus{}bg}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{nadam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Saving the model that performed the best on the validation set}
         \PY{n}{checkpoint3\PYZus{}bg} \PY{o}{=} \PY{n}{ModelCheckpoint}\PY{p}{(}\PY{n}{filepath}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model\PYZus{}3\PYZus{}bg.weights.best.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{save\PYZus{}best\PYZus{}only}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
         
         
         \PY{n}{history3\PYZus{}bg} \PY{o}{=} \PY{n}{model3\PYZus{}bg}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}bgsub}\PY{p}{,} \PY{n}{y\PYZus{}train\PYZus{}bgsub}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{60}\PY{p}{,} 
                             \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{X\PYZus{}valid\PYZus{}bgsub}\PY{p}{,} \PY{n}{y\PYZus{}valid\PYZus{}bgsub}\PY{p}{)}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{checkpoint3\PYZus{}bg}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Train on 300 samples, validate on 122 samples
Epoch 1/60
300/300 [==============================] - 14s 46ms/step - loss: 1.7694 - acc: 0.1867 - val\_loss: 1.6784 - val\_acc: 0.2295

Epoch 00001: val\_loss improved from inf to 1.67837, saving model to Model\_3\_bg.weights.best.hdf5
Epoch 2/60
300/300 [==============================] - 12s 41ms/step - loss: 1.6728 - acc: 0.2433 - val\_loss: 1.5299 - val\_acc: 0.2377

Epoch 00002: val\_loss improved from 1.67837 to 1.52991, saving model to Model\_3\_bg.weights.best.hdf5
Epoch 3/60
300/300 [==============================] - 12s 41ms/step - loss: 1.5662 - acc: 0.3300 - val\_loss: 1.4814 - val\_acc: 0.2049

Epoch 00003: val\_loss improved from 1.52991 to 1.48142, saving model to Model\_3\_bg.weights.best.hdf5
Epoch 4/60
300/300 [==============================] - 12s 41ms/step - loss: 1.4951 - acc: 0.3433 - val\_loss: 1.4161 - val\_acc: 0.3607

Epoch 00004: val\_loss improved from 1.48142 to 1.41607, saving model to Model\_3\_bg.weights.best.hdf5
Epoch 5/60
300/300 [==============================] - 13s 44ms/step - loss: 1.4554 - acc: 0.3267 - val\_loss: 1.3782 - val\_acc: 0.3852

Epoch 00005: val\_loss improved from 1.41607 to 1.37823, saving model to Model\_3\_bg.weights.best.hdf5
Epoch 6/60
300/300 [==============================] - 14s 45ms/step - loss: 1.4614 - acc: 0.3667 - val\_loss: 1.3471 - val\_acc: 0.2787

Epoch 00006: val\_loss improved from 1.37823 to 1.34715, saving model to Model\_3\_bg.weights.best.hdf5
Epoch 7/60
300/300 [==============================] - 14s 46ms/step - loss: 1.3885 - acc: 0.3167 - val\_loss: 1.2862 - val\_acc: 0.4016

Epoch 00007: val\_loss improved from 1.34715 to 1.28621, saving model to Model\_3\_bg.weights.best.hdf5
Epoch 8/60
300/300 [==============================] - 13s 43ms/step - loss: 1.3177 - acc: 0.3900 - val\_loss: 1.3017 - val\_acc: 0.3033

Epoch 00008: val\_loss did not improve from 1.28621
Epoch 9/60
300/300 [==============================] - 13s 42ms/step - loss: 1.3016 - acc: 0.3900 - val\_loss: 1.1849 - val\_acc: 0.3279

Epoch 00009: val\_loss improved from 1.28621 to 1.18485, saving model to Model\_3\_bg.weights.best.hdf5
Epoch 10/60
300/300 [==============================] - 13s 44ms/step - loss: 1.2469 - acc: 0.3967 - val\_loss: 1.2242 - val\_acc: 0.4344

Epoch 00010: val\_loss did not improve from 1.18485
Epoch 11/60
300/300 [==============================] - 13s 44ms/step - loss: 1.1731 - acc: 0.4333 - val\_loss: 1.0259 - val\_acc: 0.4754

Epoch 00011: val\_loss improved from 1.18485 to 1.02587, saving model to Model\_3\_bg.weights.best.hdf5
Epoch 12/60
300/300 [==============================] - 14s 48ms/step - loss: 1.1101 - acc: 0.4400 - val\_loss: 0.9689 - val\_acc: 0.4508

Epoch 00012: val\_loss improved from 1.02587 to 0.96894, saving model to Model\_3\_bg.weights.best.hdf5
Epoch 13/60
300/300 [==============================] - 12s 39ms/step - loss: 1.1344 - acc: 0.4800 - val\_loss: 0.9954 - val\_acc: 0.4180

Epoch 00013: val\_loss did not improve from 0.96894
Epoch 14/60
300/300 [==============================] - 12s 39ms/step - loss: 1.0535 - acc: 0.4767 - val\_loss: 0.9753 - val\_acc: 0.5082

Epoch 00014: val\_loss did not improve from 0.96894
Epoch 15/60
300/300 [==============================] - 12s 40ms/step - loss: 1.0002 - acc: 0.4667 - val\_loss: 0.9099 - val\_acc: 0.5410

Epoch 00015: val\_loss improved from 0.96894 to 0.90985, saving model to Model\_3\_bg.weights.best.hdf5
Epoch 16/60
300/300 [==============================] - 12s 39ms/step - loss: 0.9570 - acc: 0.5067 - val\_loss: 0.8885 - val\_acc: 0.5574

Epoch 00016: val\_loss improved from 0.90985 to 0.88849, saving model to Model\_3\_bg.weights.best.hdf5
Epoch 17/60
300/300 [==============================] - 12s 39ms/step - loss: 0.9266 - acc: 0.4700 - val\_loss: 0.8888 - val\_acc: 0.4754

Epoch 00017: val\_loss did not improve from 0.88849
Epoch 18/60
300/300 [==============================] - 12s 39ms/step - loss: 0.8518 - acc: 0.5567 - val\_loss: 0.8710 - val\_acc: 0.5656

Epoch 00018: val\_loss improved from 0.88849 to 0.87101, saving model to Model\_3\_bg.weights.best.hdf5
Epoch 19/60
300/300 [==============================] - 12s 40ms/step - loss: 0.9680 - acc: 0.5100 - val\_loss: 0.8600 - val\_acc: 0.5738

Epoch 00019: val\_loss improved from 0.87101 to 0.86004, saving model to Model\_3\_bg.weights.best.hdf5
Epoch 20/60
300/300 [==============================] - 12s 40ms/step - loss: 0.8512 - acc: 0.5067 - val\_loss: 0.9190 - val\_acc: 0.4590

Epoch 00020: val\_loss did not improve from 0.86004
Epoch 21/60
300/300 [==============================] - 12s 40ms/step - loss: 0.8991 - acc: 0.5500 - val\_loss: 0.9887 - val\_acc: 0.5164

Epoch 00021: val\_loss did not improve from 0.86004
Epoch 22/60
300/300 [==============================] - 12s 40ms/step - loss: 0.9034 - acc: 0.5367 - val\_loss: 0.9014 - val\_acc: 0.5246

Epoch 00022: val\_loss did not improve from 0.86004
Epoch 23/60
300/300 [==============================] - 12s 39ms/step - loss: 0.8458 - acc: 0.5600 - val\_loss: 0.8729 - val\_acc: 0.5738

Epoch 00023: val\_loss did not improve from 0.86004
Epoch 24/60
300/300 [==============================] - 12s 39ms/step - loss: 0.8531 - acc: 0.5300 - val\_loss: 0.8281 - val\_acc: 0.5738

Epoch 00024: val\_loss improved from 0.86004 to 0.82813, saving model to Model\_3\_bg.weights.best.hdf5
Epoch 25/60
300/300 [==============================] - 12s 39ms/step - loss: 0.8896 - acc: 0.5333 - val\_loss: 0.8674 - val\_acc: 0.5656

Epoch 00025: val\_loss did not improve from 0.82813
Epoch 26/60
300/300 [==============================] - 12s 40ms/step - loss: 0.8626 - acc: 0.5367 - val\_loss: 0.8982 - val\_acc: 0.5328

Epoch 00026: val\_loss did not improve from 0.82813
Epoch 27/60
300/300 [==============================] - 12s 40ms/step - loss: 0.8149 - acc: 0.5600 - val\_loss: 0.9014 - val\_acc: 0.4590

Epoch 00027: val\_loss did not improve from 0.82813
Epoch 28/60
300/300 [==============================] - 12s 39ms/step - loss: 0.7772 - acc: 0.5267 - val\_loss: 0.9115 - val\_acc: 0.5738

Epoch 00028: val\_loss did not improve from 0.82813
Epoch 29/60
300/300 [==============================] - 12s 39ms/step - loss: 0.8337 - acc: 0.5100 - val\_loss: 0.8858 - val\_acc: 0.5492

Epoch 00029: val\_loss did not improve from 0.82813
Epoch 30/60
300/300 [==============================] - 12s 40ms/step - loss: 0.8169 - acc: 0.5600 - val\_loss: 0.8673 - val\_acc: 0.5574

Epoch 00030: val\_loss did not improve from 0.82813
Epoch 31/60
300/300 [==============================] - 12s 39ms/step - loss: 0.8213 - acc: 0.5433 - val\_loss: 0.8950 - val\_acc: 0.6066

Epoch 00031: val\_loss did not improve from 0.82813
Epoch 32/60
300/300 [==============================] - 12s 40ms/step - loss: 0.7896 - acc: 0.5733 - val\_loss: 0.9033 - val\_acc: 0.5328

Epoch 00032: val\_loss did not improve from 0.82813
Epoch 33/60
300/300 [==============================] - 12s 39ms/step - loss: 0.8191 - acc: 0.5400 - val\_loss: 0.8649 - val\_acc: 0.5820

Epoch 00033: val\_loss did not improve from 0.82813
Epoch 34/60
300/300 [==============================] - 12s 39ms/step - loss: 0.7975 - acc: 0.5800 - val\_loss: 0.9361 - val\_acc: 0.5328

Epoch 00034: val\_loss did not improve from 0.82813
Epoch 35/60
300/300 [==============================] - 12s 40ms/step - loss: 0.8168 - acc: 0.5667 - val\_loss: 0.8186 - val\_acc: 0.4672

Epoch 00035: val\_loss improved from 0.82813 to 0.81856, saving model to Model\_3\_bg.weights.best.hdf5
Epoch 36/60
300/300 [==============================] - 12s 39ms/step - loss: 0.7471 - acc: 0.5500 - val\_loss: 0.8579 - val\_acc: 0.5164

Epoch 00036: val\_loss did not improve from 0.81856
Epoch 37/60
300/300 [==============================] - 12s 40ms/step - loss: 0.7135 - acc: 0.6033 - val\_loss: 1.0262 - val\_acc: 0.5000

Epoch 00037: val\_loss did not improve from 0.81856
Epoch 38/60
300/300 [==============================] - 12s 39ms/step - loss: 0.8373 - acc: 0.5500 - val\_loss: 0.9708 - val\_acc: 0.5656

Epoch 00038: val\_loss did not improve from 0.81856
Epoch 39/60
300/300 [==============================] - 12s 39ms/step - loss: 0.7769 - acc: 0.5833 - val\_loss: 0.9363 - val\_acc: 0.5656

Epoch 00039: val\_loss did not improve from 0.81856
Epoch 40/60
300/300 [==============================] - 12s 40ms/step - loss: 0.7930 - acc: 0.6067 - val\_loss: 0.8937 - val\_acc: 0.5656

Epoch 00040: val\_loss did not improve from 0.81856
Epoch 41/60
300/300 [==============================] - 12s 38ms/step - loss: 0.6887 - acc: 0.6033 - val\_loss: 0.9356 - val\_acc: 0.5820

Epoch 00041: val\_loss did not improve from 0.81856
Epoch 42/60
300/300 [==============================] - 12s 40ms/step - loss: 0.7994 - acc: 0.6033 - val\_loss: 0.8603 - val\_acc: 0.6230

Epoch 00042: val\_loss did not improve from 0.81856
Epoch 43/60
300/300 [==============================] - 12s 40ms/step - loss: 0.6887 - acc: 0.6233 - val\_loss: 0.9088 - val\_acc: 0.6230

Epoch 00043: val\_loss did not improve from 0.81856
Epoch 44/60
300/300 [==============================] - 12s 40ms/step - loss: 0.6812 - acc: 0.6500 - val\_loss: 0.9068 - val\_acc: 0.6639

Epoch 00044: val\_loss did not improve from 0.81856
Epoch 45/60
300/300 [==============================] - 12s 40ms/step - loss: 0.6777 - acc: 0.6233 - val\_loss: 0.8710 - val\_acc: 0.6967

Epoch 00045: val\_loss did not improve from 0.81856
Epoch 46/60
300/300 [==============================] - 12s 39ms/step - loss: 0.6586 - acc: 0.6500 - val\_loss: 0.9186 - val\_acc: 0.6230

Epoch 00046: val\_loss did not improve from 0.81856
Epoch 47/60
300/300 [==============================] - 12s 39ms/step - loss: 0.7704 - acc: 0.6433 - val\_loss: 0.9160 - val\_acc: 0.6721

Epoch 00047: val\_loss did not improve from 0.81856
Epoch 48/60
300/300 [==============================] - 12s 40ms/step - loss: 0.6815 - acc: 0.6733 - val\_loss: 0.9183 - val\_acc: 0.6967

Epoch 00048: val\_loss did not improve from 0.81856
Epoch 49/60
300/300 [==============================] - 12s 39ms/step - loss: 0.7001 - acc: 0.6333 - val\_loss: 0.8876 - val\_acc: 0.6148

Epoch 00049: val\_loss did not improve from 0.81856
Epoch 50/60
300/300 [==============================] - 12s 39ms/step - loss: 0.6430 - acc: 0.6300 - val\_loss: 0.8188 - val\_acc: 0.6393

Epoch 00050: val\_loss did not improve from 0.81856
Epoch 51/60
300/300 [==============================] - 12s 40ms/step - loss: 0.6401 - acc: 0.6767 - val\_loss: 0.9215 - val\_acc: 0.6557

Epoch 00051: val\_loss did not improve from 0.81856
Epoch 52/60
300/300 [==============================] - 12s 39ms/step - loss: 0.6469 - acc: 0.7133 - val\_loss: 0.8996 - val\_acc: 0.5902

Epoch 00052: val\_loss did not improve from 0.81856
Epoch 53/60
300/300 [==============================] - 12s 40ms/step - loss: 0.6355 - acc: 0.6967 - val\_loss: 1.2114 - val\_acc: 0.5738

Epoch 00053: val\_loss did not improve from 0.81856
Epoch 54/60
300/300 [==============================] - 12s 39ms/step - loss: 0.6176 - acc: 0.6900 - val\_loss: 0.9933 - val\_acc: 0.6311

Epoch 00054: val\_loss did not improve from 0.81856
Epoch 55/60
300/300 [==============================] - 12s 40ms/step - loss: 0.6546 - acc: 0.6867 - val\_loss: 0.8370 - val\_acc: 0.7377

Epoch 00055: val\_loss did not improve from 0.81856
Epoch 56/60
300/300 [==============================] - 12s 39ms/step - loss: 0.6154 - acc: 0.7000 - val\_loss: 0.7232 - val\_acc: 0.7295

Epoch 00056: val\_loss improved from 0.81856 to 0.72324, saving model to Model\_3\_bg.weights.best.hdf5
Epoch 57/60
300/300 [==============================] - 12s 40ms/step - loss: 0.6334 - acc: 0.7133 - val\_loss: 0.9991 - val\_acc: 0.6311

Epoch 00057: val\_loss did not improve from 0.72324
Epoch 58/60
300/300 [==============================] - 12s 40ms/step - loss: 0.7053 - acc: 0.6933 - val\_loss: 0.8532 - val\_acc: 0.5738

Epoch 00058: val\_loss did not improve from 0.72324
Epoch 59/60
300/300 [==============================] - 12s 39ms/step - loss: 0.5915 - acc: 0.7433 - val\_loss: 0.8216 - val\_acc: 0.7213

Epoch 00059: val\_loss did not improve from 0.72324
Epoch 60/60
300/300 [==============================] - 12s 39ms/step - loss: 0.4986 - acc: 0.7633 - val\_loss: 0.8381 - val\_acc: 0.7131

Epoch 00060: val\_loss did not improve from 0.72324

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}75}]:} \PY{c+c1}{\PYZsh{} Loading the model that performed the best on the validation set}
         \PY{n}{model3\PYZus{}bg}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model\PYZus{}3\PYZus{}bg.weights.best.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Testing the model on the Test data}
         \PY{p}{(}\PY{n}{loss3\PYZus{}bg}\PY{p}{,} \PY{n}{accuracy3\PYZus{}bg}\PY{p}{)} \PY{o}{=} \PY{n}{model3\PYZus{}bg}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}bgsub}\PY{p}{,} \PY{n}{y\PYZus{}test\PYZus{}bgsub}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
         
         \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy on test data: \PYZob{}:.2f\PYZcb{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracy3\PYZus{}bg} \PY{o}{*} \PY{l+m+mi}{100}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy on test data: 60.00\%

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}76}]:} \PY{c+c1}{\PYZsh{} Making the plot larger}
         \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{14}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{loss3\PYZus{}bg} \PY{o}{=} \PY{n}{history3\PYZus{}bg}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}                          \PY{c+c1}{\PYZsh{} Loss on the training data}
         \PY{n}{val\PYZus{}loss3\PYZus{}bg} \PY{o}{=} \PY{n}{history3\PYZus{}bg}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}                  \PY{c+c1}{\PYZsh{} Loss on the validation data}
         \PY{n}{epochs} \PY{o}{=} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{61}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{loss3\PYZus{}bg}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ro\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{val\PYZus{}loss3\PYZus{}bg}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{go\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}76}]:} <matplotlib.legend.Legend at 0x7f26125e68d0>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_78_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}77}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{loss3}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ro\PYZhy{}.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{val\PYZus{}loss3}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{go\PYZhy{}.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{loss3\PYZus{}bg}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bo\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Loss \PYZhy{} BG Sub}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{val\PYZus{}loss3\PYZus{}bg}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{yo\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation Loss \PYZhy{} BG Sub}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}77}]:} <matplotlib.legend.Legend at 0x7f261256bf10>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_79_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}78}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn.metrics} \PY{k+kn}{import} \PY{n}{confusion\PYZus{}matrix}
         \PY{k+kn}{import} \PY{n+nn}{itertools}
         \PY{n}{y\PYZus{}predictions3\PYZus{}bg} \PY{o}{=} \PY{n}{model3\PYZus{}bg}\PY{o}{.}\PY{n}{predict\PYZus{}classes}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}bgsub}\PY{p}{)}\PY{p}{;}
         \PY{n}{y\PYZus{}pred3\PYZus{}bg}\PY{o}{=}\PY{n}{to\PYZus{}categorical}\PY{p}{(}\PY{n}{y\PYZus{}predictions3\PYZus{}bg}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{;}
         \PY{n}{y\PYZus{}test\PYZus{}cm} \PY{o}{=} \PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{;}
         \PY{c+c1}{\PYZsh{}print y\PYZus{}predictions[:50]}
         \PY{c+c1}{\PYZsh{}print y\PYZus{}test[:50]}
         \PY{n}{model\PYZus{}cnf\PYZus{}matrix3\PYZus{}bg} \PY{o}{=} \PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{y\PYZus{}test\PYZus{}cm}\PY{p}{,}\PY{n}{y\PYZus{}pred3\PYZus{}bg}\PY{p}{)}\PY{p}{;}
         \PY{n}{confusion\PYZus{}matrix\PYZus{}plot} \PY{o}{=} \PY{n}{plot\PYZus{}confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{model\PYZus{}cnf\PYZus{}matrix3\PYZus{}bg}\PY{p}{,} 
                                                       \PY{n}{classes}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{boxing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{handclapping}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{handwaving}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{jogging}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{running}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{walking}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} 
                                                       \PY{n}{normalize}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Normalized confusion matrix

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_80_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{the-model-with-more-dropouts}{%
\section{The model with more
dropouts}\label{the-model-with-more-dropouts}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}60}]:} \PY{c+c1}{\PYZsh{} from keras.models import Sequential}
         \PY{k+kn}{from} \PY{n+nn}{keras.layers} \PY{k+kn}{import} \PY{n}{Conv3D}\PY{p}{,} \PY{n}{MaxPooling3D}\PY{p}{,} \PY{n}{GlobalAveragePooling3D}\PY{p}{,} \PY{n}{BatchNormalization}
         \PY{k+kn}{from} \PY{n+nn}{keras.layers.core} \PY{k+kn}{import} \PY{n}{Dense}\PY{p}{,} \PY{n}{Dropout}
         
         \PY{c+c1}{\PYZsh{} Using the Sequential Model}
         \PY{n}{model10} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Adding Alternate convolutional and pooling layers}
         \PY{n}{model10}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv3D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                          \PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{n}{model10}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.3}\PY{p}{)}\PY{p}{)}
         \PY{n}{model10}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling3D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{model10}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv3D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{model10}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.3}\PY{p}{)}\PY{p}{)}
         \PY{n}{model10}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling3D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{model10}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv3D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{model10}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{)}
         \PY{n}{model10}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling3D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}model.add(Conv3D(filters=1024, kernel\PYZus{}size=(2, 3, 3), strides=(1, 1, 1), padding=\PYZsq{}same\PYZsq{}, activation=\PYZsq{}relu\PYZsq{}))}
         \PY{c+c1}{\PYZsh{}model.add(MaxPooling3D(pool\PYZus{}size=2, strides=(2, 2, 2), padding=\PYZsq{}same\PYZsq{}))}
         
         \PY{c+c1}{\PYZsh{} A global average pooling layer to get a 1\PYZhy{}d vector}
         \PY{c+c1}{\PYZsh{} The vector will have a depth (same as number of elements in the vector) of 1024}
         \PY{n}{model10}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{GlobalAveragePooling3D}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Hidden layer}
         \PY{n}{model10}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Dropout Layer}
         \PY{n}{model10}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Output layer}
         \PY{n}{model10}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{model10}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
conv3d\_17 (Conv3D)           (None, 35, 20, 20, 16)    736       
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_3 (Dropout)          (None, 35, 20, 20, 16)    0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling3d\_17 (MaxPooling (None, 18, 10, 10, 16)    0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv3d\_18 (Conv3D)           (None, 18, 10, 10, 64)    18496     
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_4 (Dropout)          (None, 18, 10, 10, 64)    0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling3d\_18 (MaxPooling (None, 9, 5, 5, 64)       0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv3d\_19 (Conv3D)           (None, 9, 5, 5, 256)      295168    
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_5 (Dropout)          (None, 9, 5, 5, 256)      0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling3d\_19 (MaxPooling (None, 5, 3, 3, 256)      0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
global\_average\_pooling3d\_7 ( (None, 256)               0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_13 (Dense)             (None, 32)                8224      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_6 (Dropout)          (None, 32)                0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_14 (Dense)             (None, 6)                 198       
=================================================================
Total params: 322,822
Trainable params: 322,822
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}61}]:} \PY{c+c1}{\PYZsh{} Imports}
         \PY{k+kn}{from} \PY{n+nn}{keras.callbacks} \PY{k+kn}{import} \PY{n}{ModelCheckpoint}
         
         \PY{c+c1}{\PYZsh{} Compiling the model}
         \PY{n}{model10}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{nadam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Saving the model that performed the best on the validation set}
         \PY{n}{checkpoint10} \PY{o}{=} \PY{n}{ModelCheckpoint}\PY{p}{(}\PY{n}{filepath}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model\PYZus{}10.weights.best.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{save\PYZus{}best\PYZus{}only}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
         
         
         \PY{n}{history10} \PY{o}{=} \PY{n}{model10}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} 
                             \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{X\PYZus{}valid}\PY{p}{,} \PY{n}{y\PYZus{}valid}\PY{p}{)}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{checkpoint10}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Train on 300 samples, validate on 122 samples
Epoch 1/100
 - 18s - loss: 1.8469 - acc: 0.2267 - val\_loss: 1.7901 - val\_acc: 0.1475

Epoch 00001: val\_loss improved from inf to 1.79013, saving model to Model\_10.weights.best.hdf5
Epoch 2/100
 - 16s - loss: 1.7841 - acc: 0.1933 - val\_loss: 1.7843 - val\_acc: 0.1475

Epoch 00002: val\_loss improved from 1.79013 to 1.78429, saving model to Model\_10.weights.best.hdf5
Epoch 3/100
 - 17s - loss: 1.8574 - acc: 0.1833 - val\_loss: 1.7805 - val\_acc: 0.2213

Epoch 00003: val\_loss improved from 1.78429 to 1.78049, saving model to Model\_10.weights.best.hdf5
Epoch 4/100
 - 16s - loss: 1.8045 - acc: 0.1833 - val\_loss: 1.7802 - val\_acc: 0.2213

Epoch 00004: val\_loss improved from 1.78049 to 1.78021, saving model to Model\_10.weights.best.hdf5
Epoch 5/100
 - 16s - loss: 9.5724 - acc: 0.2033 - val\_loss: 12.6831 - val\_acc: 0.2131

Epoch 00005: val\_loss did not improve from 1.78021
Epoch 6/100
 - 16s - loss: 11.4169 - acc: 0.2233 - val\_loss: 12.6831 - val\_acc: 0.2131

Epoch 00006: val\_loss did not improve from 1.78021
Epoch 7/100
 - 16s - loss: 11.9530 - acc: 0.1833 - val\_loss: 12.6831 - val\_acc: 0.2131

Epoch 00007: val\_loss did not improve from 1.78021
Epoch 8/100
 - 16s - loss: 11.5337 - acc: 0.2167 - val\_loss: 12.6831 - val\_acc: 0.2131

Epoch 00008: val\_loss did not improve from 1.78021
Epoch 9/100
 - 16s - loss: 11.2707 - acc: 0.1867 - val\_loss: 12.6831 - val\_acc: 0.2131

Epoch 00009: val\_loss did not improve from 1.78021
Epoch 10/100
 - 17s - loss: 10.8916 - acc: 0.2200 - val\_loss: 12.6831 - val\_acc: 0.2131

Epoch 00010: val\_loss did not improve from 1.78021
Epoch 11/100
 - 16s - loss: 8.2734 - acc: 0.1567 - val\_loss: 1.7803 - val\_acc: 0.2213

Epoch 00011: val\_loss did not improve from 1.78021
Epoch 12/100
 - 16s - loss: 1.7766 - acc: 0.2000 - val\_loss: 1.7813 - val\_acc: 0.2213

Epoch 00012: val\_loss did not improve from 1.78021
Epoch 13/100
 - 16s - loss: 1.7737 - acc: 0.2033 - val\_loss: 1.7826 - val\_acc: 0.1475

Epoch 00013: val\_loss did not improve from 1.78021
Epoch 14/100
 - 16s - loss: 1.7728 - acc: 0.2133 - val\_loss: 1.7840 - val\_acc: 0.1475

Epoch 00014: val\_loss did not improve from 1.78021
Epoch 15/100
 - 16s - loss: 1.7719 - acc: 0.2133 - val\_loss: 1.7842 - val\_acc: 0.1475

Epoch 00015: val\_loss did not improve from 1.78021
Epoch 16/100
 - 16s - loss: 1.7713 - acc: 0.2133 - val\_loss: 1.7848 - val\_acc: 0.1475

Epoch 00016: val\_loss did not improve from 1.78021
Epoch 17/100
 - 17s - loss: 1.7714 - acc: 0.1767 - val\_loss: 1.7854 - val\_acc: 0.1475

Epoch 00017: val\_loss did not improve from 1.78021
Epoch 18/100
 - 16s - loss: 1.7711 - acc: 0.2133 - val\_loss: 1.7859 - val\_acc: 0.1475

Epoch 00018: val\_loss did not improve from 1.78021
Epoch 19/100
 - 16s - loss: 1.7713 - acc: 0.2067 - val\_loss: 1.7861 - val\_acc: 0.1475

Epoch 00019: val\_loss did not improve from 1.78021
Epoch 20/100
 - 16s - loss: 1.7710 - acc: 0.2133 - val\_loss: 1.7864 - val\_acc: 0.1475

Epoch 00020: val\_loss did not improve from 1.78021
Epoch 21/100
 - 16s - loss: 1.7711 - acc: 0.2133 - val\_loss: 1.7862 - val\_acc: 0.1475

Epoch 00021: val\_loss did not improve from 1.78021
Epoch 22/100
 - 16s - loss: 1.7706 - acc: 0.2133 - val\_loss: 1.7862 - val\_acc: 0.1475

Epoch 00022: val\_loss did not improve from 1.78021
Epoch 23/100
 - 16s - loss: 1.7708 - acc: 0.2133 - val\_loss: 1.7867 - val\_acc: 0.1475

Epoch 00023: val\_loss did not improve from 1.78021
Epoch 24/100
 - 16s - loss: 1.7715 - acc: 0.2133 - val\_loss: 1.7867 - val\_acc: 0.1475

Epoch 00024: val\_loss did not improve from 1.78021
Epoch 25/100
 - 17s - loss: 1.7712 - acc: 0.2133 - val\_loss: 1.7868 - val\_acc: 0.1475

Epoch 00025: val\_loss did not improve from 1.78021
Epoch 26/100
 - 16s - loss: 1.7706 - acc: 0.2133 - val\_loss: 1.7866 - val\_acc: 0.1475

Epoch 00026: val\_loss did not improve from 1.78021
Epoch 27/100
 - 16s - loss: 1.7708 - acc: 0.2133 - val\_loss: 1.7865 - val\_acc: 0.1475

Epoch 00027: val\_loss did not improve from 1.78021
Epoch 28/100
 - 16s - loss: 1.7709 - acc: 0.2133 - val\_loss: 1.7864 - val\_acc: 0.1475

Epoch 00028: val\_loss did not improve from 1.78021
Epoch 29/100
 - 16s - loss: 1.7712 - acc: 0.2133 - val\_loss: 1.7870 - val\_acc: 0.1475

Epoch 00029: val\_loss did not improve from 1.78021
Epoch 30/100
 - 16s - loss: 1.7709 - acc: 0.2133 - val\_loss: 1.7867 - val\_acc: 0.1475

Epoch 00030: val\_loss did not improve from 1.78021
Epoch 31/100
 - 16s - loss: 1.7709 - acc: 0.2133 - val\_loss: 1.7870 - val\_acc: 0.1475

Epoch 00031: val\_loss did not improve from 1.78021
Epoch 32/100
 - 16s - loss: 1.7711 - acc: 0.2133 - val\_loss: 1.7865 - val\_acc: 0.1475

Epoch 00032: val\_loss did not improve from 1.78021
Epoch 33/100
 - 16s - loss: 1.7710 - acc: 0.2133 - val\_loss: 1.7866 - val\_acc: 0.1475

Epoch 00033: val\_loss did not improve from 1.78021
Epoch 34/100
 - 16s - loss: 1.7712 - acc: 0.2133 - val\_loss: 1.7869 - val\_acc: 0.1475

Epoch 00034: val\_loss did not improve from 1.78021
Epoch 35/100
 - 16s - loss: 1.7712 - acc: 0.2133 - val\_loss: 1.7872 - val\_acc: 0.1475

Epoch 00035: val\_loss did not improve from 1.78021
Epoch 36/100
 - 16s - loss: 1.7709 - acc: 0.2133 - val\_loss: 1.7868 - val\_acc: 0.1475

Epoch 00036: val\_loss did not improve from 1.78021
Epoch 37/100
 - 16s - loss: 1.7714 - acc: 0.2133 - val\_loss: 1.7865 - val\_acc: 0.1475

Epoch 00037: val\_loss did not improve from 1.78021
Epoch 38/100
 - 16s - loss: 1.7708 - acc: 0.2133 - val\_loss: 1.7866 - val\_acc: 0.1475

Epoch 00038: val\_loss did not improve from 1.78021
Epoch 39/100
 - 16s - loss: 1.7709 - acc: 0.2133 - val\_loss: 1.7866 - val\_acc: 0.1475

Epoch 00039: val\_loss did not improve from 1.78021
Epoch 40/100
 - 16s - loss: 1.7710 - acc: 0.2133 - val\_loss: 1.7869 - val\_acc: 0.1475

Epoch 00040: val\_loss did not improve from 1.78021
Epoch 41/100
 - 16s - loss: 1.7709 - acc: 0.2133 - val\_loss: 1.7863 - val\_acc: 0.1475

Epoch 00041: val\_loss did not improve from 1.78021
Epoch 42/100
 - 16s - loss: 1.7708 - acc: 0.2133 - val\_loss: 1.7867 - val\_acc: 0.1475

Epoch 00042: val\_loss did not improve from 1.78021
Epoch 43/100
 - 16s - loss: 1.7708 - acc: 0.2133 - val\_loss: 1.7864 - val\_acc: 0.1475

Epoch 00043: val\_loss did not improve from 1.78021
Epoch 44/100
 - 16s - loss: 1.7712 - acc: 0.2133 - val\_loss: 1.7870 - val\_acc: 0.1475

Epoch 00044: val\_loss did not improve from 1.78021
Epoch 45/100
 - 16s - loss: 1.7709 - acc: 0.2133 - val\_loss: 1.7867 - val\_acc: 0.1475

Epoch 00045: val\_loss did not improve from 1.78021
Epoch 46/100
 - 16s - loss: 1.7711 - acc: 0.2133 - val\_loss: 1.7865 - val\_acc: 0.1475

Epoch 00046: val\_loss did not improve from 1.78021
Epoch 47/100
 - 17s - loss: 1.7707 - acc: 0.2133 - val\_loss: 1.7874 - val\_acc: 0.1475

Epoch 00047: val\_loss did not improve from 1.78021
Epoch 48/100
 - 16s - loss: 1.7711 - acc: 0.2133 - val\_loss: 1.7860 - val\_acc: 0.1475

Epoch 00048: val\_loss did not improve from 1.78021
Epoch 49/100
 - 16s - loss: 1.7713 - acc: 0.2133 - val\_loss: 1.7865 - val\_acc: 0.1475

Epoch 00049: val\_loss did not improve from 1.78021
Epoch 50/100
 - 16s - loss: 1.7711 - acc: 0.2133 - val\_loss: 1.7862 - val\_acc: 0.1475

Epoch 00050: val\_loss did not improve from 1.78021
Epoch 51/100
 - 16s - loss: 1.7708 - acc: 0.2133 - val\_loss: 1.7870 - val\_acc: 0.1475

Epoch 00051: val\_loss did not improve from 1.78021
Epoch 52/100
 - 16s - loss: 1.7709 - acc: 0.2133 - val\_loss: 1.7862 - val\_acc: 0.1475

Epoch 00052: val\_loss did not improve from 1.78021
Epoch 53/100
 - 16s - loss: 1.7712 - acc: 0.2133 - val\_loss: 1.7870 - val\_acc: 0.1475

Epoch 00053: val\_loss did not improve from 1.78021
Epoch 54/100
 - 16s - loss: 1.7710 - acc: 0.2133 - val\_loss: 1.7866 - val\_acc: 0.1475

Epoch 00054: val\_loss did not improve from 1.78021
Epoch 55/100
 - 17s - loss: 1.7713 - acc: 0.2133 - val\_loss: 1.7865 - val\_acc: 0.1475

Epoch 00055: val\_loss did not improve from 1.78021
Epoch 56/100
 - 16s - loss: 1.7713 - acc: 0.2133 - val\_loss: 1.7862 - val\_acc: 0.1475

Epoch 00056: val\_loss did not improve from 1.78021
Epoch 57/100
 - 16s - loss: 1.7708 - acc: 0.2133 - val\_loss: 1.7863 - val\_acc: 0.1475

Epoch 00057: val\_loss did not improve from 1.78021
Epoch 58/100
 - 17s - loss: 1.7707 - acc: 0.2133 - val\_loss: 1.7864 - val\_acc: 0.1475

Epoch 00058: val\_loss did not improve from 1.78021
Epoch 59/100
 - 16s - loss: 1.7709 - acc: 0.2133 - val\_loss: 1.7863 - val\_acc: 0.1475

Epoch 00059: val\_loss did not improve from 1.78021
Epoch 60/100
 - 16s - loss: 1.7710 - acc: 0.2133 - val\_loss: 1.7869 - val\_acc: 0.1475

Epoch 00060: val\_loss did not improve from 1.78021
Epoch 61/100
 - 16s - loss: 1.7709 - acc: 0.2133 - val\_loss: 1.7867 - val\_acc: 0.1475

Epoch 00061: val\_loss did not improve from 1.78021
Epoch 62/100
 - 17s - loss: 1.7706 - acc: 0.2133 - val\_loss: 1.7865 - val\_acc: 0.1475

Epoch 00062: val\_loss did not improve from 1.78021
Epoch 63/100
 - 16s - loss: 1.7710 - acc: 0.2133 - val\_loss: 1.7867 - val\_acc: 0.1475

Epoch 00063: val\_loss did not improve from 1.78021
Epoch 64/100
 - 16s - loss: 1.7705 - acc: 0.2133 - val\_loss: 1.7867 - val\_acc: 0.1475

Epoch 00064: val\_loss did not improve from 1.78021
Epoch 65/100
 - 16s - loss: 1.7707 - acc: 0.2133 - val\_loss: 1.7860 - val\_acc: 0.1475

Epoch 00065: val\_loss did not improve from 1.78021
Epoch 66/100
 - 16s - loss: 1.7708 - acc: 0.2133 - val\_loss: 1.7862 - val\_acc: 0.1475

Epoch 00066: val\_loss did not improve from 1.78021
Epoch 67/100
 - 16s - loss: 1.7706 - acc: 0.2133 - val\_loss: 1.7862 - val\_acc: 0.1475

Epoch 00067: val\_loss did not improve from 1.78021
Epoch 68/100
 - 16s - loss: 1.7707 - acc: 0.2133 - val\_loss: 1.7864 - val\_acc: 0.1475

Epoch 00068: val\_loss did not improve from 1.78021
Epoch 69/100
 - 18s - loss: 1.7711 - acc: 0.2133 - val\_loss: 1.7864 - val\_acc: 0.1475

Epoch 00069: val\_loss did not improve from 1.78021
Epoch 70/100
 - 16s - loss: 1.7709 - acc: 0.2133 - val\_loss: 1.7867 - val\_acc: 0.1475

Epoch 00070: val\_loss did not improve from 1.78021
Epoch 71/100
 - 17s - loss: 1.7707 - acc: 0.2133 - val\_loss: 1.7867 - val\_acc: 0.1475

Epoch 00071: val\_loss did not improve from 1.78021
Epoch 72/100
 - 16s - loss: 1.7710 - acc: 0.2133 - val\_loss: 1.7870 - val\_acc: 0.1475

Epoch 00072: val\_loss did not improve from 1.78021
Epoch 73/100
 - 17s - loss: 1.7707 - acc: 0.2133 - val\_loss: 1.7864 - val\_acc: 0.1475

Epoch 00073: val\_loss did not improve from 1.78021
Epoch 74/100
 - 16s - loss: 1.7709 - acc: 0.2133 - val\_loss: 1.7867 - val\_acc: 0.1475

Epoch 00074: val\_loss did not improve from 1.78021
Epoch 75/100
 - 16s - loss: 1.7712 - acc: 0.2133 - val\_loss: 1.7867 - val\_acc: 0.1475

Epoch 00075: val\_loss did not improve from 1.78021
Epoch 76/100
 - 17s - loss: 1.7709 - acc: 0.2133 - val\_loss: 1.7870 - val\_acc: 0.1475

Epoch 00076: val\_loss did not improve from 1.78021
Epoch 77/100
 - 16s - loss: 1.7710 - acc: 0.2133 - val\_loss: 1.7868 - val\_acc: 0.1475

Epoch 00077: val\_loss did not improve from 1.78021
Epoch 78/100
 - 16s - loss: 1.7710 - acc: 0.2133 - val\_loss: 1.7872 - val\_acc: 0.1475

Epoch 00078: val\_loss did not improve from 1.78021
Epoch 79/100
 - 16s - loss: 1.7712 - acc: 0.2133 - val\_loss: 1.7871 - val\_acc: 0.1475

Epoch 00079: val\_loss did not improve from 1.78021
Epoch 80/100
 - 16s - loss: 1.7711 - acc: 0.2133 - val\_loss: 1.7865 - val\_acc: 0.1475

Epoch 00080: val\_loss did not improve from 1.78021
Epoch 81/100
 - 16s - loss: 1.7709 - acc: 0.2133 - val\_loss: 1.7866 - val\_acc: 0.1475

Epoch 00081: val\_loss did not improve from 1.78021
Epoch 82/100
 - 17s - loss: 1.7710 - acc: 0.2133 - val\_loss: 1.7869 - val\_acc: 0.1475

Epoch 00082: val\_loss did not improve from 1.78021
Epoch 83/100
 - 17s - loss: 1.7710 - acc: 0.2133 - val\_loss: 1.7868 - val\_acc: 0.1475

Epoch 00083: val\_loss did not improve from 1.78021
Epoch 84/100
 - 17s - loss: 1.7707 - acc: 0.2133 - val\_loss: 1.7864 - val\_acc: 0.1475

Epoch 00084: val\_loss did not improve from 1.78021
Epoch 85/100
 - 16s - loss: 1.7714 - acc: 0.2133 - val\_loss: 1.7863 - val\_acc: 0.1475

Epoch 00085: val\_loss did not improve from 1.78021
Epoch 86/100
 - 16s - loss: 1.7709 - acc: 0.2133 - val\_loss: 1.7860 - val\_acc: 0.1475

Epoch 00086: val\_loss did not improve from 1.78021
Epoch 87/100
 - 16s - loss: 1.7706 - acc: 0.2133 - val\_loss: 1.7867 - val\_acc: 0.1475

Epoch 00087: val\_loss did not improve from 1.78021
Epoch 88/100
 - 16s - loss: 1.7711 - acc: 0.2133 - val\_loss: 1.7862 - val\_acc: 0.1475

Epoch 00088: val\_loss did not improve from 1.78021
Epoch 89/100
 - 16s - loss: 1.7712 - acc: 0.2133 - val\_loss: 1.7868 - val\_acc: 0.1475

Epoch 00089: val\_loss did not improve from 1.78021
Epoch 90/100
 - 16s - loss: 1.7714 - acc: 0.2133 - val\_loss: 1.7862 - val\_acc: 0.1475

Epoch 00090: val\_loss did not improve from 1.78021
Epoch 91/100
 - 16s - loss: 1.7709 - acc: 0.2133 - val\_loss: 1.7863 - val\_acc: 0.1475

Epoch 00091: val\_loss did not improve from 1.78021
Epoch 92/100
 - 17s - loss: 1.7709 - acc: 0.2133 - val\_loss: 1.7864 - val\_acc: 0.1475

Epoch 00092: val\_loss did not improve from 1.78021
Epoch 93/100
 - 16s - loss: 1.7710 - acc: 0.2133 - val\_loss: 1.7870 - val\_acc: 0.1475

Epoch 00093: val\_loss did not improve from 1.78021
Epoch 94/100
 - 16s - loss: 1.7710 - acc: 0.2133 - val\_loss: 1.7863 - val\_acc: 0.1475

Epoch 00094: val\_loss did not improve from 1.78021
Epoch 95/100
 - 16s - loss: 1.7709 - acc: 0.2133 - val\_loss: 1.7869 - val\_acc: 0.1475

Epoch 00095: val\_loss did not improve from 1.78021
Epoch 96/100
 - 16s - loss: 1.7710 - acc: 0.2133 - val\_loss: 1.7864 - val\_acc: 0.1475

Epoch 00096: val\_loss did not improve from 1.78021
Epoch 97/100
 - 16s - loss: 1.7707 - acc: 0.2133 - val\_loss: 1.7869 - val\_acc: 0.1475

Epoch 00097: val\_loss did not improve from 1.78021
Epoch 98/100
 - 17s - loss: 1.7711 - acc: 0.2133 - val\_loss: 1.7870 - val\_acc: 0.1475

Epoch 00098: val\_loss did not improve from 1.78021
Epoch 99/100
 - 16s - loss: 1.7710 - acc: 0.2133 - val\_loss: 1.7863 - val\_acc: 0.1475

Epoch 00099: val\_loss did not improve from 1.78021
Epoch 100/100
 - 16s - loss: 1.7708 - acc: 0.2133 - val\_loss: 1.7866 - val\_acc: 0.1475

Epoch 00100: val\_loss did not improve from 1.78021

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}62}]:} \PY{c+c1}{\PYZsh{} Loading the model that performed the best on the validation set}
         \PY{n}{model10}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model\PYZus{}10.weights.best.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Testing the model on the Test data}
         \PY{p}{(}\PY{n}{loss10}\PY{p}{,} \PY{n}{accuracy10}\PY{p}{)} \PY{o}{=} \PY{n}{model3}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
         
         \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy on test data: \PYZob{}:.2f\PYZcb{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracy10} \PY{o}{*} \PY{l+m+mi}{100}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy on test data: 71.00\%

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}63}]:} \PY{c+c1}{\PYZsh{} Making the plot larger}
         \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{14}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{loss10} \PY{o}{=} \PY{n}{history10}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}                          \PY{c+c1}{\PYZsh{} Loss on the training data}
         \PY{n}{val\PYZus{}loss10} \PY{o}{=} \PY{n}{history10}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}                  \PY{c+c1}{\PYZsh{} Loss on the validation data}
         \PY{n}{epochs} \PY{o}{=} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{101}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{loss10}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ro\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{val\PYZus{}loss10}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{go\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}63}]:} <matplotlib.legend.Legend at 0x7f262d4ede10>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_85_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}64}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn.metrics} \PY{k+kn}{import} \PY{n}{confusion\PYZus{}matrix}
         \PY{k+kn}{import} \PY{n+nn}{itertools}
         \PY{n}{y\PYZus{}predictions10} \PY{o}{=} \PY{n}{model10}\PY{o}{.}\PY{n}{predict\PYZus{}classes}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{p}{;}
         \PY{n}{y\PYZus{}pred10}\PY{o}{=}\PY{n}{to\PYZus{}categorical}\PY{p}{(}\PY{n}{y\PYZus{}predictions10}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{;}
         \PY{n}{y\PYZus{}test\PYZus{}cm} \PY{o}{=} \PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{;}
         
         \PY{c+c1}{\PYZsh{}print y\PYZus{}predictions3[:50]}
         \PY{c+c1}{\PYZsh{}print y\PYZus{}test[:50]}
         \PY{n}{model\PYZus{}cnf\PYZus{}matrix10} \PY{o}{=} \PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{y\PYZus{}test\PYZus{}cm}\PY{p}{,}\PY{n}{y\PYZus{}pred10}\PY{p}{)}\PY{p}{;}
         \PY{c+c1}{\PYZsh{}print(model\PYZus{}cnf\PYZus{}matrix3)}
         \PY{n}{confusion\PYZus{}matrix\PYZus{}plot} \PY{o}{=} \PY{n}{plot\PYZus{}confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{model\PYZus{}cnf\PYZus{}matrix10}\PY{p}{,} 
                                                       \PY{n}{classes}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{boxing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{handclapping}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{handwaving}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{jogging}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{running}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{walking}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} 
                                                       \PY{n}{normalize}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Normalized confusion matrix

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_86_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{model-bg-subtraction-more-dropouts}{%
\subsection{Model BG Subtraction more
dropouts}\label{model-bg-subtraction-more-dropouts}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}79}]:} \PY{k+kn}{from} \PY{n+nn}{keras.models} \PY{k+kn}{import} \PY{n}{Sequential}
         \PY{k+kn}{from} \PY{n+nn}{keras.layers} \PY{k+kn}{import} \PY{n}{Conv3D}\PY{p}{,} \PY{n}{MaxPooling3D}\PY{p}{,} \PY{n}{GlobalAveragePooling3D}\PY{p}{,} \PY{n}{BatchNormalization}
         \PY{k+kn}{from} \PY{n+nn}{keras.layers.core} \PY{k+kn}{import} \PY{n}{Dense}\PY{p}{,} \PY{n}{Dropout}
         
         \PY{c+c1}{\PYZsh{} Using the Sequential Model}
         \PY{n}{model10\PYZus{}bg} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Adding Alternate convolutional and pooling layers}
         \PY{n}{model10\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv3D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                          \PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{n}{model10\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling3D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{model10\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{)}
         \PY{n}{model10\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv3D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{model10\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling3D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{model10\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{model10\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv3D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{c+c1}{\PYZsh{}model10\PYZus{}bg.add(Dropout(0.5))}
         \PY{n}{model10\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling3D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{model10\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{)}
         \PY{c+c1}{\PYZsh{}model.add(Conv3D(filters=1024, kernel\PYZus{}size=(2, 3, 3), strides=(1, 1, 1), padding=\PYZsq{}same\PYZsq{}, activation=\PYZsq{}relu\PYZsq{}))}
         \PY{c+c1}{\PYZsh{}model.add(MaxPooling3D(pool\PYZus{}size=2, strides=(2, 2, 2), padding=\PYZsq{}same\PYZsq{}))}
         
         \PY{c+c1}{\PYZsh{} A global average pooling layer to get a 1\PYZhy{}d vector}
         \PY{c+c1}{\PYZsh{} The vector will have a depth (same as number of elements in the vector) of 1024}
         \PY{n}{model10\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{GlobalAveragePooling3D}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Hidden layer}
         \PY{n}{model10\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{)}
         \PY{n}{model10\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Dropout Layer}
         \PY{n}{model10\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Output layer}
         \PY{n}{model10\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{model10\PYZus{}bg}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
conv3d\_29 (Conv3D)           (None, 35, 20, 20, 16)    2016      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling3d\_29 (MaxPooling (None, 18, 10, 10, 16)    0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_18 (Dropout)         (None, 18, 10, 10, 16)    0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv3d\_30 (Conv3D)           (None, 18, 10, 10, 64)    27712     
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling3d\_30 (MaxPooling (None, 9, 5, 5, 64)       0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_19 (Dropout)         (None, 9, 5, 5, 64)       0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv3d\_31 (Conv3D)           (None, 9, 5, 5, 256)      442624    
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling3d\_31 (MaxPooling (None, 5, 3, 3, 256)      0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_20 (Dropout)         (None, 5, 3, 3, 256)      0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
global\_average\_pooling3d\_11  (None, 256)               0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_21 (Dropout)         (None, 256)               0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_23 (Dense)             (None, 32)                8224      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_22 (Dropout)         (None, 32)                0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_24 (Dense)             (None, 6)                 198       
=================================================================
Total params: 480,774
Trainable params: 480,774
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}80}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{} Imports}
         \PY{k+kn}{from} \PY{n+nn}{keras.callbacks} \PY{k+kn}{import} \PY{n}{ModelCheckpoint}
         
         \PY{c+c1}{\PYZsh{} Compiling the model}
         \PY{n}{model10\PYZus{}bg}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{nadam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Saving the model that performed the best on the validation set}
         \PY{n}{checkpoint10\PYZus{}bg} \PY{o}{=} \PY{n}{ModelCheckpoint}\PY{p}{(}\PY{n}{filepath}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model\PYZus{}10\PYZus{}bg.weights.best.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{save\PYZus{}best\PYZus{}only}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
         
         
         \PY{n}{history10\PYZus{}bg} \PY{o}{=} \PY{n}{model10\PYZus{}bg}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}bgsub}\PY{p}{,} \PY{n}{y\PYZus{}train\PYZus{}bgsub}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{150}\PY{p}{,} 
                             \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{X\PYZus{}valid\PYZus{}bgsub}\PY{p}{,} \PY{n}{y\PYZus{}valid\PYZus{}bgsub}\PY{p}{)}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{checkpoint10\PYZus{}bg}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Train on 300 samples, validate on 122 samples
Epoch 1/150
300/300 [==============================] - 15s 50ms/step - loss: 1.7286 - acc: 0.1767 - val\_loss: 1.7059 - val\_acc: 0.2705

Epoch 00001: val\_loss improved from inf to 1.70588, saving model to Model\_10\_bg.weights.best.hdf5
Epoch 2/150
300/300 [==============================] - 13s 44ms/step - loss: 1.5726 - acc: 0.2433 - val\_loss: 1.4835 - val\_acc: 0.2295

Epoch 00002: val\_loss improved from 1.70588 to 1.48353, saving model to Model\_10\_bg.weights.best.hdf5
Epoch 3/150
300/300 [==============================] - 14s 47ms/step - loss: 1.4657 - acc: 0.3133 - val\_loss: 1.3985 - val\_acc: 0.2049

Epoch 00003: val\_loss improved from 1.48353 to 1.39851, saving model to Model\_10\_bg.weights.best.hdf5
Epoch 4/150
300/300 [==============================] - 13s 45ms/step - loss: 1.2863 - acc: 0.5067 - val\_loss: 1.3512 - val\_acc: 0.4262

Epoch 00004: val\_loss improved from 1.39851 to 1.35120, saving model to Model\_10\_bg.weights.best.hdf5
Epoch 5/150
300/300 [==============================] - 13s 44ms/step - loss: 1.3414 - acc: 0.4300 - val\_loss: 1.1354 - val\_acc: 0.4508

Epoch 00005: val\_loss improved from 1.35120 to 1.13537, saving model to Model\_10\_bg.weights.best.hdf5
Epoch 6/150
300/300 [==============================] - 13s 44ms/step - loss: 1.1177 - acc: 0.5133 - val\_loss: 1.0481 - val\_acc: 0.5246

Epoch 00006: val\_loss improved from 1.13537 to 1.04810, saving model to Model\_10\_bg.weights.best.hdf5
Epoch 7/150
300/300 [==============================] - 13s 43ms/step - loss: 1.0637 - acc: 0.5300 - val\_loss: 0.9827 - val\_acc: 0.5656

Epoch 00007: val\_loss improved from 1.04810 to 0.98274, saving model to Model\_10\_bg.weights.best.hdf5
Epoch 8/150
300/300 [==============================] - 13s 43ms/step - loss: 1.0477 - acc: 0.5433 - val\_loss: 0.9538 - val\_acc: 0.5410

Epoch 00008: val\_loss improved from 0.98274 to 0.95376, saving model to Model\_10\_bg.weights.best.hdf5
Epoch 9/150
300/300 [==============================] - 13s 43ms/step - loss: 0.9287 - acc: 0.5733 - val\_loss: 0.8227 - val\_acc: 0.5984

Epoch 00009: val\_loss improved from 0.95376 to 0.82265, saving model to Model\_10\_bg.weights.best.hdf5
Epoch 10/150
300/300 [==============================] - 13s 43ms/step - loss: 0.9168 - acc: 0.5533 - val\_loss: 0.8422 - val\_acc: 0.6066

Epoch 00010: val\_loss did not improve from 0.82265
Epoch 11/150
300/300 [==============================] - 13s 43ms/step - loss: 0.9447 - acc: 0.5400 - val\_loss: 0.8635 - val\_acc: 0.4836

Epoch 00011: val\_loss did not improve from 0.82265
Epoch 12/150
300/300 [==============================] - 13s 44ms/step - loss: 0.9067 - acc: 0.5333 - val\_loss: 0.8977 - val\_acc: 0.5820

Epoch 00012: val\_loss did not improve from 0.82265
Epoch 13/150
300/300 [==============================] - 13s 45ms/step - loss: 0.8594 - acc: 0.5367 - val\_loss: 0.8138 - val\_acc: 0.6475

Epoch 00013: val\_loss improved from 0.82265 to 0.81381, saving model to Model\_10\_bg.weights.best.hdf5
Epoch 14/150
300/300 [==============================] - 13s 44ms/step - loss: 0.8175 - acc: 0.6067 - val\_loss: 0.7589 - val\_acc: 0.6230

Epoch 00014: val\_loss improved from 0.81381 to 0.75893, saving model to Model\_10\_bg.weights.best.hdf5
Epoch 15/150
300/300 [==============================] - 13s 44ms/step - loss: 0.7903 - acc: 0.6233 - val\_loss: 0.7975 - val\_acc: 0.5574

Epoch 00015: val\_loss did not improve from 0.75893
Epoch 16/150
300/300 [==============================] - 13s 45ms/step - loss: 0.8411 - acc: 0.5800 - val\_loss: 0.7343 - val\_acc: 0.6148

Epoch 00016: val\_loss improved from 0.75893 to 0.73428, saving model to Model\_10\_bg.weights.best.hdf5
Epoch 17/150
300/300 [==============================] - 13s 44ms/step - loss: 0.8422 - acc: 0.5833 - val\_loss: 0.7951 - val\_acc: 0.5738

Epoch 00017: val\_loss did not improve from 0.73428
Epoch 18/150
300/300 [==============================] - 13s 44ms/step - loss: 0.7452 - acc: 0.6400 - val\_loss: 0.7726 - val\_acc: 0.6148

Epoch 00018: val\_loss did not improve from 0.73428
Epoch 19/150
300/300 [==============================] - 13s 44ms/step - loss: 0.7808 - acc: 0.6200 - val\_loss: 0.7930 - val\_acc: 0.5820

Epoch 00019: val\_loss did not improve from 0.73428
Epoch 20/150
300/300 [==============================] - 13s 44ms/step - loss: 0.8089 - acc: 0.5867 - val\_loss: 0.7211 - val\_acc: 0.6148

Epoch 00020: val\_loss improved from 0.73428 to 0.72109, saving model to Model\_10\_bg.weights.best.hdf5
Epoch 21/150
300/300 [==============================] - 13s 44ms/step - loss: 0.7464 - acc: 0.6200 - val\_loss: 0.7071 - val\_acc: 0.6148

Epoch 00021: val\_loss improved from 0.72109 to 0.70713, saving model to Model\_10\_bg.weights.best.hdf5
Epoch 22/150
300/300 [==============================] - 13s 43ms/step - loss: 0.6943 - acc: 0.6433 - val\_loss: 0.6802 - val\_acc: 0.7131

Epoch 00022: val\_loss improved from 0.70713 to 0.68020, saving model to Model\_10\_bg.weights.best.hdf5
Epoch 23/150
300/300 [==============================] - 13s 44ms/step - loss: 0.7273 - acc: 0.6667 - val\_loss: 0.6512 - val\_acc: 0.6967

Epoch 00023: val\_loss improved from 0.68020 to 0.65121, saving model to Model\_10\_bg.weights.best.hdf5
Epoch 24/150
300/300 [==============================] - 13s 44ms/step - loss: 0.7020 - acc: 0.7100 - val\_loss: 0.6605 - val\_acc: 0.7459

Epoch 00024: val\_loss did not improve from 0.65121
Epoch 25/150
300/300 [==============================] - 13s 45ms/step - loss: 0.5878 - acc: 0.6900 - val\_loss: 0.5197 - val\_acc: 0.7623

Epoch 00025: val\_loss improved from 0.65121 to 0.51974, saving model to Model\_10\_bg.weights.best.hdf5
Epoch 26/150
300/300 [==============================] - 13s 44ms/step - loss: 0.6097 - acc: 0.7100 - val\_loss: 0.5609 - val\_acc: 0.7623

Epoch 00026: val\_loss did not improve from 0.51974
Epoch 27/150
300/300 [==============================] - 13s 44ms/step - loss: 0.4865 - acc: 0.7867 - val\_loss: 0.5346 - val\_acc: 0.7459

Epoch 00027: val\_loss did not improve from 0.51974
Epoch 28/150
300/300 [==============================] - 13s 44ms/step - loss: 0.5563 - acc: 0.7433 - val\_loss: 0.7327 - val\_acc: 0.6721

Epoch 00028: val\_loss did not improve from 0.51974
Epoch 29/150
300/300 [==============================] - 13s 44ms/step - loss: 0.5633 - acc: 0.7333 - val\_loss: 0.4997 - val\_acc: 0.7459

Epoch 00029: val\_loss improved from 0.51974 to 0.49970, saving model to Model\_10\_bg.weights.best.hdf5
Epoch 30/150
300/300 [==============================] - 13s 44ms/step - loss: 0.5874 - acc: 0.7367 - val\_loss: 0.6243 - val\_acc: 0.6885

Epoch 00030: val\_loss did not improve from 0.49970
Epoch 31/150
300/300 [==============================] - 14s 45ms/step - loss: 0.4516 - acc: 0.7767 - val\_loss: 0.5446 - val\_acc: 0.7295

Epoch 00031: val\_loss did not improve from 0.49970
Epoch 32/150
300/300 [==============================] - 13s 44ms/step - loss: 0.5006 - acc: 0.7667 - val\_loss: 0.5190 - val\_acc: 0.7459

Epoch 00032: val\_loss did not improve from 0.49970
Epoch 33/150
300/300 [==============================] - 13s 44ms/step - loss: 0.4532 - acc: 0.7700 - val\_loss: 0.5511 - val\_acc: 0.7295

Epoch 00033: val\_loss did not improve from 0.49970
Epoch 34/150
300/300 [==============================] - 13s 44ms/step - loss: 0.4244 - acc: 0.7933 - val\_loss: 0.4819 - val\_acc: 0.7951

Epoch 00034: val\_loss improved from 0.49970 to 0.48195, saving model to Model\_10\_bg.weights.best.hdf5
Epoch 35/150
300/300 [==============================] - 13s 44ms/step - loss: 0.4071 - acc: 0.8233 - val\_loss: 0.5780 - val\_acc: 0.7705

Epoch 00035: val\_loss did not improve from 0.48195
Epoch 36/150
300/300 [==============================] - 13s 45ms/step - loss: 0.3970 - acc: 0.8200 - val\_loss: 0.4308 - val\_acc: 0.8361

Epoch 00036: val\_loss improved from 0.48195 to 0.43077, saving model to Model\_10\_bg.weights.best.hdf5
Epoch 37/150
300/300 [==============================] - 13s 44ms/step - loss: 0.5311 - acc: 0.7667 - val\_loss: 0.4843 - val\_acc: 0.7623

Epoch 00037: val\_loss did not improve from 0.43077
Epoch 38/150
300/300 [==============================] - 13s 45ms/step - loss: 0.4369 - acc: 0.7867 - val\_loss: 0.5146 - val\_acc: 0.7623

Epoch 00038: val\_loss did not improve from 0.43077
Epoch 39/150
300/300 [==============================] - 13s 44ms/step - loss: 0.3703 - acc: 0.8267 - val\_loss: 0.4581 - val\_acc: 0.7951

Epoch 00039: val\_loss did not improve from 0.43077
Epoch 40/150
300/300 [==============================] - 13s 45ms/step - loss: 0.4018 - acc: 0.8000 - val\_loss: 0.5479 - val\_acc: 0.8033

Epoch 00040: val\_loss did not improve from 0.43077
Epoch 41/150
300/300 [==============================] - 13s 45ms/step - loss: 0.4011 - acc: 0.8000 - val\_loss: 0.4834 - val\_acc: 0.7951

Epoch 00041: val\_loss did not improve from 0.43077
Epoch 42/150
300/300 [==============================] - 13s 44ms/step - loss: 0.3321 - acc: 0.8400 - val\_loss: 0.4852 - val\_acc: 0.7951

Epoch 00042: val\_loss did not improve from 0.43077
Epoch 43/150
300/300 [==============================] - 13s 44ms/step - loss: 0.4046 - acc: 0.8267 - val\_loss: 0.5562 - val\_acc: 0.7541

Epoch 00043: val\_loss did not improve from 0.43077
Epoch 44/150
300/300 [==============================] - 13s 44ms/step - loss: 0.4149 - acc: 0.8267 - val\_loss: 0.5690 - val\_acc: 0.7541

Epoch 00044: val\_loss did not improve from 0.43077
Epoch 45/150
300/300 [==============================] - 13s 44ms/step - loss: 0.3581 - acc: 0.8500 - val\_loss: 0.5107 - val\_acc: 0.7951

Epoch 00045: val\_loss did not improve from 0.43077
Epoch 46/150
300/300 [==============================] - 13s 44ms/step - loss: 0.3064 - acc: 0.8900 - val\_loss: 0.5543 - val\_acc: 0.7705

Epoch 00046: val\_loss did not improve from 0.43077
Epoch 47/150
300/300 [==============================] - 13s 45ms/step - loss: 0.3295 - acc: 0.8333 - val\_loss: 0.5575 - val\_acc: 0.7869

Epoch 00047: val\_loss did not improve from 0.43077
Epoch 48/150
300/300 [==============================] - 13s 45ms/step - loss: 0.3547 - acc: 0.8300 - val\_loss: 0.6437 - val\_acc: 0.7459

Epoch 00048: val\_loss did not improve from 0.43077
Epoch 49/150
300/300 [==============================] - 14s 45ms/step - loss: 0.3886 - acc: 0.8333 - val\_loss: 0.4970 - val\_acc: 0.8197

Epoch 00049: val\_loss did not improve from 0.43077
Epoch 50/150
300/300 [==============================] - 13s 44ms/step - loss: 0.3092 - acc: 0.8300 - val\_loss: 0.4990 - val\_acc: 0.8033

Epoch 00050: val\_loss did not improve from 0.43077
Epoch 51/150
300/300 [==============================] - 13s 44ms/step - loss: 0.3066 - acc: 0.8533 - val\_loss: 0.4644 - val\_acc: 0.8443

Epoch 00051: val\_loss did not improve from 0.43077
Epoch 52/150
300/300 [==============================] - 13s 44ms/step - loss: 0.3132 - acc: 0.8767 - val\_loss: 0.5111 - val\_acc: 0.8115

Epoch 00052: val\_loss did not improve from 0.43077
Epoch 53/150
300/300 [==============================] - 13s 44ms/step - loss: 0.3078 - acc: 0.8667 - val\_loss: 0.5556 - val\_acc: 0.7787

Epoch 00053: val\_loss did not improve from 0.43077
Epoch 54/150
300/300 [==============================] - 13s 45ms/step - loss: 0.2898 - acc: 0.8733 - val\_loss: 0.5151 - val\_acc: 0.8115

Epoch 00054: val\_loss did not improve from 0.43077
Epoch 55/150
300/300 [==============================] - 13s 45ms/step - loss: 0.5241 - acc: 0.8133 - val\_loss: 0.5694 - val\_acc: 0.7541

Epoch 00055: val\_loss did not improve from 0.43077
Epoch 56/150
300/300 [==============================] - 13s 45ms/step - loss: 0.3332 - acc: 0.8633 - val\_loss: 0.5947 - val\_acc: 0.7869

Epoch 00056: val\_loss did not improve from 0.43077
Epoch 57/150
300/300 [==============================] - 13s 44ms/step - loss: 0.3094 - acc: 0.8833 - val\_loss: 0.6487 - val\_acc: 0.8115

Epoch 00057: val\_loss did not improve from 0.43077
Epoch 58/150
300/300 [==============================] - 13s 44ms/step - loss: 0.4351 - acc: 0.8100 - val\_loss: 0.5775 - val\_acc: 0.7623

Epoch 00058: val\_loss did not improve from 0.43077
Epoch 59/150
300/300 [==============================] - 13s 45ms/step - loss: 0.3746 - acc: 0.8300 - val\_loss: 0.5229 - val\_acc: 0.8279

Epoch 00059: val\_loss did not improve from 0.43077
Epoch 60/150
300/300 [==============================] - 13s 44ms/step - loss: 0.2414 - acc: 0.9000 - val\_loss: 0.5448 - val\_acc: 0.8033

Epoch 00060: val\_loss did not improve from 0.43077
Epoch 61/150
300/300 [==============================] - 13s 44ms/step - loss: 0.3689 - acc: 0.8600 - val\_loss: 0.6194 - val\_acc: 0.7541

Epoch 00061: val\_loss did not improve from 0.43077
Epoch 62/150
300/300 [==============================] - 13s 44ms/step - loss: 0.2733 - acc: 0.8833 - val\_loss: 0.4512 - val\_acc: 0.8607

Epoch 00062: val\_loss did not improve from 0.43077
Epoch 63/150
300/300 [==============================] - 13s 45ms/step - loss: 0.2862 - acc: 0.8767 - val\_loss: 0.5168 - val\_acc: 0.8361

Epoch 00063: val\_loss did not improve from 0.43077
Epoch 64/150
300/300 [==============================] - 13s 45ms/step - loss: 0.2318 - acc: 0.9100 - val\_loss: 0.4249 - val\_acc: 0.8361

Epoch 00064: val\_loss improved from 0.43077 to 0.42494, saving model to Model\_10\_bg.weights.best.hdf5
Epoch 65/150
300/300 [==============================] - 13s 44ms/step - loss: 0.2294 - acc: 0.9167 - val\_loss: 0.4918 - val\_acc: 0.8197

Epoch 00065: val\_loss did not improve from 0.42494
Epoch 66/150
300/300 [==============================] - 13s 44ms/step - loss: 0.2585 - acc: 0.8867 - val\_loss: 0.5037 - val\_acc: 0.8033

Epoch 00066: val\_loss did not improve from 0.42494
Epoch 67/150
300/300 [==============================] - 13s 44ms/step - loss: 0.2446 - acc: 0.8933 - val\_loss: 0.4526 - val\_acc: 0.8689

Epoch 00067: val\_loss did not improve from 0.42494
Epoch 68/150
300/300 [==============================] - 13s 44ms/step - loss: 0.3249 - acc: 0.8800 - val\_loss: 0.4552 - val\_acc: 0.8607

Epoch 00068: val\_loss did not improve from 0.42494
Epoch 69/150
300/300 [==============================] - 13s 44ms/step - loss: 0.2653 - acc: 0.8933 - val\_loss: 0.5637 - val\_acc: 0.7869

Epoch 00069: val\_loss did not improve from 0.42494
Epoch 70/150
300/300 [==============================] - 13s 45ms/step - loss: 0.2434 - acc: 0.9033 - val\_loss: 0.4974 - val\_acc: 0.8033

Epoch 00070: val\_loss did not improve from 0.42494
Epoch 71/150
300/300 [==============================] - 14s 45ms/step - loss: 0.2155 - acc: 0.9233 - val\_loss: 0.5320 - val\_acc: 0.7951

Epoch 00071: val\_loss did not improve from 0.42494
Epoch 72/150
300/300 [==============================] - 13s 45ms/step - loss: 0.2082 - acc: 0.9267 - val\_loss: 0.5040 - val\_acc: 0.8525

Epoch 00072: val\_loss did not improve from 0.42494
Epoch 73/150
300/300 [==============================] - 13s 45ms/step - loss: 0.2385 - acc: 0.9067 - val\_loss: 0.4669 - val\_acc: 0.8361

Epoch 00073: val\_loss did not improve from 0.42494
Epoch 74/150
300/300 [==============================] - 13s 44ms/step - loss: 0.2371 - acc: 0.9133 - val\_loss: 0.4511 - val\_acc: 0.8607

Epoch 00074: val\_loss did not improve from 0.42494
Epoch 75/150
300/300 [==============================] - 13s 44ms/step - loss: 0.2268 - acc: 0.9200 - val\_loss: 0.4697 - val\_acc: 0.8197

Epoch 00075: val\_loss did not improve from 0.42494
Epoch 76/150
300/300 [==============================] - 13s 44ms/step - loss: 0.2750 - acc: 0.9200 - val\_loss: 0.5017 - val\_acc: 0.8525

Epoch 00076: val\_loss did not improve from 0.42494
Epoch 77/150
300/300 [==============================] - 13s 44ms/step - loss: 0.2316 - acc: 0.9067 - val\_loss: 0.4687 - val\_acc: 0.8689

Epoch 00077: val\_loss did not improve from 0.42494
Epoch 78/150
300/300 [==============================] - 13s 44ms/step - loss: 0.1996 - acc: 0.9267 - val\_loss: 0.5078 - val\_acc: 0.8525

Epoch 00078: val\_loss did not improve from 0.42494
Epoch 79/150
300/300 [==============================] - 13s 45ms/step - loss: 0.2141 - acc: 0.9300 - val\_loss: 0.5358 - val\_acc: 0.7951

Epoch 00079: val\_loss did not improve from 0.42494
Epoch 80/150
300/300 [==============================] - 13s 45ms/step - loss: 0.2682 - acc: 0.9033 - val\_loss: 0.5761 - val\_acc: 0.7869

Epoch 00080: val\_loss did not improve from 0.42494
Epoch 81/150
300/300 [==============================] - 13s 45ms/step - loss: 0.2939 - acc: 0.8867 - val\_loss: 0.5878 - val\_acc: 0.7623

Epoch 00081: val\_loss did not improve from 0.42494
Epoch 82/150
300/300 [==============================] - 13s 44ms/step - loss: 0.2411 - acc: 0.9067 - val\_loss: 0.4536 - val\_acc: 0.8361

Epoch 00082: val\_loss did not improve from 0.42494
Epoch 83/150
300/300 [==============================] - 13s 43ms/step - loss: 0.1788 - acc: 0.9333 - val\_loss: 0.4373 - val\_acc: 0.8852

Epoch 00083: val\_loss did not improve from 0.42494
Epoch 84/150
300/300 [==============================] - 13s 44ms/step - loss: 0.1230 - acc: 0.9467 - val\_loss: 0.5089 - val\_acc: 0.8197

Epoch 00084: val\_loss did not improve from 0.42494
Epoch 85/150
300/300 [==============================] - 13s 45ms/step - loss: 0.2097 - acc: 0.9233 - val\_loss: 0.5180 - val\_acc: 0.8525

Epoch 00085: val\_loss did not improve from 0.42494
Epoch 86/150
300/300 [==============================] - 14s 46ms/step - loss: 0.2685 - acc: 0.8967 - val\_loss: 0.4815 - val\_acc: 0.8361

Epoch 00086: val\_loss did not improve from 0.42494
Epoch 87/150
300/300 [==============================] - 13s 44ms/step - loss: 0.3092 - acc: 0.8767 - val\_loss: 0.4830 - val\_acc: 0.8115

Epoch 00087: val\_loss did not improve from 0.42494
Epoch 88/150
300/300 [==============================] - 13s 44ms/step - loss: 0.2264 - acc: 0.9267 - val\_loss: 0.3978 - val\_acc: 0.8607

Epoch 00088: val\_loss improved from 0.42494 to 0.39781, saving model to Model\_10\_bg.weights.best.hdf5
Epoch 89/150
300/300 [==============================] - 13s 44ms/step - loss: 0.1762 - acc: 0.9233 - val\_loss: 0.5053 - val\_acc: 0.8607

Epoch 00089: val\_loss did not improve from 0.39781
Epoch 90/150
300/300 [==============================] - 13s 44ms/step - loss: 0.2088 - acc: 0.9200 - val\_loss: 0.5912 - val\_acc: 0.7787

Epoch 00090: val\_loss did not improve from 0.39781
Epoch 91/150
300/300 [==============================] - 13s 44ms/step - loss: 0.1411 - acc: 0.9500 - val\_loss: 0.4188 - val\_acc: 0.8689

Epoch 00091: val\_loss did not improve from 0.39781
Epoch 92/150
300/300 [==============================] - 13s 44ms/step - loss: 0.1421 - acc: 0.9567 - val\_loss: 0.4346 - val\_acc: 0.8607

Epoch 00092: val\_loss did not improve from 0.39781
Epoch 93/150
300/300 [==============================] - 13s 44ms/step - loss: 0.1501 - acc: 0.9467 - val\_loss: 0.5250 - val\_acc: 0.8525

Epoch 00093: val\_loss did not improve from 0.39781
Epoch 94/150
300/300 [==============================] - 13s 44ms/step - loss: 0.1459 - acc: 0.9500 - val\_loss: 0.5108 - val\_acc: 0.8525

Epoch 00094: val\_loss did not improve from 0.39781
Epoch 95/150
300/300 [==============================] - 13s 44ms/step - loss: 0.2655 - acc: 0.9200 - val\_loss: 0.3607 - val\_acc: 0.8689

Epoch 00095: val\_loss improved from 0.39781 to 0.36068, saving model to Model\_10\_bg.weights.best.hdf5
Epoch 96/150
300/300 [==============================] - 13s 44ms/step - loss: 0.1583 - acc: 0.9200 - val\_loss: 0.4021 - val\_acc: 0.8443

Epoch 00096: val\_loss did not improve from 0.36068
Epoch 97/150
300/300 [==============================] - 13s 45ms/step - loss: 0.1273 - acc: 0.9567 - val\_loss: 0.4304 - val\_acc: 0.8689

Epoch 00097: val\_loss did not improve from 0.36068
Epoch 98/150
300/300 [==============================] - 13s 45ms/step - loss: 0.1568 - acc: 0.9600 - val\_loss: 0.4357 - val\_acc: 0.8770

Epoch 00098: val\_loss did not improve from 0.36068
Epoch 99/150
300/300 [==============================] - 13s 44ms/step - loss: 0.1306 - acc: 0.9600 - val\_loss: 0.5139 - val\_acc: 0.8279

Epoch 00099: val\_loss did not improve from 0.36068
Epoch 100/150
300/300 [==============================] - 13s 45ms/step - loss: 0.1991 - acc: 0.9233 - val\_loss: 0.4990 - val\_acc: 0.8852

Epoch 00100: val\_loss did not improve from 0.36068
Epoch 101/150
300/300 [==============================] - 13s 45ms/step - loss: 0.1692 - acc: 0.9367 - val\_loss: 0.4087 - val\_acc: 0.8852

Epoch 00101: val\_loss did not improve from 0.36068
Epoch 102/150
300/300 [==============================] - 13s 44ms/step - loss: 0.1143 - acc: 0.9567 - val\_loss: 0.4449 - val\_acc: 0.8443

Epoch 00102: val\_loss did not improve from 0.36068
Epoch 103/150
300/300 [==============================] - 13s 44ms/step - loss: 0.1559 - acc: 0.9567 - val\_loss: 0.4226 - val\_acc: 0.8689

Epoch 00103: val\_loss did not improve from 0.36068
Epoch 104/150
300/300 [==============================] - 14s 45ms/step - loss: 0.1317 - acc: 0.9567 - val\_loss: 0.3787 - val\_acc: 0.8934

Epoch 00104: val\_loss did not improve from 0.36068
Epoch 105/150
300/300 [==============================] - 13s 44ms/step - loss: 0.1249 - acc: 0.9467 - val\_loss: 0.4952 - val\_acc: 0.8197

Epoch 00105: val\_loss did not improve from 0.36068
Epoch 106/150
300/300 [==============================] - 13s 44ms/step - loss: 0.1023 - acc: 0.9600 - val\_loss: 0.4967 - val\_acc: 0.8525

Epoch 00106: val\_loss did not improve from 0.36068
Epoch 107/150
300/300 [==============================] - 13s 45ms/step - loss: 0.1850 - acc: 0.9367 - val\_loss: 0.3931 - val\_acc: 0.8770

Epoch 00107: val\_loss did not improve from 0.36068
Epoch 108/150
300/300 [==============================] - 13s 44ms/step - loss: 0.1903 - acc: 0.9533 - val\_loss: 0.4366 - val\_acc: 0.8607

Epoch 00108: val\_loss did not improve from 0.36068
Epoch 109/150
300/300 [==============================] - 13s 45ms/step - loss: 0.1328 - acc: 0.9467 - val\_loss: 0.3787 - val\_acc: 0.9016

Epoch 00109: val\_loss did not improve from 0.36068
Epoch 110/150
300/300 [==============================] - 13s 44ms/step - loss: 0.1331 - acc: 0.9567 - val\_loss: 0.4157 - val\_acc: 0.8934

Epoch 00110: val\_loss did not improve from 0.36068
Epoch 111/150
300/300 [==============================] - 13s 45ms/step - loss: 0.1338 - acc: 0.9633 - val\_loss: 0.4252 - val\_acc: 0.8852

Epoch 00111: val\_loss did not improve from 0.36068
Epoch 112/150
300/300 [==============================] - 13s 44ms/step - loss: 0.0875 - acc: 0.9667 - val\_loss: 0.4331 - val\_acc: 0.8852

Epoch 00112: val\_loss did not improve from 0.36068
Epoch 113/150
300/300 [==============================] - 14s 45ms/step - loss: 0.0879 - acc: 0.9667 - val\_loss: 0.4412 - val\_acc: 0.8852

Epoch 00113: val\_loss did not improve from 0.36068
Epoch 114/150
300/300 [==============================] - 13s 45ms/step - loss: 0.0970 - acc: 0.9700 - val\_loss: 0.4555 - val\_acc: 0.8852

Epoch 00114: val\_loss did not improve from 0.36068
Epoch 115/150
300/300 [==============================] - 13s 45ms/step - loss: 0.1072 - acc: 0.9567 - val\_loss: 0.5031 - val\_acc: 0.8852

Epoch 00115: val\_loss did not improve from 0.36068
Epoch 116/150
300/300 [==============================] - 13s 44ms/step - loss: 0.1209 - acc: 0.9433 - val\_loss: 0.5432 - val\_acc: 0.8689

Epoch 00116: val\_loss did not improve from 0.36068
Epoch 117/150
300/300 [==============================] - 13s 45ms/step - loss: 0.0442 - acc: 0.9867 - val\_loss: 0.4525 - val\_acc: 0.9016

Epoch 00117: val\_loss did not improve from 0.36068
Epoch 118/150
300/300 [==============================] - 13s 44ms/step - loss: 0.1231 - acc: 0.9567 - val\_loss: 0.5935 - val\_acc: 0.8689

Epoch 00118: val\_loss did not improve from 0.36068
Epoch 119/150
300/300 [==============================] - 13s 45ms/step - loss: 0.1019 - acc: 0.9567 - val\_loss: 0.4472 - val\_acc: 0.9016

Epoch 00119: val\_loss did not improve from 0.36068
Epoch 120/150
300/300 [==============================] - 13s 44ms/step - loss: 0.0596 - acc: 0.9767 - val\_loss: 0.4885 - val\_acc: 0.8852

Epoch 00120: val\_loss did not improve from 0.36068
Epoch 121/150
300/300 [==============================] - 13s 45ms/step - loss: 0.0970 - acc: 0.9567 - val\_loss: 0.5987 - val\_acc: 0.8361

Epoch 00121: val\_loss did not improve from 0.36068
Epoch 122/150
300/300 [==============================] - 14s 45ms/step - loss: 0.1918 - acc: 0.9300 - val\_loss: 0.5916 - val\_acc: 0.8115

Epoch 00122: val\_loss did not improve from 0.36068
Epoch 123/150
300/300 [==============================] - 13s 45ms/step - loss: 0.2906 - acc: 0.9267 - val\_loss: 0.5789 - val\_acc: 0.8197

Epoch 00123: val\_loss did not improve from 0.36068
Epoch 124/150
300/300 [==============================] - 14s 45ms/step - loss: 0.1778 - acc: 0.9200 - val\_loss: 0.5164 - val\_acc: 0.8525

Epoch 00124: val\_loss did not improve from 0.36068
Epoch 125/150
300/300 [==============================] - 13s 44ms/step - loss: 0.1471 - acc: 0.9433 - val\_loss: 0.5285 - val\_acc: 0.8607

Epoch 00125: val\_loss did not improve from 0.36068
Epoch 126/150
300/300 [==============================] - 14s 45ms/step - loss: 0.1554 - acc: 0.9433 - val\_loss: 0.4450 - val\_acc: 0.8607

Epoch 00126: val\_loss did not improve from 0.36068
Epoch 127/150
300/300 [==============================] - 13s 44ms/step - loss: 0.0937 - acc: 0.9667 - val\_loss: 0.4793 - val\_acc: 0.8934

Epoch 00127: val\_loss did not improve from 0.36068
Epoch 128/150
300/300 [==============================] - 13s 44ms/step - loss: 0.1273 - acc: 0.9500 - val\_loss: 0.4285 - val\_acc: 0.8852

Epoch 00128: val\_loss did not improve from 0.36068
Epoch 129/150
300/300 [==============================] - 13s 45ms/step - loss: 0.0684 - acc: 0.9867 - val\_loss: 0.5826 - val\_acc: 0.8525

Epoch 00129: val\_loss did not improve from 0.36068
Epoch 130/150
300/300 [==============================] - 13s 45ms/step - loss: 0.1506 - acc: 0.9400 - val\_loss: 0.4559 - val\_acc: 0.8443

Epoch 00130: val\_loss did not improve from 0.36068
Epoch 131/150
300/300 [==============================] - 13s 44ms/step - loss: 0.1186 - acc: 0.9667 - val\_loss: 0.4587 - val\_acc: 0.8689

Epoch 00131: val\_loss did not improve from 0.36068
Epoch 132/150
300/300 [==============================] - 13s 44ms/step - loss: 0.1083 - acc: 0.9667 - val\_loss: 0.5416 - val\_acc: 0.8770

Epoch 00132: val\_loss did not improve from 0.36068
Epoch 133/150
300/300 [==============================] - 13s 44ms/step - loss: 0.1335 - acc: 0.9667 - val\_loss: 0.4332 - val\_acc: 0.8934

Epoch 00133: val\_loss did not improve from 0.36068
Epoch 134/150
300/300 [==============================] - 13s 44ms/step - loss: 0.0924 - acc: 0.9633 - val\_loss: 0.5419 - val\_acc: 0.8770

Epoch 00134: val\_loss did not improve from 0.36068
Epoch 135/150
300/300 [==============================] - 13s 43ms/step - loss: 0.1145 - acc: 0.9533 - val\_loss: 0.4230 - val\_acc: 0.8852

Epoch 00135: val\_loss did not improve from 0.36068
Epoch 136/150
300/300 [==============================] - 13s 44ms/step - loss: 0.1552 - acc: 0.9567 - val\_loss: 0.4287 - val\_acc: 0.8770

Epoch 00136: val\_loss did not improve from 0.36068
Epoch 137/150
300/300 [==============================] - 14s 45ms/step - loss: 0.0849 - acc: 0.9733 - val\_loss: 0.5229 - val\_acc: 0.8607

Epoch 00137: val\_loss did not improve from 0.36068
Epoch 138/150
300/300 [==============================] - 13s 44ms/step - loss: 0.1255 - acc: 0.9567 - val\_loss: 0.4215 - val\_acc: 0.9016

Epoch 00138: val\_loss did not improve from 0.36068
Epoch 139/150
300/300 [==============================] - 14s 45ms/step - loss: 0.0835 - acc: 0.9633 - val\_loss: 0.4443 - val\_acc: 0.9098

Epoch 00139: val\_loss did not improve from 0.36068
Epoch 140/150
300/300 [==============================] - 14s 47ms/step - loss: 0.0773 - acc: 0.9700 - val\_loss: 0.5044 - val\_acc: 0.9098

Epoch 00140: val\_loss did not improve from 0.36068
Epoch 141/150
300/300 [==============================] - 13s 44ms/step - loss: 0.0995 - acc: 0.9700 - val\_loss: 0.4883 - val\_acc: 0.9180

Epoch 00141: val\_loss did not improve from 0.36068
Epoch 142/150
300/300 [==============================] - 14s 46ms/step - loss: 0.1227 - acc: 0.9600 - val\_loss: 0.4970 - val\_acc: 0.8770

Epoch 00142: val\_loss did not improve from 0.36068
Epoch 143/150
300/300 [==============================] - 13s 45ms/step - loss: 0.1434 - acc: 0.9500 - val\_loss: 0.5314 - val\_acc: 0.8934

Epoch 00143: val\_loss did not improve from 0.36068
Epoch 144/150
300/300 [==============================] - 13s 44ms/step - loss: 0.0904 - acc: 0.9733 - val\_loss: 0.6663 - val\_acc: 0.8770

Epoch 00144: val\_loss did not improve from 0.36068
Epoch 145/150
300/300 [==============================] - 13s 44ms/step - loss: 0.1780 - acc: 0.9500 - val\_loss: 0.5435 - val\_acc: 0.8934

Epoch 00145: val\_loss did not improve from 0.36068
Epoch 146/150
300/300 [==============================] - 13s 44ms/step - loss: 0.1388 - acc: 0.9600 - val\_loss: 0.5899 - val\_acc: 0.8525

Epoch 00146: val\_loss did not improve from 0.36068
Epoch 147/150
300/300 [==============================] - 13s 44ms/step - loss: 0.0671 - acc: 0.9833 - val\_loss: 0.5064 - val\_acc: 0.8934

Epoch 00147: val\_loss did not improve from 0.36068
Epoch 148/150
300/300 [==============================] - 14s 45ms/step - loss: 0.0563 - acc: 0.9733 - val\_loss: 0.6033 - val\_acc: 0.8852

Epoch 00148: val\_loss did not improve from 0.36068
Epoch 149/150
300/300 [==============================] - 14s 45ms/step - loss: 0.1350 - acc: 0.9667 - val\_loss: 0.6928 - val\_acc: 0.8525

Epoch 00149: val\_loss did not improve from 0.36068
Epoch 150/150
300/300 [==============================] - 13s 44ms/step - loss: 0.2057 - acc: 0.9300 - val\_loss: 0.3515 - val\_acc: 0.9016

Epoch 00150: val\_loss improved from 0.36068 to 0.35154, saving model to Model\_10\_bg.weights.best.hdf5

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}81}]:} \PY{c+c1}{\PYZsh{} Loading the model that performed the best on the validation set}
         \PY{n}{model10\PYZus{}bg}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model\PYZus{}10\PYZus{}bg.weights.best.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Testing the model on the Test data}
         \PY{p}{(}\PY{n}{loss10\PYZus{}bg}\PY{p}{,} \PY{n}{accuracy10\PYZus{}bg}\PY{p}{)} \PY{o}{=} \PY{n}{model10\PYZus{}bg}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}bgsub}\PY{p}{,} \PY{n}{y\PYZus{}test\PYZus{}bgsub}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
         
         \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy on test data: \PYZob{}:.2f\PYZcb{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracy10\PYZus{}bg} \PY{o}{*} \PY{l+m+mi}{100}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy on test data: 82.00\%

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}83}]:} \PY{c+c1}{\PYZsh{} Making the plot larger}
         \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{14}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{loss10\PYZus{}bg} \PY{o}{=} \PY{n}{history10\PYZus{}bg}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}                          \PY{c+c1}{\PYZsh{} Loss on the training data}
         \PY{n}{val\PYZus{}loss10\PYZus{}bg} \PY{o}{=} \PY{n}{history10\PYZus{}bg}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}                  \PY{c+c1}{\PYZsh{} Loss on the validation data}
         \PY{n}{epochs} \PY{o}{=} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{151}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{loss10\PYZus{}bg}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ro\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{val\PYZus{}loss10\PYZus{}bg}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{go\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}83}]:} <matplotlib.legend.Legend at 0x7f2611220c90>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_91_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{loss10}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ro\PYZhy{}.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{val\PYZus{}loss10}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{go\PYZhy{}.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{loss10\PYZus{}bg}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bo\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Loss \PYZhy{} BG Sub}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{val\PYZus{}loss10\PYZus{}bg}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{yo\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation Loss \PYZhy{} BG Sub}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k+kn}{from} \PY{n+nn}{sklearn.metrics} \PY{k+kn}{import} \PY{n}{confusion\PYZus{}matrix}
        \PY{k+kn}{import} \PY{n+nn}{itertools}
        \PY{n}{y\PYZus{}predictions10\PYZus{}bg} \PY{o}{=} \PY{n}{model10\PYZus{}bg}\PY{o}{.}\PY{n}{predict\PYZus{}classes}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}bgsub}\PY{p}{)}\PY{p}{;}
        \PY{n}{y\PYZus{}pred10\PYZus{}bg}\PY{o}{=}\PY{n}{to\PYZus{}categorical}\PY{p}{(}\PY{n}{y\PYZus{}predictions10\PYZus{}bg}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{;}
        \PY{n}{y\PYZus{}test\PYZus{}cm} \PY{o}{=} \PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{;}
        \PY{c+c1}{\PYZsh{}print y\PYZus{}predictions[:50]}
        \PY{c+c1}{\PYZsh{}print y\PYZus{}test[:50]}
        \PY{n}{model\PYZus{}cnf\PYZus{}matrix10\PYZus{}bg} \PY{o}{=} \PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{y\PYZus{}test\PYZus{}cm}\PY{p}{,}\PY{n}{y\PYZus{}pred10\PYZus{}bg}\PY{p}{)}\PY{p}{;}
        \PY{n}{confusion\PYZus{}matrix\PYZus{}plot} \PY{o}{=} \PY{n}{plot\PYZus{}confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{model\PYZus{}cnf\PYZus{}matrix10\PYZus{}bg}\PY{p}{,} 
                                                      \PY{n}{classes}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{boxing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{handclapping}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{handwaving}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{jogging}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{running}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{walking}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} 
                                                      \PY{n}{normalize}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}
\end{Verbatim}


    \hypertarget{model-with-varying-dropouts}{%
\subsection{Model with Varying
Dropouts}\label{model-with-varying-dropouts}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k+kn}{from} \PY{n+nn}{keras.models} \PY{k+kn}{import} \PY{n}{Sequential}
        \PY{k+kn}{from} \PY{n+nn}{keras.layers} \PY{k+kn}{import} \PY{n}{Conv3D}\PY{p}{,} \PY{n}{MaxPooling3D}\PY{p}{,} \PY{n}{GlobalAveragePooling3D}\PY{p}{,} \PY{n}{BatchNormalization}
        \PY{k+kn}{from} \PY{n+nn}{keras.layers.core} \PY{k+kn}{import} \PY{n}{Dense}\PY{p}{,} \PY{n}{Dropout}
        
        \PY{c+c1}{\PYZsh{} Using the Sequential Model}
        \PY{n}{model4\PYZus{}bg} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Adding Alternate convolutional and pooling layers}
        \PY{n}{model4\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv3D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                         \PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}
        \PY{n}{model4\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling3D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{model4\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv3D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        \PY{n}{model4\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling3D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{model4\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv3D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        \PY{n}{model4\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling3D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{}model.add(Conv3D(filters=1024, kernel\PYZus{}size=(2, 3, 3), strides=(1, 1, 1), padding=\PYZsq{}same\PYZsq{}, activation=\PYZsq{}relu\PYZsq{}))}
        \PY{c+c1}{\PYZsh{}model.add(MaxPooling3D(pool\PYZus{}size=2, strides=(2, 2, 2), padding=\PYZsq{}same\PYZsq{}))}
        
        \PY{c+c1}{\PYZsh{} A global average pooling layer to get a 1\PYZhy{}d vector}
        \PY{c+c1}{\PYZsh{} The vector will have a depth (same as number of elements in the vector) of 1024}
        \PY{n}{model4\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{GlobalAveragePooling3D}\PY{p}{(}\PY{p}{)}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Hidden layer}
        \PY{n}{model4\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Dropout Layer}
        \PY{n}{model4\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.3}\PY{p}{)}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Output layer}
        \PY{n}{model4\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{model4\PYZus{}bg}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{} Imports}
        \PY{k+kn}{from} \PY{n+nn}{keras.callbacks} \PY{k+kn}{import} \PY{n}{ModelCheckpoint}
        
        \PY{c+c1}{\PYZsh{} Compiling the model}
        \PY{n}{model4\PYZus{}bg}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{nadam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Saving the model that performed the best on the validation set}
        \PY{n}{checkpoint4\PYZus{}bg} \PY{o}{=} \PY{n}{ModelCheckpoint}\PY{p}{(}\PY{n}{filepath}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model\PYZus{}4\PYZus{}bg.weights.best.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{save\PYZus{}best\PYZus{}only}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
        
        
        \PY{n}{history4\PYZus{}bg} \PY{o}{=} \PY{n}{model4\PYZus{}bg}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}bgsub}\PY{p}{,} \PY{n}{y\PYZus{}train\PYZus{}bgsub}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{60}\PY{p}{,} 
                            \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{X\PYZus{}valid\PYZus{}bgsub}\PY{p}{,} \PY{n}{y\PYZus{}valid\PYZus{}bgsub}\PY{p}{)}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{checkpoint4\PYZus{}bg}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{} Loading the model that performed the best on the validation set}
        \PY{n}{model4\PYZus{}bg}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model\PYZus{}4\PYZus{}bg.weights.best.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Testing the model on the Test data}
        \PY{p}{(}\PY{n}{loss4\PYZus{}bg}\PY{p}{,} \PY{n}{accuracy4\PYZus{}bg}\PY{p}{)} \PY{o}{=} \PY{n}{model4\PYZus{}bg}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}bgsub}\PY{p}{,} \PY{n}{y\PYZus{}test\PYZus{}bgsub}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
        
        \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy on test data: \PYZob{}:.2f\PYZcb{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracy4\PYZus{}bg} \PY{o}{*} \PY{l+m+mi}{100}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{} Making the plot larger}
        \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{14}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{loss4\PYZus{}bg} \PY{o}{=} \PY{n}{history4\PYZus{}bg}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}                          \PY{c+c1}{\PYZsh{} Loss on the training data}
        \PY{n}{val\PYZus{}loss4\PYZus{}bg} \PY{o}{=} \PY{n}{history4\PYZus{}bg}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}                  \PY{c+c1}{\PYZsh{} Loss on the validation data}
        \PY{n}{epochs} \PY{o}{=} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{61}\PY{p}{)}
        
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{loss4\PYZus{}bg}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ro\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{val\PYZus{}loss4\PYZus{}bg}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{go\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{loss4\PYZus{}bg}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ro\PYZhy{}.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Loss \PYZhy{}0.3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{val\PYZus{}loss4\PYZus{}bg}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{go\PYZhy{}.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation Loss \PYZhy{}0.3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{loss3\PYZus{}bg}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bo\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Loss \PYZhy{} 0.5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{val\PYZus{}loss3\PYZus{}bg}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{yo\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation Loss \PYZhy{} 0.5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k+kn}{from} \PY{n+nn}{sklearn.metrics} \PY{k+kn}{import} \PY{n}{confusion\PYZus{}matrix}
        \PY{k+kn}{import} \PY{n+nn}{itertools}
        \PY{n}{y\PYZus{}predictions4\PYZus{}bg} \PY{o}{=} \PY{n}{model4\PYZus{}bg}\PY{o}{.}\PY{n}{predict\PYZus{}classes}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}bgsub}\PY{p}{)}\PY{p}{;}
        \PY{n}{y\PYZus{}pred4\PYZus{}bg}\PY{o}{=}\PY{n}{to\PYZus{}categorical}\PY{p}{(}\PY{n}{y\PYZus{}predictions4\PYZus{}bg}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{;}
        \PY{n}{y\PYZus{}test\PYZus{}cm} \PY{o}{=} \PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{;}
        \PY{c+c1}{\PYZsh{}print y\PYZus{}predictions[:50]}
        \PY{c+c1}{\PYZsh{}print y\PYZus{}test[:50]}
        \PY{n}{model\PYZus{}cnf\PYZus{}matrix4\PYZus{}bg} \PY{o}{=} \PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{y\PYZus{}test\PYZus{}cm}\PY{p}{,}\PY{n}{y\PYZus{}pred4\PYZus{}bg}\PY{p}{)}\PY{p}{;}
        \PY{n}{confusion\PYZus{}matrix\PYZus{}plot} \PY{o}{=} \PY{n}{plot\PYZus{}confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{model\PYZus{}cnf\PYZus{}matrix4\PYZus{}bg}\PY{p}{,} 
                                                      \PY{n}{classes}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{boxing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{handclapping}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{handwaving}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{jogging}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{running}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{walking}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} 
                                                      \PY{n}{normalize}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}
\end{Verbatim}


    \hypertarget{dropout}{%
\subsection{0.7 Dropout}\label{dropout}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k+kn}{from} \PY{n+nn}{keras.models} \PY{k+kn}{import} \PY{n}{Sequential}
        \PY{k+kn}{from} \PY{n+nn}{keras.layers} \PY{k+kn}{import} \PY{n}{Conv3D}\PY{p}{,} \PY{n}{MaxPooling3D}\PY{p}{,} \PY{n}{GlobalAveragePooling3D}\PY{p}{,} \PY{n}{BatchNormalization}
        \PY{k+kn}{from} \PY{n+nn}{keras.layers.core} \PY{k+kn}{import} \PY{n}{Dense}\PY{p}{,} \PY{n}{Dropout}
        
        \PY{c+c1}{\PYZsh{} Using the Sequential Model}
        \PY{n}{model5\PYZus{}bg} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Adding Alternate convolutional and pooling layers}
        \PY{n}{model5\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv3D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                         \PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}
        \PY{n}{model5\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling3D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{model5\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv3D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        \PY{n}{model5\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling3D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{model5\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv3D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        \PY{n}{model5\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling3D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{}model.add(Conv3D(filters=1024, kernel\PYZus{}size=(2, 3, 3), strides=(1, 1, 1), padding=\PYZsq{}same\PYZsq{}, activation=\PYZsq{}relu\PYZsq{}))}
        \PY{c+c1}{\PYZsh{}model.add(MaxPooling3D(pool\PYZus{}size=2, strides=(2, 2, 2), padding=\PYZsq{}same\PYZsq{}))}
        
        \PY{c+c1}{\PYZsh{} A global average pooling layer to get a 1\PYZhy{}d vector}
        \PY{c+c1}{\PYZsh{} The vector will have a depth (same as number of elements in the vector) of 1024}
        \PY{n}{model5\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{GlobalAveragePooling3D}\PY{p}{(}\PY{p}{)}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Hidden layer}
        \PY{n}{model5\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Dropout Layer}
        \PY{n}{model5\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.7}\PY{p}{)}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Output layer}
        \PY{n}{model5\PYZus{}bg}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{model5\PYZus{}bg}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{} Imports}
        \PY{k+kn}{from} \PY{n+nn}{keras.callbacks} \PY{k+kn}{import} \PY{n}{ModelCheckpoint}
        
        \PY{c+c1}{\PYZsh{} Compiling the model}
        \PY{n}{model5\PYZus{}bg}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{nadam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Saving the model that performed the best on the validation set}
        \PY{n}{checkpoint5\PYZus{}bg} \PY{o}{=} \PY{n}{ModelCheckpoint}\PY{p}{(}\PY{n}{filepath}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model\PYZus{}5\PYZus{}bg.weights.best.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{save\PYZus{}best\PYZus{}only}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
        
        
        \PY{n}{history5\PYZus{}bg} \PY{o}{=} \PY{n}{model5\PYZus{}bg}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}bgsub}\PY{p}{,} \PY{n}{y\PYZus{}train\PYZus{}bgsub}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{60}\PY{p}{,} 
                            \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{X\PYZus{}valid\PYZus{}bgsub}\PY{p}{,} \PY{n}{y\PYZus{}valid\PYZus{}bgsub}\PY{p}{)}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{checkpoint5\PYZus{}bg}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{} Loading the model that performed the best on the validation set}
        \PY{n}{model5\PYZus{}bg}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model\PYZus{}5\PYZus{}bg.weights.best.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Testing the model on the Test data}
        \PY{p}{(}\PY{n}{loss5\PYZus{}bg}\PY{p}{,} \PY{n}{accuracy5\PYZus{}bg}\PY{p}{)} \PY{o}{=} \PY{n}{model5\PYZus{}bg}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}bgsub}\PY{p}{,} \PY{n}{y\PYZus{}test\PYZus{}bgsub}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
        
        \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy on test data: \PYZob{}:.2f\PYZcb{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracy5\PYZus{}bg} \PY{o}{*} \PY{l+m+mi}{100}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{} Making the plot larger}
        \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{14}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{loss5\PYZus{}bg} \PY{o}{=} \PY{n}{history5\PYZus{}bg}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}                          \PY{c+c1}{\PYZsh{} Loss on the training data}
        \PY{n}{val\PYZus{}loss5\PYZus{}bg} \PY{o}{=} \PY{n}{history5\PYZus{}bg}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}                  \PY{c+c1}{\PYZsh{} Loss on the validation data}
        \PY{n}{epochs} \PY{o}{=} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{61}\PY{p}{)}
        
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{loss5\PYZus{}bg}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ro\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{val\PYZus{}loss5\PYZus{}bg}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{go\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{loss4\PYZus{}bg}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ro\PYZhy{}.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Loss \PYZhy{}0.3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{val\PYZus{}loss4\PYZus{}bg}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{go\PYZhy{}.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation Loss \PYZhy{}0.3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{loss3\PYZus{}bg}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bo\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Loss \PYZhy{} 0.5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{val\PYZus{}loss3\PYZus{}bg}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{yo\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation Loss \PYZhy{} 0.5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{loss5\PYZus{}bg}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b\PYZca{}\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{black}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Loss \PYZhy{} 0.7}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{val\PYZus{}loss5\PYZus{}bg}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b\PYZca{}\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{purple}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation Loss \PYZhy{} 0.7}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k+kn}{from} \PY{n+nn}{sklearn.metrics} \PY{k+kn}{import} \PY{n}{confusion\PYZus{}matrix}
        \PY{k+kn}{import} \PY{n+nn}{itertools}
        \PY{n}{y\PYZus{}predictions4\PYZus{}bg} \PY{o}{=} \PY{n}{model5\PYZus{}bg}\PY{o}{.}\PY{n}{predict\PYZus{}classes}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}bgsub}\PY{p}{)}\PY{p}{;}
        \PY{n}{y\PYZus{}pred4\PYZus{}bg}\PY{o}{=}\PY{n}{to\PYZus{}categorical}\PY{p}{(}\PY{n}{y\PYZus{}predictions4\PYZus{}bg}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{;}
        \PY{n}{y\PYZus{}test\PYZus{}cm} \PY{o}{=} \PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{;}
        \PY{c+c1}{\PYZsh{}print y\PYZus{}predictions[:50]}
        \PY{c+c1}{\PYZsh{}print y\PYZus{}test[:50]}
        \PY{n}{model\PYZus{}cnf\PYZus{}matrix4\PYZus{}bg} \PY{o}{=} \PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{y\PYZus{}test\PYZus{}cm}\PY{p}{,}\PY{n}{y\PYZus{}pred4\PYZus{}bg}\PY{p}{)}\PY{p}{;}
        \PY{n}{confusion\PYZus{}matrix\PYZus{}plot} \PY{o}{=} \PY{n}{plot\PYZus{}confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{model\PYZus{}cnf\PYZus{}matrix4\PYZus{}bg}\PY{p}{,} 
                                                      \PY{n}{classes}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{boxing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{handclapping}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{handwaving}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{jogging}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{running}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{walking}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} 
                                                      \PY{n}{normalize}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}
\end{Verbatim}


    \hypertarget{different-input-dimensions-on-model3}{%
\subsection{Different Input dimensions on
model3}\label{different-input-dimensions-on-model3}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{X\PYZus{}train\PYZus{}bgsub\PYZus{}2} \PY{o}{=} \PY{n}{read\PYZus{}videos\PYZus{}bgsub}\PY{p}{(}\PY{n}{train\PYZus{}files}\PY{p}{,}\PY{l+m+mi}{40}\PY{p}{)}
        \PY{n}{y\PYZus{}train\PYZus{}bgsub\PYZus{}2} \PY{o}{=} \PY{n}{to\PYZus{}categorical}\PY{p}{(}\PY{n}{train\PYZus{}targets}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}
        \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape of training data:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{X\PYZus{}train\PYZus{}bgsub\PYZus{}2}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
        \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape of training labels:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y\PYZus{}train\PYZus{}bgsub\PYZus{}2}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{X\PYZus{}test\PYZus{}bgsub\PYZus{}2} \PY{o}{=} \PY{n}{read\PYZus{}videos\PYZus{}bgsub}\PY{p}{(}\PY{n}{test\PYZus{}files}\PY{p}{,}\PY{l+m+mi}{40}\PY{p}{)}
        \PY{n}{y\PYZus{}test\PYZus{}bgsub\PYZus{}2} \PY{o}{=} \PY{n}{to\PYZus{}categorical}\PY{p}{(}\PY{n}{test\PYZus{}targets}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}
        \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape of testing data:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{X\PYZus{}test\PYZus{}bgsub\PYZus{}2}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
        \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape of testing labels:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y\PYZus{}test\PYZus{}bgsub\PYZus{}2}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{X\PYZus{}valid\PYZus{}bgsub\PYZus{}2} \PY{o}{=} \PY{n}{read\PYZus{}videos\PYZus{}bgsub}\PY{p}{(}\PY{n}{valid\PYZus{}files}\PY{p}{,}\PY{l+m+mi}{40}\PY{p}{)}
        \PY{n}{y\PYZus{}valid\PYZus{}bgsub\PYZus{}2} \PY{o}{=} \PY{n}{to\PYZus{}categorical}\PY{p}{(}\PY{n}{valid\PYZus{}targets}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}
        \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape of validation data:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{X\PYZus{}valid\PYZus{}bgsub\PYZus{}2}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
        \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape of validation labels:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y\PYZus{}valid\PYZus{}bgsub\PYZus{}2}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{X\PYZus{}train\PYZus{}bgsub\PYZus{}3} \PY{o}{=} \PY{n}{read\PYZus{}videos\PYZus{}bgsub}\PY{p}{(}\PY{n}{train\PYZus{}files}\PY{p}{,}\PY{l+m+mi}{60}\PY{p}{)}
        \PY{n}{y\PYZus{}train\PYZus{}bgsub\PYZus{}3} \PY{o}{=} \PY{n}{to\PYZus{}categorical}\PY{p}{(}\PY{n}{train\PYZus{}targets}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}
        \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape of training data:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{X\PYZus{}train\PYZus{}bgsub\PYZus{}3}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
        \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape of training labels:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y\PYZus{}train\PYZus{}bgsub\PYZus{}3}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{X\PYZus{}test\PYZus{}bgsub\PYZus{}3} \PY{o}{=} \PY{n}{read\PYZus{}videos\PYZus{}bgsub}\PY{p}{(}\PY{n}{test\PYZus{}files}\PY{p}{,}\PY{l+m+mi}{60}\PY{p}{)}
        \PY{n}{y\PYZus{}test\PYZus{}bgsub\PYZus{}3} \PY{o}{=} \PY{n}{to\PYZus{}categorical}\PY{p}{(}\PY{n}{test\PYZus{}targets}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}
        \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape of testing data:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{X\PYZus{}test\PYZus{}bgsub\PYZus{}3}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
        \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape of testing labels:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y\PYZus{}test\PYZus{}bgsub\PYZus{}3}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{X\PYZus{}valid\PYZus{}bgsub\PYZus{}3} \PY{o}{=} \PY{n}{read\PYZus{}videos\PYZus{}bgsub}\PY{p}{(}\PY{n}{valid\PYZus{}files}\PY{p}{,}\PY{l+m+mi}{60}\PY{p}{)}
        \PY{n}{y\PYZus{}valid\PYZus{}bgsub\PYZus{}3} \PY{o}{=} \PY{n}{to\PYZus{}categorical}\PY{p}{(}\PY{n}{valid\PYZus{}targets}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}
        \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape of validation data:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{X\PYZus{}valid\PYZus{}bgsub\PYZus{}3}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
        \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape of validation labels:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y\PYZus{}valid\PYZus{}bgsub\PYZus{}3}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k+kn}{from} \PY{n+nn}{keras.models} \PY{k+kn}{import} \PY{n}{Sequential}
        \PY{k+kn}{from} \PY{n+nn}{keras.layers} \PY{k+kn}{import} \PY{n}{Conv3D}\PY{p}{,} \PY{n}{MaxPooling3D}\PY{p}{,} \PY{n}{GlobalAveragePooling3D}\PY{p}{,} \PY{n}{BatchNormalization}
        \PY{k+kn}{from} \PY{n+nn}{keras.layers.core} \PY{k+kn}{import} \PY{n}{Dense}\PY{p}{,} \PY{n}{Dropout}
        
        \PY{c+c1}{\PYZsh{} Using the Sequential Model}
        \PY{n}{model3\PYZus{}bg2} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Adding Alternate convolutional and pooling layers}
        \PY{n}{model3\PYZus{}bg2}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv3D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                         \PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}bgsub\PYZus{}2}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}
        \PY{n}{model3\PYZus{}bg2}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling3D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{model3\PYZus{}bg2}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv3D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        \PY{n}{model3\PYZus{}bg2}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling3D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{model3\PYZus{}bg2}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv3D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        \PY{n}{model3\PYZus{}bg2}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling3D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{}model.add(Conv3D(filters=1024, kernel\PYZus{}size=(2, 3, 3), strides=(1, 1, 1), padding=\PYZsq{}same\PYZsq{}, activation=\PYZsq{}relu\PYZsq{}))}
        \PY{c+c1}{\PYZsh{}model.add(MaxPooling3D(pool\PYZus{}size=2, strides=(2, 2, 2), padding=\PYZsq{}same\PYZsq{}))}
        
        \PY{c+c1}{\PYZsh{} A global average pooling layer to get a 1\PYZhy{}d vector}
        \PY{c+c1}{\PYZsh{} The vector will have a depth (same as number of elements in the vector) of 1024}
        \PY{n}{model3\PYZus{}bg2}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{GlobalAveragePooling3D}\PY{p}{(}\PY{p}{)}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Hidden layer}
        \PY{n}{model3\PYZus{}bg2}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Dropout Layer}
        \PY{n}{model3\PYZus{}bg2}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Output layer}
        \PY{n}{model3\PYZus{}bg2}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{model3\PYZus{}bg2}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{} Imports}
        \PY{k+kn}{from} \PY{n+nn}{keras.callbacks} \PY{k+kn}{import} \PY{n}{ModelCheckpoint}
        
        \PY{c+c1}{\PYZsh{} Compiling the model}
        \PY{n}{model3\PYZus{}bg2}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{nadam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Saving the model that performed the best on the validation set}
        \PY{n}{checkpoint3\PYZus{}bg2} \PY{o}{=} \PY{n}{ModelCheckpoint}\PY{p}{(}\PY{n}{filepath}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model\PYZus{}3\PYZus{}bg2.weights.best.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{save\PYZus{}best\PYZus{}only}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
        
        
        \PY{n}{history3\PYZus{}bg2} \PY{o}{=} \PY{n}{model3\PYZus{}bg2}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}bgsub\PYZus{}2}\PY{p}{,} \PY{n}{y\PYZus{}train\PYZus{}bgsub\PYZus{}2}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{60}\PY{p}{,} 
                            \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{X\PYZus{}valid\PYZus{}bgsub\PYZus{}2}\PY{p}{,} \PY{n}{y\PYZus{}valid\PYZus{}bgsub\PYZus{}2}\PY{p}{)}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{checkpoint3\PYZus{}bg2}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{} Loading the model that performed the best on the validation set}
        \PY{n}{model3\PYZus{}bg2}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model\PYZus{}3\PYZus{}bg2.weights.best.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Testing the model on the Test data}
        \PY{p}{(}\PY{n}{loss3\PYZus{}bg2}\PY{p}{,} \PY{n}{accuracy3\PYZus{}bg2}\PY{p}{)} \PY{o}{=} \PY{n}{model3\PYZus{}bg2}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}bgsub\PYZus{}2}\PY{p}{,} \PY{n}{y\PYZus{}test\PYZus{}bgsub\PYZus{}2}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
        
        \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy on test data: \PYZob{}:.2f\PYZcb{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracy3\PYZus{}bg2} \PY{o}{*} \PY{l+m+mi}{100}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{} Making the plot larger}
        \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{14}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{loss3\PYZus{}bg2} \PY{o}{=} \PY{n}{history3\PYZus{}bg2}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}                          \PY{c+c1}{\PYZsh{} Loss on the training data}
        \PY{n}{val\PYZus{}loss3\PYZus{}bg2} \PY{o}{=} \PY{n}{history3\PYZus{}bg2}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}                  \PY{c+c1}{\PYZsh{} Loss on the validation data}
        \PY{n}{epochs} \PY{o}{=} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{61}\PY{p}{)}
        
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{loss3\PYZus{}bg2}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ro\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{val\PYZus{}loss3\PYZus{}bg2}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{go\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{loss3\PYZus{}bg2}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ro\PYZhy{}.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Loss \PYZhy{} 40*40}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{val\PYZus{}loss3\PYZus{}bg2}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{go\PYZhy{}.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation Loss \PYZhy{} 40*40}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{loss3\PYZus{}bg}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bo\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Loss \PYZhy{} 20*20}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{val\PYZus{}loss3\PYZus{}bg}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{yo\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation Loss \PYZhy{} 20*20}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k+kn}{from} \PY{n+nn}{sklearn.metrics} \PY{k+kn}{import} \PY{n}{confusion\PYZus{}matrix}
        \PY{k+kn}{import} \PY{n+nn}{itertools}
        \PY{n}{y\PYZus{}predictions3\PYZus{}bg2} \PY{o}{=} \PY{n}{model3\PYZus{}bg2}\PY{o}{.}\PY{n}{predict\PYZus{}classes}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}bgsub\PYZus{}2}\PY{p}{)}\PY{p}{;}
        \PY{n}{y\PYZus{}pred3\PYZus{}bg2}\PY{o}{=}\PY{n}{to\PYZus{}categorical}\PY{p}{(}\PY{n}{y\PYZus{}predictions3\PYZus{}bg2}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{;}
        \PY{n}{y\PYZus{}test\PYZus{}cm} \PY{o}{=} \PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{;}
        \PY{c+c1}{\PYZsh{}print y\PYZus{}predictions[:50]}
        \PY{c+c1}{\PYZsh{}print y\PYZus{}test[:50]}
        \PY{n}{model\PYZus{}cnf\PYZus{}matrix3\PYZus{}bg2} \PY{o}{=} \PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{y\PYZus{}test\PYZus{}cm}\PY{p}{,}\PY{n}{y\PYZus{}pred3\PYZus{}bg2}\PY{p}{)}\PY{p}{;}
        \PY{n}{confusion\PYZus{}matrix\PYZus{}plot} \PY{o}{=} \PY{n}{plot\PYZus{}confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{model\PYZus{}cnf\PYZus{}matrix3\PYZus{}bg2}\PY{p}{,} 
                                                      \PY{n}{classes}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{boxing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{handclapping}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{handwaving}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{jogging}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{running}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{walking}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} 
                                                      \PY{n}{normalize}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}
\end{Verbatim}


    \hypertarget{section}{%
\subsection{60*60}\label{section}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k+kn}{from} \PY{n+nn}{keras.models} \PY{k+kn}{import} \PY{n}{Sequential}
        \PY{k+kn}{from} \PY{n+nn}{keras.layers} \PY{k+kn}{import} \PY{n}{Conv3D}\PY{p}{,} \PY{n}{MaxPooling3D}\PY{p}{,} \PY{n}{GlobalAveragePooling3D}\PY{p}{,} \PY{n}{BatchNormalization}
        \PY{k+kn}{from} \PY{n+nn}{keras.layers.core} \PY{k+kn}{import} \PY{n}{Dense}\PY{p}{,} \PY{n}{Dropout}
        
        \PY{c+c1}{\PYZsh{} Using the Sequential Model}
        \PY{n}{model3\PYZus{}bg3} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Adding Alternate convolutional and pooling layers}
        \PY{n}{model3\PYZus{}bg3}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv3D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                         \PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}bgsub\PYZus{}3}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}
        \PY{n}{model3\PYZus{}bg3}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling3D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{model3\PYZus{}bg3}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv3D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        \PY{n}{model3\PYZus{}bg3}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling3D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{model3\PYZus{}bg3}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv3D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        \PY{n}{model3\PYZus{}bg3}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling3D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{}model.add(Conv3D(filters=1024, kernel\PYZus{}size=(2, 3, 3), strides=(1, 1, 1), padding=\PYZsq{}same\PYZsq{}, activation=\PYZsq{}relu\PYZsq{}))}
        \PY{c+c1}{\PYZsh{}model.add(MaxPooling3D(pool\PYZus{}size=2, strides=(2, 2, 2), padding=\PYZsq{}same\PYZsq{}))}
        
        \PY{c+c1}{\PYZsh{} A global average pooling layer to get a 1\PYZhy{}d vector}
        \PY{c+c1}{\PYZsh{} The vector will have a depth (same as number of elements in the vector) of 1024}
        \PY{n}{model3\PYZus{}bg3}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{GlobalAveragePooling3D}\PY{p}{(}\PY{p}{)}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Hidden layer}
        \PY{n}{model3\PYZus{}bg3}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Dropout Layer}
        \PY{n}{model3\PYZus{}bg3}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Output layer}
        \PY{n}{model3\PYZus{}bg3}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{model3\PYZus{}bg3}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{} Imports}
        \PY{k+kn}{from} \PY{n+nn}{keras.callbacks} \PY{k+kn}{import} \PY{n}{ModelCheckpoint}
        
        \PY{c+c1}{\PYZsh{} Compiling the model}
        \PY{n}{model3\PYZus{}bg3}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{nadam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Saving the model that performed the best on the validation set}
        \PY{n}{checkpoint3\PYZus{}bg3} \PY{o}{=} \PY{n}{ModelCheckpoint}\PY{p}{(}\PY{n}{filepath}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model\PYZus{}3\PYZus{}bg3.weights.best.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{save\PYZus{}best\PYZus{}only}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
        
        
        \PY{n}{history3\PYZus{}bg3} \PY{o}{=} \PY{n}{model3\PYZus{}bg3}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}bgsub\PYZus{}3}\PY{p}{,} \PY{n}{y\PYZus{}train\PYZus{}bgsub\PYZus{}3}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{60}\PY{p}{,} 
                            \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{X\PYZus{}valid\PYZus{}bgsub\PYZus{}3}\PY{p}{,} \PY{n}{y\PYZus{}valid\PYZus{}bgsub\PYZus{}3}\PY{p}{)}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{checkpoint3\PYZus{}bg3}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{} Loading the model that performed the best on the validation set}
        \PY{n}{model3\PYZus{}bg3}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model\PYZus{}3\PYZus{}bg3.weights.best.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Testing the model on the Test data}
        \PY{p}{(}\PY{n}{loss3\PYZus{}bg3}\PY{p}{,} \PY{n}{accuracy3\PYZus{}bg3}\PY{p}{)} \PY{o}{=} \PY{n}{model3\PYZus{}bg3}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}bgsub\PYZus{}3}\PY{p}{,} \PY{n}{y\PYZus{}test\PYZus{}bgsub\PYZus{}3}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
        
        \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy on test data: \PYZob{}:.2f\PYZcb{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracy3\PYZus{}bg3} \PY{o}{*} \PY{l+m+mi}{100}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{} Making the plot larger}
        \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{14}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{loss3\PYZus{}bg3} \PY{o}{=} \PY{n}{history3\PYZus{}bg3}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}                          \PY{c+c1}{\PYZsh{} Loss on the training data}
        \PY{n}{val\PYZus{}loss3\PYZus{}bg3} \PY{o}{=} \PY{n}{history3\PYZus{}bg3}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}                  \PY{c+c1}{\PYZsh{} Loss on the validation data}
        \PY{n}{epochs} \PY{o}{=} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{61}\PY{p}{)}
        
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{loss3\PYZus{}bg3}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ro\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{val\PYZus{}loss3\PYZus{}bg3}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{go\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{loss3\PYZus{}bg3}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ro\PYZhy{}.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Loss \PYZhy{} 60*60}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{val\PYZus{}loss3\PYZus{}bg3}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{go\PYZhy{}.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation Loss \PYZhy{} 60*60}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{loss3\PYZus{}bg2}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ro\PYZhy{}.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{black}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Loss \PYZhy{} 40*40}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{val\PYZus{}loss3\PYZus{}bg2}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{go\PYZhy{}.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{purple}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation Loss \PYZhy{} 40*40}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{loss3\PYZus{}bg}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bo\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Loss \PYZhy{} 20*20}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{val\PYZus{}loss3\PYZus{}bg}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{yo\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation Loss \PYZhy{} 20*20}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k+kn}{from} \PY{n+nn}{sklearn.metrics} \PY{k+kn}{import} \PY{n}{confusion\PYZus{}matrix}
        \PY{k+kn}{import} \PY{n+nn}{itertools}
        \PY{n}{y\PYZus{}predictions3\PYZus{}bg3} \PY{o}{=} \PY{n}{model3\PYZus{}bg3}\PY{o}{.}\PY{n}{predict\PYZus{}classes}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}bgsub\PYZus{}3}\PY{p}{)}\PY{p}{;}
        \PY{n}{y\PYZus{}pred3\PYZus{}bg3}\PY{o}{=}\PY{n}{to\PYZus{}categorical}\PY{p}{(}\PY{n}{y\PYZus{}predictions3\PYZus{}bg3}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{;}
        \PY{n}{y\PYZus{}test\PYZus{}cm} \PY{o}{=} \PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{;}
        \PY{c+c1}{\PYZsh{}print y\PYZus{}predictions[:50]}
        \PY{c+c1}{\PYZsh{}print y\PYZus{}test[:50]}
        \PY{n}{model\PYZus{}cnf\PYZus{}matrix3\PYZus{}bg} \PY{o}{=} \PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{y\PYZus{}test\PYZus{}cm}\PY{p}{,}\PY{n}{y\PYZus{}pred3\PYZus{}bg3}\PY{p}{)}\PY{p}{;}
        \PY{n}{confusion\PYZus{}matrix\PYZus{}plot} \PY{o}{=} \PY{n}{plot\PYZus{}confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{model\PYZus{}cnf\PYZus{}matrix3\PYZus{}bg}\PY{p}{,} 
                                                      \PY{n}{classes}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{boxing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{handclapping}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{handwaving}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{jogging}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{running}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{walking}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} 
                                                      \PY{n}{normalize}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}
\end{Verbatim}


    \hypertarget{ignore-the-below-models}{%
\section{Ignore the below models}\label{ignore-the-below-models}}
from keras.models import Sequential
from keras.layers import Conv3D, MaxPooling3D, GlobalAveragePooling3D, BatchNormalization
from keras.layers.core import Dense, Dropout

# Using the Sequential Model
model2 = Sequential()

# Adding Alternate convolutional and pooling layers
model2.add(Conv3D(filters=16, kernel_size=(5, 3, 3), strides=(1, 1, 1), padding='same', activation='relu', 
                 input_shape=X_train.shape[1:]))
model2.add(MaxPooling3D(pool_size=2, strides=(2, 2, 2), padding='same'))


model2.add(Conv3D(filters=64, kernel_size=(2, 3, 3), strides=(1, 1, 1), padding='valid', activation='relu'))
model2.add(MaxPooling3D(pool_size=2, strides=(2, 2, 2), padding='same'))


model2.add(Conv3D(filters=256, kernel_size=(2, 3, 3), strides=(1, 1, 1), padding='valid', activation='relu'))
model2.add(MaxPooling3D(pool_size=1, strides=(2, 2, 2), padding='same'))
#model2.add(Dropout(0.5))

#model2.add(Conv3D(filters=1024, kernel_size=(2, 3, 3), strides=(1, 1, 1), padding='same', activation='relu'))
#model2.add(MaxPooling3D(pool_size=2, strides=(2, 2, 2), padding='same'))

# A global average pooling layer to get a 1-d vector
# The vector will have a depth (same as number of elements in the vector) of 1024
model2.add(GlobalAveragePooling3D())

# Hidden layer
model2.add(Dense(32, activation='relu'))

# Dropout Layer
model2.add(Dropout(0.5))

# Output layer
model2.add(Dense(6, activation='softmax'))

model2.summary()# Imports
from keras.callbacks import ModelCheckpoint

# Compiling the model
model2.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])

# Saving the model that performed the best on the validation set
checkpoint = ModelCheckpoint(filepath='Model_4.weights.best.hdf5', save_best_only=True, verbose=1)


history = model2.fit(X_train, y_train, batch_size=16, epochs=30, 
                    validation_data=(X_valid, y_valid), verbose=2, callbacks=[checkpoint])# Loading the model that performed the best on the validation set
model2.load_weights('Model_4.weights.best.hdf5')

# Testing the model on the Test data
(loss, accuracy) = model2.evaluate(X_test, y_test, batch_size=16, verbose=0)

print('Accuracy on test data: {:.2f}%'.format(accuracy * 100))plt.figure(figsize=(14, 8))

loss = history.history['loss']                          # Loss on the training data
val_loss = history.history['val_loss']                  # Loss on the validation data
epochs = range(1, 31)

plt.plot(epochs, loss, 'ro-', label='Training Loss')
plt.plot(epochs, val_loss, 'go-', label = 'Validation Loss')
plt.legend()

    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
